# 服务器端对话功能改造指南

## 📋 改造概览

**服务器地址**: 172.31.179.77  
**文件位置**: `/home/cshcsh/rag知识系统/qwen_vl_lora.py`  
**改造目标**: 添加 `/chat` 端点，支持带上下文的多轮对话  
**预计时间**: 30-60分钟

---

## 🎯 改造步骤

### 步骤 1: 登录服务器并备份

```bash
# 1. SSH登录服务器
ssh cshcsh@172.31.179.77

# 2. 进入项目目录
cd /home/cshcsh/rag知识系统

# 3. 备份原文件（重要！）
cp qwen_vl_lora.py qwen_vl_lora.py.backup_$(date +%Y%m%d_%H%M%S)

# 4. 验证备份
ls -lh qwen_vl_lora.py*
```

---

### 步骤 2: 在文件末尾添加对话端点

使用vim或nano编辑文件：

```bash
vim qwen_vl_lora.py
# 或
nano qwen_vl_lora.py
```

**在文件末尾（在 `if __name__ == '__main__':` 之前）添加以下代码：**

```python
# ============================================
# 新增：对话端点（AI学习伙伴功能）
# ============================================

@app.route('/chat', methods=['POST'])
def chat_with_context():
    """
    带上下文的对话接口
    支持基于批改结果的多轮对话
    """
    try:
        data = request.get_json()
        
        # 获取参数
        task_id = data.get('task_id')
        conversation_history = data.get('conversation_history', [])
        grading_context = data.get('grading_context', {})
        
        # 参数验证
        if not task_id:
            return jsonify({
                'success': False,
                'error': '缺少task_id参数'
            }), 400
            
        if not conversation_history:
            return jsonify({
                'success': False,
                'error': '缺少对话历史'
            }), 400
            
        print(f"\n{'='*60}")
        print(f"💬 [对话] 收到对话请求")
        print(f"   任务ID: {task_id}")
        print(f"   历史消息数: {len(conversation_history)}")
        print(f"   批改上下文: {'有' if grading_context else '无'}")
        print(f"{'='*60}\n")
        
        # 构建系统提示词（包含批改上下文）
        system_prompt = build_chat_system_prompt(grading_context)
        
        # 构建完整对话消息
        messages = []
        
        # 添加系统角色（包含批改信息）
        messages.append({
            "role": "system",
            "content": system_prompt
        })
        
        # 添加对话历史（只取最后10轮，避免上下文过长）
        recent_history = conversation_history[-20:] if len(conversation_history) > 20 else conversation_history
        for msg in recent_history:
            messages.append({
                "role": msg.get('role'),
                "content": msg.get('content')
            })
        
        print(f"📝 构建对话上下文: {len(messages)} 条消息")
        
        # 调用模型生成回复
        response_text = generate_chat_response(messages)
        
        print(f"✅ [对话] 生成回复成功，长度: {len(response_text)}")
        
        return jsonify({
            'success': True,
            'response': response_text,
            'context_used': True,
            'timestamp': int(time.time())
        })
        
    except Exception as e:
        print(f"❌ [对话] 错误: {str(e)}")
        import traceback
        traceback.print_exc()
        
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


def build_chat_system_prompt(grading_context):
    """
    构建包含批改信息的系统提示词
    """
    # 提取批改结果信息
    summary = grading_context.get('summary', {})
    total_questions = summary.get('total_questions', 0)
    correct_count = summary.get('correct_count', 0)
    accuracy_rate = summary.get('accuracy_rate', 0)
    
    grading_results = grading_context.get('grading_result', [])
    wrong_knowledges = grading_context.get('wrong_knowledges', [])
    
    # 构建系统提示词
    prompt = f"""你是一个专业的AI学习伙伴，刚刚批改了学生的作业。以下是批改结果：

【批改概况】
- 总题数：{total_questions}题
- 正确题数：{correct_count}题
- 正确率：{accuracy_rate*100:.1f}%

"""
    
    # 添加错题信息
    if grading_results:
        wrong_questions = [r for r in grading_results if not r.get('correct', True)]
        if wrong_questions:
            prompt += "【错题详情】\n"
            for i, q in enumerate(wrong_questions[:5], 1):  # 最多显示5道错题
                prompt += f"{i}. 题目：{q.get('question', '未知')[:100]}...\n"
                prompt += f"   学生答案：{q.get('answer', '未答')}\n"
                prompt += f"   正确答案：{q.get('correct_answer', '未提供')}\n"
                prompt += f"   错误原因：{q.get('explanation', '暂无')[:150]}...\n\n"
    
    # 添加薄弱知识点
    if wrong_knowledges:
        prompt += "【薄弱知识点】\n"
        for kp in wrong_knowledges[:10]:
            prompt += f"- {kp}\n"
        prompt += "\n"
    
    prompt += """【你的任务】
1. 基于上述批改结果回答学生的问题
2. 提供针对性的学习建议和解题思路
3. 解释错误原因，帮助学生理解知识点
4. 语言要友好、耐心、鼓励性
5. 回答要具体、清晰、有帮助

请开始对话吧！"""
    
    return prompt


def generate_chat_response(messages):
    """
    使用Qwen2.5-VL生成对话回复
    使用对话优化的参数
    """
    try:
        # 格式化消息为文本（简化版本）
        conversation_text = ""
        for msg in messages:
            role = msg.get('role', 'user')
            content = msg.get('content', '')
            
            if role == 'system':
                conversation_text += f"System: {content}\n\n"
            elif role == 'user':
                conversation_text += f"User: {content}\n\n"
            elif role == 'assistant':
                conversation_text += f"Assistant: {content}\n\n"
        
        conversation_text += "Assistant: "
        
        print(f"📤 发送对话到模型...")
        
        # 使用tokenizer处理
        inputs = tokenizer(
            conversation_text,
            return_tensors="pt",
            truncation=True,
            max_length=4096  # 对话模式使用较短的上下文
        ).to(model.device)
        
        # 对话生成参数（与批改不同）
        generation_config = {
            "max_new_tokens": 512,        # 对话回复较短
            "temperature": 0.7,           # 增加创造性
            "top_p": 0.9,                 # 核采样
            "top_k": 50,
            "do_sample": True,            # 启用采样
            "repetition_penalty": 1.1,    # 避免重复
            "pad_token_id": tokenizer.pad_token_id,
            "eos_token_id": tokenizer.eos_token_id,
        }
        
        # 生成回复
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                **generation_config
            )
        
        # 解码
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # 提取助手回复（移除输入部分）
        if "Assistant: " in full_response:
            parts = full_response.split("Assistant: ")
            response = parts[-1].strip()
        else:
            response = full_response.strip()
        
        # 清理响应
        response = response.replace("User:", "").strip()
        
        print(f"✅ 生成回复完成")
        
        return response
        
    except Exception as e:
        print(f"❌ 生成回复失败: {str(e)}")
        # 返回友好的错误消息
        return "抱歉，我在思考时遇到了一些问题。能否换个方式问我呢？"


# ============================================
# 对话端点添加完成
# ============================================
```

---

### 步骤 3: 保存并重启服务

```bash
# 1. 保存文件
# vim: 按 ESC，然后输入 :wq
# nano: 按 Ctrl+X，然后 Y，然后 Enter

# 2. 检查Python语法
python3 -m py_compile qwen_vl_lora.py

# 3. 如果有错误，恢复备份
# cp qwen_vl_lora.py.backup_XXXXXX qwen_vl_lora.py

# 4. 停止旧服务
pkill -f qwen_vl_lora.py

# 5. 重新启动服务（使用nohup后台运行）
nohup python3 qwen_vl_lora.py > qwen_vl_lora.log 2>&1 &

# 6. 查看进程
ps aux | grep qwen_vl_lora

# 7. 查看日志确认启动
tail -f qwen_vl_lora.log
```

---

### 步骤 4: 测试对话端点

```bash
# 使用curl测试对话端点
curl -X POST http://172.31.179.77:8007/chat \
  -H "Content-Type: application/json" \
  -d '{
    "task_id": "test_123",
    "conversation_history": [
      {
        "role": "user",
        "content": "为什么我的第一题错了？"
      }
    ],
    "grading_context": {
      "summary": {
        "total_questions": 5,
        "correct_count": 3,
        "accuracy_rate": 0.6
      },
      "grading_result": [
        {
          "question": "1+1=?",
          "answer": "3",
          "correct_answer": "2",
          "correct": false,
          "explanation": "基本算术错误"
        }
      ],
      "wrong_knowledges": ["基础加法"]
    }
  }'
```

**预期返回**：
```json
{
  "success": true,
  "response": "第一题你答错了是因为...",
  "context_used": true,
  "timestamp": 1234567890
}
```

---

## 🔍 关键注意事项

### 1. 已存在的导入检查
确保文件顶部有以下导入：
```python
import time
import torch
```

如果没有，在文件开头添加。

### 2. tokenizer和model变量
确保这些变量在文件中已定义。它们应该在服务启动时初始化。

### 3. 端口确认
确保服务运行在端口8007：
```python
if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8007, debug=False)
```

### 4. GPU内存管理
对话使用了较少的token（512 vs 8000），不会显著增加GPU负担。

---

## 🐛 故障排除

### 问题1: 服务启动失败
```bash
# 查看详细错误
tail -100 qwen_vl_lora.log

# 检查语法错误
python3 -m py_compile qwen_vl_lora.py
```

### 问题2: tokenizer未定义
在添加的代码前确认tokenizer已初始化：
```python
# 应该在文件的全局变量区域有类似代码
tokenizer = AutoTokenizer.from_pretrained(...)
model = AutoModel.from_pretrained(...)
```

### 问题3: 内存不足
减少对话的max_new_tokens：
```python
"max_new_tokens": 256,  # 从512改为256
```

### 问题4: 端点404
确认服务已重启并监听8007端口：
```bash
netstat -tlnp | grep 8007
```

---

## ✅ 验证清单

完成后检查：

- [ ] 文件已备份
- [ ] 代码已添加到正确位置
- [ ] 语法检查通过
- [ ] 服务成功重启
- [ ] 端口8007正在监听
- [ ] curl测试返回成功
- [ ] 日志无错误信息

---

## 📊 完成后的系统架构

```
前端用户
    ↓
本地后端 (5000端口)
    ↓
POST /api/chat/message
    ↓
组装上下文（批改结果 + 对话历史）
    ↓
POST http://172.31.179.77:8007/chat
    ↓
服务器端Qwen2.5-VL
    ↓
生成针对性回复
    ↓
返回给用户
```

---

## 🎯 可选优化（改造完成后）

### 优化1: 添加对话缓存
```python
# 缓存最近的对话，避免重复处理
conversation_cache = {}
```

### 优化2: 流式响应
```python
# 使用SSE返回流式文本
def generate_stream():
    # ... 逐字返回
```

### 优化3: 对话历史压缩
```python
# 智能压缩历史，只保留关键信息
def compress_history(messages):
    # ... 总结压缩
```

---

## 📞 需要帮助？

如遇问题：
1. 检查 `qwen_vl_lora.log` 日志文件
2. 确认备份文件存在
3. 可以随时恢复备份
4. 参考本地后端的降级响应机制

---

**改造难度**: ⭐⭐☆☆☆ (中等偏易)  
**风险等级**: 🟢 低（已有备份机制）  
**预期效果**: 🚀 完整的AI对话功能

祝改造顺利！🎉
