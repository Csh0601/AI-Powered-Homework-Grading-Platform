# FastAPIæœåŠ¡å™¨å¯¹è¯ç«¯ç‚¹å®ç°æŒ‡å—

## ğŸ“‹ æœåŠ¡å™¨ä¿¡æ¯

**åœ°å€**: 172.31.179.77:8007  
**æ¡†æ¶**: FastAPI (å¼‚æ­¥æ¡†æ¶)  
**é¡¹ç›®è·¯å¾„**: `/home/cshcsh/ragçŸ¥è¯†ç³»ç»Ÿ`  
**ä¸»æ–‡ä»¶**: éœ€è¦ç¡®è®¤ (é€šå¸¸æ˜¯ `main.py` æˆ– `app.py`)

---

## ğŸ¯ å®æ–½æ­¥éª¤

### æ­¥éª¤ 1: ç™»å½•æœåŠ¡å™¨å¹¶å®šä½æ–‡ä»¶

```bash
# 1. SSHç™»å½•
ssh cshcsh@172.31.179.77

# 2. è¿›å…¥é¡¹ç›®ç›®å½•
cd /home/cshcsh/ragçŸ¥è¯†ç³»ç»Ÿ

# 3. æŸ¥æ‰¾FastAPIä¸»æ–‡ä»¶
ls -la *.py

# 4. æŸ¥æ‰¾å¯èƒ½çš„æ–‡ä»¶
find . -maxdepth 2 -name "*.py" -type f | grep -E "(main|app|server)"
```

---

### æ­¥éª¤ 2: æ·»åŠ Pydanticæ¨¡å‹å®šä¹‰

åœ¨FastAPIä¸»æ–‡ä»¶ä¸­æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼ˆé€šå¸¸åœ¨æ–‡ä»¶å¼€å¤´çš„å¯¼å…¥åï¼‰ï¼š

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
import time

# ============================================
# å¯¹è¯åŠŸèƒ½çš„æ•°æ®æ¨¡å‹
# ============================================

class ChatMessage(BaseModel):
    """å•æ¡èŠå¤©æ¶ˆæ¯"""
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²: user æˆ– assistant")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")

class GradingSummary(BaseModel):
    """æ‰¹æ”¹æ‘˜è¦"""
    total_questions: int = Field(0, description="æ€»é¢˜æ•°")
    correct_count: int = Field(0, description="æ­£ç¡®é¢˜æ•°")
    accuracy_rate: float = Field(0.0, description="æ­£ç¡®ç‡")

class GradingResultItem(BaseModel):
    """å•ä¸ªæ‰¹æ”¹ç»“æœ"""
    question: str = Field("", description="é¢˜ç›®å†…å®¹")
    answer: str = Field("", description="å­¦ç”Ÿç­”æ¡ˆ")
    correct_answer: str = Field("", description="æ­£ç¡®ç­”æ¡ˆ")
    correct: bool = Field(False, description="æ˜¯å¦æ­£ç¡®")
    explanation: str = Field("", description="æ‰¹æ”¹è¯´æ˜")

class GradingContext(BaseModel):
    """æ‰¹æ”¹ä¸Šä¸‹æ–‡"""
    summary: Optional[GradingSummary] = None
    grading_result: List[GradingResultItem] = Field(default_factory=list)
    wrong_knowledges: List[str] = Field(default_factory=list)

class ChatRequest(BaseModel):
    """å¯¹è¯è¯·æ±‚æ¨¡å‹"""
    task_id: str = Field(..., description="ä»»åŠ¡ID")
    conversation_history: List[ChatMessage] = Field(..., description="å¯¹è¯å†å²")
    grading_context: GradingContext = Field(..., description="æ‰¹æ”¹ä¸Šä¸‹æ–‡")

class ChatResponse(BaseModel):
    """å¯¹è¯å“åº”æ¨¡å‹"""
    success: bool = Field(True, description="æ˜¯å¦æˆåŠŸ")
    response: str = Field("", description="AIå›å¤å†…å®¹")
    context_used: bool = Field(True, description="æ˜¯å¦ä½¿ç”¨äº†ä¸Šä¸‹æ–‡")
    timestamp: int = Field(..., description="æ—¶é—´æˆ³")
    error: Optional[str] = Field(None, description="é”™è¯¯ä¿¡æ¯")
```

---

### æ­¥éª¤ 3: æ·»åŠ å¯¹è¯ç«¯ç‚¹

åœ¨FastAPIåº”ç”¨ä¸­æ·»åŠ ä»¥ä¸‹è·¯ç”±ï¼š

```python
# ============================================
# å¯¹è¯ç«¯ç‚¹å®ç°
# ============================================

@app.post("/chat", response_model=ChatResponse)
async def chat_with_context(request: ChatRequest):
    """
    å¸¦ä¸Šä¸‹æ–‡çš„å¯¹è¯æ¥å£
    æ”¯æŒåŸºäºæ‰¹æ”¹ç»“æœçš„æ™ºèƒ½å¯¹è¯
    """
    try:
        print(f"\n{'='*60}")
        print(f"ğŸ’¬ [å¯¹è¯] æ”¶åˆ°å¯¹è¯è¯·æ±‚")
        print(f"   ä»»åŠ¡ID: {request.task_id}")
        print(f"   å†å²æ¶ˆæ¯æ•°: {len(request.conversation_history)}")
        print(f"   æ‰¹æ”¹ä¸Šä¸‹æ–‡: å·²æä¾›")
        print(f"{'='*60}\n")
        
        # 1. æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = build_chat_system_prompt(request.grading_context)
        
        # 2. æ„å»ºå®Œæ•´å¯¹è¯æ¶ˆæ¯
        messages = []
        
        # æ·»åŠ ç³»ç»Ÿè§’è‰²
        messages.append({
            "role": "system",
            "content": system_prompt
        })
        
        # æ·»åŠ å¯¹è¯å†å²ï¼ˆæœ€è¿‘20è½®ï¼‰
        recent_history = request.conversation_history[-20:] if len(request.conversation_history) > 20 else request.conversation_history
        for msg in recent_history:
            messages.append({
                "role": msg.role,
                "content": msg.content
            })
        
        print(f"ğŸ“ æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡: {len(messages)} æ¡æ¶ˆæ¯")
        
        # 3. è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›å¤
        response_text = await generate_chat_response(messages)
        
        print(f"âœ… [å¯¹è¯] ç”Ÿæˆå›å¤æˆåŠŸï¼Œé•¿åº¦: {len(response_text)}")
        
        return ChatResponse(
            success=True,
            response=response_text,
            context_used=True,
            timestamp=int(time.time()),
            error=None
        )
        
    except Exception as e:
        print(f"âŒ [å¯¹è¯] é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        
        return ChatResponse(
            success=False,
            response="",
            context_used=False,
            timestamp=int(time.time()),
            error=str(e)
        )


def build_chat_system_prompt(grading_context: GradingContext) -> str:
    """
    æ„å»ºåŒ…å«æ‰¹æ”¹ä¿¡æ¯çš„ç³»ç»Ÿæç¤ºè¯
    """
    # æå–æ‰¹æ”¹ä¿¡æ¯
    summary = grading_context.summary
    total_questions = summary.total_questions if summary else 0
    correct_count = summary.correct_count if summary else 0
    accuracy_rate = summary.accuracy_rate if summary else 0
    
    grading_results = grading_context.grading_result
    wrong_knowledges = grading_context.wrong_knowledges
    
    # æ„å»ºç³»ç»Ÿæç¤ºè¯
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå­¦ä¹ ä¼™ä¼´ï¼Œåˆšåˆšæ‰¹æ”¹äº†å­¦ç”Ÿçš„ä½œä¸šã€‚ä»¥ä¸‹æ˜¯æ‰¹æ”¹ç»“æœï¼š

ã€æ‰¹æ”¹æ¦‚å†µã€‘
- æ€»é¢˜æ•°ï¼š{total_questions}é¢˜
- æ­£ç¡®é¢˜æ•°ï¼š{correct_count}é¢˜
- æ­£ç¡®ç‡ï¼š{accuracy_rate*100:.1f}%

"""
    
    # æ·»åŠ é”™é¢˜è¯¦æƒ…
    if grading_results:
        wrong_questions = [r for r in grading_results if not r.correct]
        if wrong_questions:
            prompt += "ã€é”™é¢˜è¯¦æƒ…ã€‘\n"
            for i, q in enumerate(wrong_questions[:5], 1):  # æœ€å¤šæ˜¾ç¤º5é“
                prompt += f"{i}. é¢˜ç›®ï¼š{q.question[:100]}...\n"
                prompt += f"   å­¦ç”Ÿç­”æ¡ˆï¼š{q.answer}\n"
                prompt += f"   æ­£ç¡®ç­”æ¡ˆï¼š{q.correct_answer}\n"
                prompt += f"   é”™è¯¯åŸå› ï¼š{q.explanation[:150]}...\n\n"
    
    # æ·»åŠ è–„å¼±çŸ¥è¯†ç‚¹
    if wrong_knowledges:
        prompt += "ã€è–„å¼±çŸ¥è¯†ç‚¹ã€‘\n"
        for kp in wrong_knowledges[:10]:
            prompt += f"- {kp}\n"
        prompt += "\n"
    
    prompt += """ã€ä½ çš„ä»»åŠ¡ã€‘
1. åŸºäºä¸Šè¿°æ‰¹æ”¹ç»“æœå›ç­”å­¦ç”Ÿçš„é—®é¢˜
2. æä¾›é’ˆå¯¹æ€§çš„å­¦ä¹ å»ºè®®å’Œè§£é¢˜æ€è·¯
3. è§£é‡Šé”™è¯¯åŸå› ï¼Œå¸®åŠ©å­¦ç”Ÿç†è§£çŸ¥è¯†ç‚¹
4. è¯­è¨€è¦å‹å¥½ã€è€å¿ƒã€é¼“åŠ±æ€§
5. å›ç­”è¦å…·ä½“ã€æ¸…æ™°ã€æœ‰å¸®åŠ©

è¯·å¼€å§‹å¯¹è¯å§ï¼"""
    
    return prompt


async def generate_chat_response(messages: List[Dict]) -> str:
    """
    ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå¯¹è¯å›å¤ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
    
    Args:
        messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
        
    Returns:
        AIå›å¤å†…å®¹
    """
    try:
        # æ ¼å¼åŒ–æ¶ˆæ¯ä¸ºæ–‡æœ¬
        conversation_text = ""
        for msg in messages:
            role = msg.get('role', 'user')
            content = msg.get('content', '')
            
            if role == 'system':
                conversation_text += f"System: {content}\n\n"
            elif role == 'user':
                conversation_text += f"User: {content}\n\n"
            elif role == 'assistant':
                conversation_text += f"Assistant: {content}\n\n"
        
        conversation_text += "Assistant: "
        
        print(f"ğŸ“¤ å‘é€å¯¹è¯åˆ°æ¨¡å‹...")
        
        # ä½¿ç”¨tokenizerå¤„ç†
        inputs = tokenizer(
            conversation_text,
            return_tensors="pt",
            truncation=True,
            max_length=4096  # å¯¹è¯æ¨¡å¼ä½¿ç”¨è¾ƒçŸ­çš„ä¸Šä¸‹æ–‡
        ).to(model.device)
        
        # å¯¹è¯ç”Ÿæˆå‚æ•°ï¼ˆæ›´è‡ªç„¶çš„å¯¹è¯ï¼‰
        generation_config = {
            "max_new_tokens": 512,        # å¯¹è¯å›å¤è¾ƒçŸ­
            "temperature": 0.7,           # å¢åŠ åˆ›é€ æ€§
            "top_p": 0.9,                 # æ ¸é‡‡æ ·
            "top_k": 50,
            "do_sample": True,            # å¯ç”¨é‡‡æ ·
            "repetition_penalty": 1.1,    # é¿å…é‡å¤
            "pad_token_id": tokenizer.pad_token_id,
            "eos_token_id": tokenizer.eos_token_id,
        }
        
        # ç”Ÿæˆå›å¤ï¼ˆåŒæ­¥è°ƒç”¨ï¼Œä½†åœ¨asyncå‡½æ•°ä¸­ï¼‰
        import torch
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                **generation_config
            )
        
        # è§£ç 
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–åŠ©æ‰‹å›å¤
        if "Assistant: " in full_response:
            parts = full_response.split("Assistant: ")
            response = parts[-1].strip()
        else:
            response = full_response.strip()
        
        # æ¸…ç†å“åº”
        response = response.replace("User:", "").strip()
        
        print(f"âœ… ç”Ÿæˆå›å¤å®Œæˆ")
        
        return response
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå›å¤å¤±è´¥: {str(e)}")
        return "æŠ±æ­‰ï¼Œæˆ‘åœ¨æ€è€ƒæ—¶é‡åˆ°äº†ä¸€äº›é—®é¢˜ã€‚èƒ½å¦æ¢ä¸ªæ–¹å¼é—®æˆ‘å‘¢ï¼Ÿ"
```

---

### æ­¥éª¤ 4: ç¡®è®¤å¿…è¦çš„å¯¼å…¥å’Œå˜é‡

ç¡®ä¿æ–‡ä»¶é¡¶éƒ¨æœ‰ä»¥ä¸‹å¯¼å…¥ï¼š

```python
import torch
import time
from typing import List, Dict, Any, Optional
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
```

ç¡®è®¤å…¨å±€å˜é‡å·²å®šä¹‰ï¼š

```python
# åº”è¯¥åœ¨æ–‡ä»¶ä¸­å·²æœ‰è¿™äº›å˜é‡
tokenizer = ...  # AutoTokenizerå®ä¾‹
model = ...      # æ¨¡å‹å®ä¾‹
app = FastAPI()  # FastAPIåº”ç”¨å®ä¾‹
```

---

### æ­¥éª¤ 5: é‡å¯FastAPIæœåŠ¡

```bash
# 1. æŸ¥æ‰¾FastAPIè¿›ç¨‹
ps aux | grep -E "(uvicorn|fastapi|python.*main\.py)"

# 2. åœæ­¢æ—§è¿›ç¨‹
pkill -f "uvicorn\|python.*main\.py"

# 3. é‡æ–°å¯åŠ¨æœåŠ¡ï¼ˆæ ¹æ®å®é™…å¯åŠ¨æ–¹å¼ï¼‰
# æ–¹å¼A: ä½¿ç”¨uvicorn
nohup uvicorn main:app --host 0.0.0.0 --port 8007 > fastapi.log 2>&1 &

# æ–¹å¼B: ç›´æ¥è¿è¡ŒPythonæ–‡ä»¶
nohup python3 main.py > fastapi.log 2>&1 &

# 4. éªŒè¯æœåŠ¡å¯åŠ¨
ps aux | grep python
netstat -tlnp | grep 8007

# 5. æŸ¥çœ‹æ—¥å¿—
tail -f fastapi.log
```

---

### æ­¥éª¤ 6: æµ‹è¯•å¯¹è¯ç«¯ç‚¹

```bash
# ä½¿ç”¨curlæµ‹è¯•
curl -X POST http://172.31.179.77:8007/chat \
  -H "Content-Type: application/json" \
  -d '{
    "task_id": "test_123",
    "conversation_history": [
      {
        "role": "user",
        "content": "ä¸ºä»€ä¹ˆæˆ‘çš„ç¬¬ä¸€é¢˜é”™äº†ï¼Ÿ"
      }
    ],
    "grading_context": {
      "summary": {
        "total_questions": 5,
        "correct_count": 3,
        "accuracy_rate": 0.6
      },
      "grading_result": [
        {
          "question": "1+1=?",
          "answer": "3",
          "correct_answer": "2",
          "correct": false,
          "explanation": "åŸºæœ¬ç®—æœ¯é”™è¯¯"
        }
      ],
      "wrong_knowledges": ["åŸºç¡€åŠ æ³•"]
    }
  }'
```

**é¢„æœŸè¿”å›**ï¼š
```json
{
  "success": true,
  "response": "ç¬¬ä¸€é¢˜ä½ ç­”é”™äº†æ˜¯å› ä¸º...",
  "context_used": true,
  "timestamp": 1234567890,
  "error": null
}
```

---

## ğŸ” å…³é”®å·®å¼‚ï¼šFlask vs FastAPI

### Flaskç‰ˆæœ¬ï¼ˆæ—§ï¼‰
```python
@app.route('/chat', methods=['POST'])
def chat_with_context():
    data = request.get_json()
    # ... åŒæ­¥å¤„ç†
    return jsonify(result)
```

### FastAPIç‰ˆæœ¬ï¼ˆæ–°ï¼‰
```python
@app.post("/chat", response_model=ChatResponse)
async def chat_with_context(request: ChatRequest):
    # ... å¼‚æ­¥å¤„ç†
    return ChatResponse(...)
```

### ä¸»è¦å˜åŒ–
1. âœ… **è£…é¥°å™¨**: `@app.route` â†’ `@app.post`
2. âœ… **å‚æ•°**: `request.get_json()` â†’ ç›´æ¥ä½¿ç”¨Pydanticæ¨¡å‹
3. âœ… **è¿”å›**: `jsonify()` â†’ ç›´æ¥è¿”å›Pydanticæ¨¡å‹
4. âœ… **å¼‚æ­¥**: æ”¯æŒ `async def`
5. âœ… **ç±»å‹éªŒè¯**: è‡ªåŠ¨éªŒè¯è¯·æ±‚æ ¼å¼

---

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜1: PydanticéªŒè¯é”™è¯¯
```bash
# æ£€æŸ¥è¯·æ±‚æ ¼å¼æ˜¯å¦ç¬¦åˆæ¨¡å‹å®šä¹‰
# æŸ¥çœ‹æ—¥å¿—ä¸­çš„è¯¦ç»†é”™è¯¯ä¿¡æ¯
tail -100 fastapi.log
```

### é—®é¢˜2: tokenizeræœªå®šä¹‰
```python
# ç¡®ä¿åœ¨æ–‡ä»¶ä¸­å·²åŠ è½½æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(...)
model = AutoModel.from_pretrained(...)
```

### é—®é¢˜3: ç«¯å£è¢«å ç”¨
```bash
# æŸ¥æ‰¾å ç”¨8007ç«¯å£çš„è¿›ç¨‹
lsof -i :8007
# æˆ–
netstat -tlnp | grep 8007
```

### é—®é¢˜4: GPUå†…å­˜ä¸è¶³
```python
# å‡å°‘max_new_tokens
"max_new_tokens": 256,  # ä»512å‡å°‘åˆ°256
```

---

## âœ… å®ŒæˆåéªŒè¯

1. âœ… FastAPIæœåŠ¡åœ¨8007ç«¯å£è¿è¡Œ
2. âœ… `/chat` ç«¯ç‚¹è¿”å›200çŠ¶æ€ç 
3. âœ… å“åº”æ ¼å¼ç¬¦åˆChatResponseæ¨¡å‹
4. âœ… successå­—æ®µä¸ºtrue
5. âœ… responseå­—æ®µåŒ…å«AIå›å¤
6. âœ… æ—¥å¿—æ— é”™è¯¯

---

## ğŸ“Š æœ¬åœ°åç«¯å·²é€‚é…

æœ¬åœ°åç«¯(`chat_service.py`)å·²æ›´æ–°ä¸ºï¼š
- âœ… å‘é€FastAPIæ ¼å¼çš„è¯·æ±‚
- âœ… è§£æFastAPIæ ¼å¼çš„å“åº”
- âœ… å¤„ç†successå­—æ®µ
- âœ… æå–responseå†…å®¹
- âœ… é”™è¯¯å¤„ç†å’Œé™çº§å“åº”

---

## ğŸ¯ å®Œæ•´çš„æ•°æ®æµ

```
å‰ç«¯ChatScreen
    â†“
æœ¬åœ°åç«¯ POST /api/chat/message
    â†“
chat_service._call_server_api()
    â†“
POST http://172.31.179.77:8007/chat
    â†“
FastAPIæœåŠ¡å™¨
    â†“
build_chat_system_prompt() + generate_chat_response()
    â†“
è¿”å› ChatResponse(success=True, response="AIå›å¤")
    â†“
æœ¬åœ°åç«¯æå– result['response']
    â†“
è¿”å›ç»™å‰ç«¯
    â†“
ChatScreenæ˜¾ç¤ºAIæ¶ˆæ¯
```

---

**æ”¹é€ éš¾åº¦**: â­â­â­â˜†â˜† (ä¸­ç­‰)  
**é£é™©ç­‰çº§**: ğŸŸ¢ ä½  
**é¢„æœŸæ•ˆæœ**: ğŸš€ å®Œæ•´çš„AIå¯¹è¯åŠŸèƒ½

å®Œæˆåå³å¯ä½¿ç”¨å®Œæ•´çš„AIå­¦ä¹ ä¼™ä¼´åŠŸèƒ½ï¼ğŸ‰
