2025-07-31 17:06:20 | INFO | model_worker | args: Namespace(host='0.0.0.0', port=40000, worker_address='http://localhost:40000', controller_address='http://localhost:10000', model_path='./checkpoints/llava-1.5-7b-hf', model_base=None, model_name=None, device='cuda', multi_modal=False, limit_model_concurrency=5, stream_interval=1, no_register=False, load_8bit=False, load_4bit=False, use_flash_attn=False)
2025-07-31 17:06:20 | INFO | model_worker | Loading the model llava-1.5-7b-hf on worker d48c89 ...
2025-07-31 17:06:20 | ERROR | stderr | Traceback (most recent call last):
2025-07-31 17:06:20 | ERROR | stderr |   File "C:\Users\48108\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\utils\hub.py", line 385, in cached_file
2025-07-31 17:06:20 | ERROR | stderr |     resolved_file = hf_hub_download(
2025-07-31 17:06:20 | ERROR | stderr |   File "C:\Users\48108\AppData\Local\Programs\Python\Python310\lib\site-packages\huggingface_hub\utils\_validators.py", line 106, in _inner_fn
2025-07-31 17:06:20 | ERROR | stderr |     validate_repo_id(arg_value)
2025-07-31 17:06:20 | ERROR | stderr |   File "C:\Users\48108\AppData\Local\Programs\Python\Python310\lib\site-packages\huggingface_hub\utils\_validators.py", line 154, in validate_repo_id
2025-07-31 17:06:20 | ERROR | stderr |     raise HFValidationError(
2025-07-31 17:06:20 | ERROR | stderr | huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './checkpoints/llava-1.5-7b-hf'. Use `repo_type` argument if needed.
2025-07-31 17:06:20 | ERROR | stderr | 
2025-07-31 17:06:20 | ERROR | stderr | The above exception was the direct cause of the following exception:
2025-07-31 17:06:20 | ERROR | stderr | 
2025-07-31 17:06:20 | ERROR | stderr | Traceback (most recent call last):
2025-07-31 17:06:20 | ERROR | stderr |   File "C:\Users\48108\AppData\Local\Programs\Python\Python310\lib\runpy.py", line 196, in _run_module_as_main
2025-07-31 17:06:20 | ERROR | stderr |     return _run_code(code, main_globals, None,
2025-07-31 17:06:20 | ERROR | stderr |   File "C:\Users\48108\AppData\Local\Programs\Python\Python310\lib\runpy.py", line 86, in _run_code
2025-07-31 17:06:20 | ERROR | stderr |     exec(code, run_globals)
2025-07-31 17:06:20 | ERROR | stderr |   File "C:\Users\48108\LLaVA\llava\serve\model_worker.py", line 277, in <module>
2025-07-31 17:06:20 | ERROR | stderr |     worker = ModelWorker(args.controller_address,
2025-07-31 17:06:20 | ERROR | stderr |   File "C:\Users\48108\LLaVA\llava\serve\model_worker.py", line 65, in __init__
2025-07-31 17:06:20 | ERROR | stderr |     self.tokenizer, self.model, self.image_processor, self.context_len = load_pretrained_model(
2025-07-31 17:06:20 | ERROR | stderr |   File "C:\Users\48108\LLaVA\llava\model\builder.py", line 116, in load_pretrained_model
2025-07-31 17:06:20 | ERROR | stderr |     tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
2025-07-31 17:06:20 | ERROR | stderr |   File "C:\Users\48108\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\models\auto\tokenization_auto.py", line 758, in from_pretrained
2025-07-31 17:06:20 | ERROR | stderr |     tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
2025-07-31 17:06:20 | ERROR | stderr |   File "C:\Users\48108\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\models\auto\tokenization_auto.py", line 590, in get_tokenizer_config
2025-07-31 17:06:20 | ERROR | stderr |     resolved_config_file = cached_file(
2025-07-31 17:06:20 | ERROR | stderr |   File "C:\Users\48108\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\utils\hub.py", line 450, in cached_file
2025-07-31 17:06:20 | ERROR | stderr |     raise EnvironmentError(
2025-07-31 17:06:20 | ERROR | stderr | OSError: Incorrect path_or_model_id: './checkpoints/llava-1.5-7b-hf'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
