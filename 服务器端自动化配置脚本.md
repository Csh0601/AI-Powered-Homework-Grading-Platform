# 服务器端Qwen3:30B自动化配置指南

> 这个文档专门为服务器端的Cursor AI助手准备，包含所有必要的配置命令和验证步骤。

## 🎯 任务目标
在服务器 `172.31.179.77` 上配置Qwen3:30B模型，提供API服务给远程客户端调用。

## 📋 服务器信息
- **服务器IP**: 172.31.179.77
- **用户名**: cshcsh
- **密码**: Csh061212
- **目标模型**: qwen3:30B
- **服务端口**: 11434

## 🚀 自动化配置步骤

### 步骤1: 检查系统环境
```bash
# 检查操作系统信息
uname -a
cat /etc/os-release

# 检查磁盘空间（qwen3:30B需要约20GB空间）
df -h

# 检查内存（推荐16GB+）
free -h

# 检查是否已安装ollama
which ollama
ollama --version
```

### 步骤2: 安装Ollama（如果未安装）
```bash
# 如果ollama未安装，执行以下命令
curl -fsSL https://ollama.ai/install.sh | sh

# 验证安装
ollama --version
```

### 步骤3: 下载和配置Qwen3:30B模型
```bash
# 检查现有模型
ollama list

# 下载qwen3:30B模型（这可能需要较长时间）
echo "开始下载qwen3:30B模型，预计需要10-30分钟..."
ollama pull qwen3:30B

# 验证模型下载完成
ollama list | grep qwen3:30B
if [ $? -eq 0 ]; then
    echo "✅ qwen3:30B模型下载成功"
else
    echo "❌ qwen3:30B模型下载失败"
    exit 1
fi
```

### 步骤4: 配置Ollama服务监听外部连接
```bash
# 停止可能运行的ollama服务
pkill ollama

# 设置环境变量以监听所有接口
export OLLAMA_HOST=0.0.0.0:11434

# 创建ollama配置目录
mkdir -p ~/.config/systemd/user

# 创建systemd服务文件
cat > ~/.config/systemd/user/ollama.service << 'EOF'
[Unit]
Description=Ollama Service
After=network.target

[Service]
Type=simple
Environment=OLLAMA_HOST=0.0.0.0:11434
ExecStart=/usr/local/bin/ollama serve
Restart=always
RestartSec=5

[Install]
WantedBy=default.target
EOF

# 重新加载systemd配置
systemctl --user daemon-reload

# 启用并启动ollama服务
systemctl --user enable ollama
systemctl --user start ollama

# 检查服务状态
systemctl --user status ollama
```

### 步骤5: 配置防火墙
```bash
# 检查防火墙状态
sudo ufw status

# 开放11434端口
sudo ufw allow 11434

# 如果使用iptables
sudo iptables -A INPUT -p tcp --dport 11434 -j ACCEPT
sudo iptables-save > /etc/iptables/rules.v4

# 如果使用firewalld
sudo firewall-cmd --add-port=11434/tcp --permanent
sudo firewall-cmd --reload
```

### 步骤6: 验证服务运行
```bash
# 等待服务启动
sleep 10

# 检查端口监听
netstat -tlnp | grep 11434
if [ $? -eq 0 ]; then
    echo "✅ 端口11434正在监听"
else
    echo "❌ 端口11434未监听，检查服务状态"
    systemctl --user status ollama
fi

# 测试本地API
echo "测试本地API连接..."
curl -s http://localhost:11434/api/version
if [ $? -eq 0 ]; then
    echo "✅ 本地API连接成功"
else
    echo "❌ 本地API连接失败"
fi
```

### 步骤7: 测试模型API
```bash
# 测试qwen3:30B模型生成
echo "测试qwen3:30B模型生成功能..."
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3:30B",
    "prompt": "你好，请用中文简单介绍一下你自己",
    "stream": false,
    "options": {
      "num_predict": 100,
      "temperature": 0.1
    }
  }' | jq '.'

if [ $? -eq 0 ]; then
    echo "✅ qwen3:30B模型API测试成功"
else
    echo "❌ qwen3:30B模型API测试失败"
fi
```

### 步骤8: 测试外部访问（可选）
```bash
# 获取服务器外部IP
SERVER_IP=$(curl -s ifconfig.me)
echo "服务器外部IP: $SERVER_IP"

# 如果有其他服务器可以测试外部访问
echo "可以从其他机器运行以下命令测试外部访问："
echo "curl http://$SERVER_IP:11434/api/version"
```

### 步骤9: 性能优化（可选）
```bash
# 检查GPU支持
nvidia-smi 2>/dev/null
if [ $? -eq 0 ]; then
    echo "✅ 检测到GPU，ollama将自动使用GPU加速"
else
    echo "ℹ️ 未检测到GPU，将使用CPU运行"
fi

# 设置ollama使用的最大内存（以GB为单位）
# export OLLAMA_MAX_LOADED_MODELS=1
# export OLLAMA_NUM_PARALLEL=1
```

### 步骤10: 创建监控脚本
```bash
# 创建服务监控脚本
cat > ~/monitor_ollama.sh << 'EOF'
#!/bin/bash
echo "=== Ollama服务监控 ==="
echo "服务状态:"
systemctl --user status ollama --no-pager

echo -e "\n端口监听:"
netstat -tlnp | grep 11434

echo -e "\n模型列表:"
ollama list

echo -e "\n系统资源:"
free -h
df -h | grep -E "/$|/home"

echo -e "\n最近日志:"
journalctl --user -u ollama -n 10 --no-pager
EOF

chmod +x ~/monitor_ollama.sh

echo "监控脚本已创建: ~/monitor_ollama.sh"
echo "运行 ./monitor_ollama.sh 查看服务状态"
```

## 🔍 故障排除命令

### 查看日志
```bash
# 查看ollama服务日志
journalctl --user -u ollama -f

# 查看最近的错误
journalctl --user -u ollama -p err
```

### 重启服务
```bash
# 重启ollama服务
systemctl --user restart ollama

# 检查重启后状态
systemctl --user status ollama
```

### 手动启动（调试用）
```bash
# 停止systemd服务
systemctl --user stop ollama

# 手动启动（前台运行，便于调试）
OLLAMA_HOST=0.0.0.0:11434 ollama serve
```

## ✅ 验证清单

完成配置后，请确认以下项目：

- [ ] ollama服务正在运行
- [ ] qwen3:30B模型已下载
- [ ] 端口11434正在监听0.0.0.0
- [ ] 防火墙已开放11434端口
- [ ] 本地API测试成功
- [ ] 模型生成测试成功
- [ ] systemd服务已启用（开机自启）

## 📞 完成后通知

配置完成后，请提供以下信息：

1. **服务状态**: `systemctl --user status ollama`
2. **模型列表**: `ollama list`
3. **端口监听**: `netstat -tlnp | grep 11434`
4. **API测试结果**: 包含模型响应的完整输出

## 🚨 注意事项

1. **资源需求**: qwen3:30B需要大约16GB内存，确保服务器有足够资源
2. **下载时间**: 模型下载可能需要10-30分钟，取决于网络速度
3. **持久化**: 使用systemd确保服务重启后自动运行
4. **安全性**: 只在内网使用，避免直接暴露到公网
5. **监控**: 定期检查服务状态和资源使用情况

## 🎉 完成指标

当看到以下输出时，表示配置成功：

```bash
# systemctl --user status ollama
● ollama.service - Ollama Service
   Loaded: loaded
   Active: active (running)

# ollama list
NAME            ID              SIZE    MODIFIED
qwen3:30B       abcd1234...     20GB    1 minute ago

# curl http://localhost:11434/api/version
{"version":"0.x.x"}

# 模型测试返回正常的中文响应
```

---

**提示**: 将此文档发送给服务器端的Cursor，它可以逐步执行所有配置命令并提供执行结果。
