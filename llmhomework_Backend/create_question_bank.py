#!/usr/bin/env python3
"""
åˆ›å»ºå¤§è§„æ¨¡é¢˜ç›®ç­”æ¡ˆæ•°æ®åº“
Day 9: æ„å»ºå¤§è§„æ¨¡é¢˜ç›®ç­”æ¡ˆæ•°æ®åº“ - ç›®æ ‡1000+é¢˜ç›®æ ‡å‡†åº“
"""

import os
import sys
import pandas as pd
import logging
from datetime import datetime
from pathlib import Path
import json

# è®¾ç½®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'app'))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_question_bank():
    """åˆ›å»ºå¤§è§„æ¨¡é¢˜ç›®ç­”æ¡ˆæ•°æ®åº“"""
    logger.info("ğŸš€ å¼€å§‹åˆ›å»ºå¤§è§„æ¨¡é¢˜ç›®ç­”æ¡ˆæ•°æ®åº“...")
    
    # è®¾ç½®è·¯å¾„
    base_dir = Path(__file__).parent
    raw_dir = base_dir / "raw" / "subjects"
    output_dir = base_dir / "data_collection" / "processed"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir.mkdir(parents=True, exist_ok=True)
    
    all_questions = []
    all_knowledge_points = []
    
    # éå†æ‰€æœ‰å­¦ç§‘ç›®å½•
    for subject_dir in raw_dir.iterdir():
        if not subject_dir.is_dir():
            continue
            
        subject_name = subject_dir.name
        logger.info(f"ğŸ“š å¤„ç†å­¦ç§‘: {subject_name}")
        
        # å¤„ç†çŸ¥è¯†ç‚¹
        kp_dir = subject_dir / "knowledge_points"
        if kp_dir.exists():
            for kp_file in kp_dir.glob("*.csv"):
                try:
                    df = pd.read_csv(kp_file, encoding='utf-8')
                    df['subject'] = subject_name
                    all_knowledge_points.append(df)
                    logger.info(f"  âœ… åŠ è½½çŸ¥è¯†ç‚¹: {kp_file.name} ({len(df)}æ¡)")
                except Exception as e:
                    logger.error(f"  âŒ åŠ è½½çŸ¥è¯†ç‚¹å¤±è´¥: {kp_file.name} - {e}")
        
        # å¤„ç†é¢˜ç›®
        for question_type in ["exam_questions", "mock_questions"]:
            q_dir = subject_dir / question_type
            if q_dir.exists():
                for q_file in q_dir.glob("*.csv"):
                    try:
                        df = pd.read_csv(q_file, encoding='utf-8')
                        df['subject'] = subject_name
                        df['source_type'] = question_type
                        all_questions.append(df)
                        logger.info(f"  âœ… åŠ è½½é¢˜ç›®: {q_file.name} ({len(df)}æ¡)")
                    except Exception as e:
                        logger.error(f"  âŒ åŠ è½½é¢˜ç›®å¤±è´¥: {q_file.name} - {e}")
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    if all_questions:
        questions_combined = pd.concat(all_questions, ignore_index=True)
        
        # æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
        questions_combined = clean_and_standardize_questions(questions_combined)
        
        # ä¿å­˜ç»Ÿä¸€é¢˜ç›®åº“
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        questions_file = output_dir / f"question_bank_unified_{timestamp}.csv"
        questions_combined.to_csv(questions_file, index=False, encoding='utf-8')
        
        logger.info(f"âœ… é¢˜ç›®åº“åˆ›å»ºå®Œæˆ: {questions_file}")
        logger.info(f"ğŸ“Š é¢˜ç›®ç»Ÿè®¡:")
        logger.info(f"  - æ€»é¢˜ç›®æ•°: {len(questions_combined)}")
        
        # æŒ‰å­¦ç§‘ç»Ÿè®¡
        subject_stats = questions_combined['subject'].value_counts()
        for subject, count in subject_stats.items():
            logger.info(f"  - {subject}: {count}é“é¢˜ç›®")
        
        # æŒ‰é¢˜å‹ç»Ÿè®¡
        type_stats = questions_combined['question_type'].value_counts()
        for qtype, count in type_stats.items():
            logger.info(f"  - {qtype}: {count}é“é¢˜ç›®")
        
        # æŒ‰éš¾åº¦ç»Ÿè®¡
        if 'difficulty_level' in questions_combined.columns:
            difficulty_stats = questions_combined['difficulty_level'].value_counts()
            for level, count in difficulty_stats.items():
                logger.info(f"  - éš¾åº¦{level}: {count}é“é¢˜ç›®")
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        stats = {
            "timestamp": datetime.now().isoformat(),
            "total_questions": len(questions_combined),
            "subjects": subject_stats.to_dict(),
            "question_types": type_stats.to_dict(),
            "difficulty_distribution": difficulty_stats.to_dict() if 'difficulty_level' in questions_combined.columns else {},
            "files_processed": len(all_questions),
            "target_achieved": len(questions_combined) >= 1000
        }
        
        stats_file = output_dir / f"question_bank_stats_{timestamp}.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“ˆ ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {stats_file}")
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°1000+ç›®æ ‡
        if len(questions_combined) >= 1000:
            logger.info("ğŸ‰ æ­å–œï¼å·²è¾¾åˆ°1000+é¢˜ç›®çš„ç›®æ ‡ï¼")
        else:
            logger.info(f"âš ï¸ å½“å‰é¢˜ç›®æ•°é‡: {len(questions_combined)}ï¼Œè·ç¦»1000+ç›®æ ‡è¿˜å·® {1000 - len(questions_combined)}é“é¢˜ç›®")
        
        return questions_file, stats
    else:
        logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é¢˜ç›®æ•°æ®")
        return None, None

def clean_and_standardize_questions(df):
    """æ¸…æ´—å’Œæ ‡å‡†åŒ–é¢˜ç›®æ•°æ®"""
    logger.info("ğŸ§¹ å¼€å§‹æ¸…æ´—å’Œæ ‡å‡†åŒ–é¢˜ç›®æ•°æ®...")
    
    # å»é‡
    initial_count = len(df)
    df = df.drop_duplicates(subset=['question_id'], keep='first')
    logger.info(f"  - å»é‡: {initial_count} -> {len(df)} (ç§»é™¤{initial_count - len(df)}æ¡é‡å¤)")
    
    # å¡«å……ç¼ºå¤±å€¼
    df['difficulty_level'] = df['difficulty_level'].fillna(3)  # é»˜è®¤ä¸­ç­‰éš¾åº¦
    df['score'] = df['score'].fillna(5)  # é»˜è®¤5åˆ†
    df['time_limit'] = df['time_limit'].fillna(3)  # é»˜è®¤3åˆ†é’Ÿ
    
    # æ ‡å‡†åŒ–é¢˜ç›®ç±»å‹
    type_mapping = {
        'choice': 'multiple_choice',
        'fill_blank': 'fill_blank',
        'calculation': 'calculation',
        'application': 'application',
        'essay': 'essay'
    }
    df['question_type'] = df['question_type'].map(type_mapping).fillna(df['question_type'])
    
    # æ ‡å‡†åŒ–å­¦ç§‘åç§°
    subject_mapping = {
        'math': 'æ•°å­¦',
        'chinese': 'è¯­æ–‡',
        'english': 'è‹±è¯­',
        'physics': 'ç‰©ç†',
        'chemistry': 'åŒ–å­¦',
        'biology': 'ç”Ÿç‰©',
        'history': 'å†å²',
        'geography': 'åœ°ç†',
        'politics': 'æ”¿æ²»'
    }
    df['subject'] = df['subject'].map(subject_mapping).fillna(df['subject'])
    
    # æ·»åŠ è´¨é‡è¯„åˆ†
    df['quality_score'] = calculate_quality_score(df)
    
    # æ·»åŠ ç­”æ¡ˆå¤šæ ·æ€§æ”¯æŒ
    df['alternative_answers'] = df.apply(lambda x: generate_alternative_answers(x), axis=1)
    
    logger.info("âœ… æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–å®Œæˆ")
    return df

def calculate_quality_score(row):
    """è®¡ç®—é¢˜ç›®è´¨é‡è¯„åˆ†"""
    score = 5.0  # åŸºç¡€åˆ†
    
    # æ ¹æ®é¢˜ç›®é•¿åº¦è°ƒæ•´
    if len(str(row.get('stem', ''))) > 50:
        score += 1.0
    
    # æ ¹æ®è§£æé•¿åº¦è°ƒæ•´
    if len(str(row.get('explanation', ''))) > 20:
        score += 1.0
    
    # æ ¹æ®é€‰é¡¹æ•°é‡è°ƒæ•´ï¼ˆé€‰æ‹©é¢˜ï¼‰
    if row.get('question_type') == 'multiple_choice':
        options = str(row.get('options', ''))
        if options and len(options.split('|')) >= 4:
            score += 1.0
    
    # æ ¹æ®çŸ¥è¯†ç‚¹æ•°é‡è°ƒæ•´
    knowledge_points = str(row.get('knowledge_points', ''))
    if knowledge_points and len(knowledge_points.split(',')) >= 2:
        score += 1.0
    
    return min(score, 10.0)  # æœ€é«˜10åˆ†

def generate_alternative_answers(row):
    """ç”Ÿæˆå¯æ¥å—çš„æ›¿ä»£ç­”æ¡ˆ"""
    correct_answer = str(row.get('correct_answer', ''))
    if not correct_answer:
        return []
    
    alternatives = []
    
    # å¯¹äºæ•°å­¦é¢˜ï¼Œç”Ÿæˆæ•°å€¼ç›¸è¿‘çš„ç­”æ¡ˆ
    if row.get('subject') == 'æ•°å­¦' and correct_answer.replace('.', '').replace('-', '').isdigit():
        try:
            num = float(correct_answer)
            alternatives.append(str(num + 0.1))
            alternatives.append(str(num - 0.1))
        except:
            pass
    
    # å¯¹äºé€‰æ‹©é¢˜ï¼Œæ·»åŠ å…¶ä»–é€‰é¡¹
    if row.get('question_type') == 'multiple_choice':
        options = str(row.get('options', ''))
        if options:
            option_list = options.split('|')
            for option in option_list:
                if option.strip() != correct_answer.strip():
                    alternatives.append(option.strip())
    
    return alternatives[:3]  # æœ€å¤š3ä¸ªæ›¿ä»£ç­”æ¡ˆ

if __name__ == "__main__":
    questions_file, stats = create_question_bank()
    if questions_file:
        logger.info("ğŸ‰ å¤§è§„æ¨¡é¢˜ç›®ç­”æ¡ˆæ•°æ®åº“åˆ›å»ºå®Œæˆï¼")
        logger.info(f"ğŸ“ æ–‡ä»¶ä½ç½®: {questions_file}")
        if stats and stats['target_achieved']:
            logger.info("âœ… å·²è¾¾åˆ°1000+é¢˜ç›®çš„ç›®æ ‡ï¼")
    else:
        logger.error("âŒ é¢˜ç›®åº“åˆ›å»ºå¤±è´¥")
