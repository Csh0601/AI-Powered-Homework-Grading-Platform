#!/usr/bin/env python3
"""
åŸºäºè¯­ä¹‰çš„é¢˜ç›®ç›¸ä¼¼åº¦è®¡ç®—ç³»ç»Ÿ
Day10ä»»åŠ¡: è®¡ç®—ä¸åŒé¢˜ç›®çš„ç›¸ä¼¼ç¨‹åº¦

ä¸»è¦åŠŸèƒ½ï¼š
1. é¢˜ç›®å‘é‡åŒ–å’Œç´¢å¼•
2. å¿«é€Ÿç›¸ä¼¼é¢˜ç›®æ£€ç´¢
3. å¤šç»´åº¦ç›¸ä¼¼åº¦è®¡ç®—
4. è¯­ä¹‰ç›¸ä¼¼åº¦è¯„åˆ†
5. ç›¸ä¼¼é¢˜ç›®æ¨è

æŠ€æœ¯è¦ç‚¹ï¼š
- å‘é‡æ£€ç´¢å’Œç›¸ä¼¼åº¦ç®—æ³•
- TF-IDFæ–‡æœ¬å‘é‡åŒ–
- è¯­ä¹‰ç‰¹å¾æå–
- é«˜æ•ˆç´¢å¼•å’Œæœç´¢

ç›®æ ‡ï¼šæ¯«ç§’çº§ç›¸ä¼¼é¢˜ç›®æ£€ç´¢ï¼Œç›¸ä¼¼åº¦å‡†ç¡®ç‡75%+
"""

import re
import jieba
import numpy as np
import json
import os
import logging
from typing import Dict, List, Tuple, Any, Optional, Set
from collections import defaultdict, Counter
from datetime import datetime
import hashlib

# æœºå™¨å­¦ä¹ å’ŒNLPåº“
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
    from sklearn.decomposition import TruncatedSVD
    from sklearn.cluster import KMeans
    import scipy.sparse as sp
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    print("âš ï¸ æœºå™¨å­¦ä¹ åº“æœªå®Œå…¨å®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½å—é™")

logger = logging.getLogger(__name__)

class SimilaritySearchEngine:
    """é¢˜ç›®ç›¸ä¼¼åº¦æœç´¢å¼•æ“"""
    
    def __init__(self, model_path: str = None):
        """åˆå§‹åŒ–æœç´¢å¼•æ“"""
        self.model_path = model_path or os.path.join(
            os.path.dirname(__file__), '..', '..', 'models', 'similarity_search'
        )
        
        # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
        os.makedirs(self.model_path, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.tfidf_vectorizer = None
        self.question_vectors = None
        self.question_index = {}  # é¢˜ç›®IDåˆ°ç´¢å¼•çš„æ˜ å°„
        self.index_to_question = {}  # ç´¢å¼•åˆ°é¢˜ç›®æ•°æ®çš„æ˜ å°„
        self.similarity_cache = {}  # ç›¸ä¼¼åº¦ç¼“å­˜
        self.index_built = False  # ç´¢å¼•æ˜¯å¦å·²æ„å»º
        
        # ç›¸ä¼¼åº¦è®¡ç®—å‚æ•°
        self.similarity_weights = {
            'text_similarity': 0.6,     # æ–‡æœ¬ç›¸ä¼¼åº¦æƒé‡
            'type_similarity': 0.2,     # é¢˜å‹ç›¸ä¼¼åº¦æƒé‡
            'difficulty_similarity': 0.1,  # éš¾åº¦ç›¸ä¼¼åº¦æƒé‡
            'subject_similarity': 0.1   # å­¦ç§‘ç›¸ä¼¼åº¦æƒé‡
        }
        
        # æœç´¢ç»Ÿè®¡
        self.search_stats = {
            'total_searches': 0,
            'cache_hits': 0,
            'vector_searches': 0,
            'avg_search_time': 0.0,
            'indexed_questions': 0,
            'index_build_time': 0.0
        }
        
        # æ€§èƒ½ä¼˜åŒ–å‚æ•°
        self.max_cache_size = 1000  # æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
        self.cache_ttl = 3600  # ç¼“å­˜ç”Ÿå­˜æ—¶é—´ï¼ˆç§’ï¼‰
        self.cache_timestamps = {}  # ç¼“å­˜æ—¶é—´æˆ³
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        self._load_models()
    
    def preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        # æ¸…ç†HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'<[^>]+>', '', text)
        text = re.sub(r'\s+', ' ', text.strip())
        
        # ä¿ç•™é‡è¦çš„æ•°å­¦ç¬¦å·
        math_symbols = ['=', '+', '-', 'Ã—', 'Ã·', '>', '<', 'â‰¥', 'â‰¤', 'â‰ ', 'â‰ˆ', 
                       'âˆš', '^', 'Â²', 'Â³', 'Ï€', 'âˆ', 'Â°', 'âˆ ', 'â–³', 'â–¡', 'â—‹']
        
        # åˆ†è¯
        words = []
        for word in jieba.cut(text):
            word = word.strip()
            # ä¿ç•™æœ‰æ„ä¹‰çš„è¯æ±‡å’Œæ•°å­¦ç¬¦å·
            if word and (len(word) > 1 or word in math_symbols or word.isdigit()):
                words.append(word)
        
        return ' '.join(words)
    
    def build_question_index(self, questions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ„å»ºé¢˜ç›®ç´¢å¼•"""
        if not ML_AVAILABLE:
            return self._build_simple_index(questions)
        
        start_time = datetime.now()
        
        try:
            # é¢„å¤„ç†æ‰€æœ‰é¢˜ç›®æ–‡æœ¬
            processed_texts = []
            question_metadata = []
            
            for i, question in enumerate(questions):
                # å¤„ç†é¢˜ç›®æ–‡æœ¬
                stem = question.get('stem', '')
                answer = question.get('correct_answer', '')
                combined_text = f"{stem} {answer}"
                processed_text = self.preprocess_text(combined_text)
                processed_texts.append(processed_text)
                
                # å­˜å‚¨å…ƒæ•°æ®
                question_id = question.get('question_id', f"Q_{i}")
                self.question_index[question_id] = i
                self.index_to_question[i] = question
                question_metadata.append({
                    'question_id': question_id,
                    'question_type': question.get('question_type', 'unknown'),
                    'difficulty_level': question.get('difficulty_level', 3),
                    'subject': question.get('subject', 'unknown'),
                    'processed_text': processed_text
                })
            
            # æ„å»ºTF-IDFå‘é‡
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=5000,
                min_df=2,
                max_df=0.8,
                ngram_range=(1, 2),
                tokenizer=lambda x: x.split(),  # å·²ç»é¢„å¤„ç†è¿‡äº†
                lowercase=True
            )
            
            # æ‹Ÿåˆå¹¶è½¬æ¢æ–‡æœ¬
            tfidf_matrix = self.tfidf_vectorizer.fit_transform(processed_texts)
            
            # ä½¿ç”¨SVDé™ç»´ï¼ˆå¯é€‰ï¼Œç”¨äºå¤§è§„æ¨¡æ•°æ®ï¼‰
            if len(questions) > 1000:
                svd = TruncatedSVD(n_components=300, random_state=42)
                self.question_vectors = svd.fit_transform(tfidf_matrix)
            else:
                self.question_vectors = tfidf_matrix.toarray()
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.search_stats['indexed_questions'] = len(questions)
            build_time = (datetime.now() - start_time).total_seconds()
            self.search_stats['index_build_time'] = build_time
            self.index_built = True
            
            logger.info(f"æˆåŠŸæ„å»º {len(questions)} ä¸ªé¢˜ç›®çš„ç´¢å¼•ï¼Œè€—æ—¶ {build_time:.2f} ç§’")
            
            return {
                'success': True,
                'indexed_questions': len(questions),
                'build_time_seconds': build_time,
                'vector_dimensions': self.question_vectors.shape[1] if self.question_vectors is not None else 0
            }
            
        except Exception as e:
            logger.error(f"æ„å»ºé¢˜ç›®ç´¢å¼•å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _build_simple_index(self, questions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ„å»ºç®€å•ç´¢å¼•ï¼ˆæ— éœ€æœºå™¨å­¦ä¹ åº“ï¼‰"""
        # ç®€å•çš„å…³é”®è¯ç´¢å¼•
        for i, question in enumerate(questions):
            question_id = question.get('question_id', f"Q_{i}")
            self.question_index[question_id] = i
            self.index_to_question[i] = question
        
        self.search_stats['indexed_questions'] = len(questions)
        self.index_built = True
        
        return {
            'success': True,
            'indexed_questions': len(questions),
            'build_time_seconds': 0.1,
            'vector_dimensions': 0
        }
    
    def find_similar_questions(self, query_question: Dict[str, Any], 
                             top_k: int = 10, 
                             similarity_threshold: float = 0.3) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾ç›¸ä¼¼é¢˜ç›®"""
        
        start_time = datetime.now()
        self.search_stats['total_searches'] += 1
        
        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²æ„å»º
        if not self.index_built:
            logger.warning("ç´¢å¼•æœªæ„å»ºï¼Œæ— æ³•è¿›è¡Œç›¸ä¼¼åº¦æœç´¢")
            return []
        
        # æ£€æŸ¥ç¼“å­˜
        query_hash = self._get_question_hash(query_question)
        cache_key = f"{query_hash}_{top_k}_{similarity_threshold}"
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        if self._is_cache_valid(cache_key):
            self.search_stats['cache_hits'] += 1
            return self.similarity_cache[cache_key]
        
        # æ‰§è¡Œæœç´¢
        if not ML_AVAILABLE or self.question_vectors is None:
            results = self._simple_similarity_search(query_question, top_k, similarity_threshold)
        else:
            results = self._vector_similarity_search(query_question, top_k, similarity_threshold)
        
        # æ›´æ–°æœç´¢ç»Ÿè®¡
        search_time = (datetime.now() - start_time).total_seconds()
        self.search_stats['avg_search_time'] = (
            (self.search_stats['avg_search_time'] * (self.search_stats['total_searches'] - 1) + search_time) 
            / self.search_stats['total_searches']
        )
        
        # ç¼“å­˜ç»“æœ
        self._cache_result(cache_key, results)
        
        return results
    
    def _vector_similarity_search(self, query_question: Dict[str, Any], 
                                top_k: int = 10, 
                                similarity_threshold: float = 0.3) -> List[Dict[str, Any]]:
        """åŸºäºå‘é‡çš„ç›¸ä¼¼åº¦æœç´¢"""
        
        self.search_stats['vector_searches'] += 1
        
        try:
            # å¤„ç†æŸ¥è¯¢é¢˜ç›®
            query_stem = query_question.get('stem', '')
            query_answer = query_question.get('correct_answer', '')
            query_text = f"{query_stem} {query_answer}"
            processed_query = self.preprocess_text(query_text)
            
            # å‘é‡åŒ–æŸ¥è¯¢é¢˜ç›®
            query_vector = self.tfidf_vectorizer.transform([processed_query])
            
            # å¦‚æœä½¿ç”¨äº†SVDï¼Œéœ€è¦ç›¸åŒçš„å˜æ¢
            if hasattr(self, 'svd'):
                query_vector = self.svd.transform(query_vector)
            else:
                query_vector = query_vector.toarray()
            
            # è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦
            text_similarities = cosine_similarity(query_vector, self.question_vectors).flatten()
            
            # è®¡ç®—ç»¼åˆç›¸ä¼¼åº¦
            final_similarities = []
            
            for i, text_sim in enumerate(text_similarities):
                if i in self.index_to_question:
                    candidate_question = self.index_to_question[i]
                    
                    # è®¡ç®—å¤šç»´åº¦ç›¸ä¼¼åº¦
                    type_sim = self._calculate_type_similarity(query_question, candidate_question)
                    diff_sim = self._calculate_difficulty_similarity(query_question, candidate_question)
                    subj_sim = self._calculate_subject_similarity(query_question, candidate_question)
                    
                    # åŠ æƒç»¼åˆç›¸ä¼¼åº¦
                    final_sim = (
                        text_sim * self.similarity_weights['text_similarity'] +
                        type_sim * self.similarity_weights['type_similarity'] +
                        diff_sim * self.similarity_weights['difficulty_similarity'] +
                        subj_sim * self.similarity_weights['subject_similarity']
                    )
                    
                    if final_sim >= similarity_threshold:
                        final_similarities.append((i, final_sim, {
                            'text_similarity': text_sim,
                            'type_similarity': type_sim,
                            'difficulty_similarity': diff_sim,
                            'subject_similarity': subj_sim
                        }))
            
            # æ’åºå¹¶è¿”å›Top-K
            final_similarities.sort(key=lambda x: x[1], reverse=True)
            
            results = []
            for i, (idx, sim_score, sim_breakdown) in enumerate(final_similarities[:top_k]):
                question = self.index_to_question[idx].copy()
                results.append({
                    'rank': i + 1,
                    'question': question,
                    'similarity_score': round(sim_score, 4),
                    'similarity_breakdown': {k: round(v, 4) for k, v in sim_breakdown.items()},
                    'match_reasons': self._generate_match_reasons(query_question, question, sim_breakdown)
                })
            
            return results
            
        except Exception as e:
            logger.error(f"å‘é‡ç›¸ä¼¼åº¦æœç´¢å¤±è´¥: {e}")
            return []
    
    def _simple_similarity_search(self, query_question: Dict[str, Any], 
                                top_k: int = 10, 
                                similarity_threshold: float = 0.3) -> List[Dict[str, Any]]:
        """ç®€å•çš„ç›¸ä¼¼åº¦æœç´¢ï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ï¼‰"""
        
        query_text = f"{query_question.get('stem', '')} {query_question.get('correct_answer', '')}"
        query_words = set(jieba.cut(query_text.lower()))
        
        similarities = []
        
        for idx, question in self.index_to_question.items():
            question_text = f"{question.get('stem', '')} {question.get('correct_answer', '')}"
            question_words = set(jieba.cut(question_text.lower()))
            
            # è®¡ç®—Jaccardç›¸ä¼¼åº¦
            intersection = len(query_words.intersection(question_words))
            union = len(query_words.union(question_words))
            text_sim = intersection / union if union > 0 else 0
            
            # è®¡ç®—å…¶ä»–ç»´åº¦ç›¸ä¼¼åº¦
            type_sim = self._calculate_type_similarity(query_question, question)
            diff_sim = self._calculate_difficulty_similarity(query_question, question)
            subj_sim = self._calculate_subject_similarity(query_question, question)
            
            # ç»¼åˆç›¸ä¼¼åº¦
            final_sim = (
                text_sim * 0.7 +
                type_sim * 0.1 +
                diff_sim * 0.1 +
                subj_sim * 0.1
            )
            
            if final_sim >= similarity_threshold:
                similarities.append((idx, final_sim, {
                    'text_similarity': text_sim,
                    'type_similarity': type_sim,
                    'difficulty_similarity': diff_sim,
                    'subject_similarity': subj_sim
                }))
        
        # æ’åºå¹¶è¿”å›ç»“æœ
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        results = []
        for i, (idx, sim_score, sim_breakdown) in enumerate(similarities[:top_k]):
            question = self.index_to_question[idx].copy()
            results.append({
                'rank': i + 1,
                'question': question,
                'similarity_score': round(sim_score, 4),
                'similarity_breakdown': {k: round(v, 4) for k, v in sim_breakdown.items()},
                'match_reasons': self._generate_match_reasons(query_question, question, sim_breakdown)
            })
        
        return results
    
    def _calculate_type_similarity(self, q1: Dict[str, Any], q2: Dict[str, Any]) -> float:
        """è®¡ç®—é¢˜å‹ç›¸ä¼¼åº¦"""
        type1 = q1.get('question_type', 'unknown')
        type2 = q2.get('question_type', 'unknown')
        
        if type1 == type2:
            return 1.0
        
        # å®šä¹‰é¢˜å‹ç›¸ä¼¼åº¦çŸ©é˜µ
        type_similarity_matrix = {
            ('single_choice', 'multiple_choice'): 0.8,
            ('calculation', 'fill_blank'): 0.6,
            ('short_answer', 'essay'): 0.7,
            ('analysis', 'short_answer'): 0.6
        }
        
        # æ£€æŸ¥ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆåŒå‘ï¼‰
        key1 = (type1, type2)
        key2 = (type2, type1)
        
        if key1 in type_similarity_matrix:
            return type_similarity_matrix[key1]
        elif key2 in type_similarity_matrix:
            return type_similarity_matrix[key2]
        else:
            return 0.0
    
    def _calculate_difficulty_similarity(self, q1: Dict[str, Any], q2: Dict[str, Any]) -> float:
        """è®¡ç®—éš¾åº¦ç›¸ä¼¼åº¦"""
        diff1 = q1.get('difficulty_level', 3)
        diff2 = q2.get('difficulty_level', 3)
        
        # éš¾åº¦å·®è·è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜
        diff_gap = abs(diff1 - diff2)
        if diff_gap == 0:
            return 1.0
        elif diff_gap == 1:
            return 0.7
        elif diff_gap == 2:
            return 0.4
        else:
            return 0.0
    
    def _calculate_subject_similarity(self, q1: Dict[str, Any], q2: Dict[str, Any]) -> float:
        """è®¡ç®—å­¦ç§‘ç›¸ä¼¼åº¦"""
        subj1 = q1.get('subject', 'unknown')
        subj2 = q2.get('subject', 'unknown')
        
        if subj1 == subj2:
            return 1.0
        
        # å®šä¹‰å­¦ç§‘ç›¸ä¼¼åº¦
        subject_similarity_matrix = {
            ('math', 'physics'): 0.6,
            ('chinese', 'english'): 0.3,
            ('physics', 'chemistry'): 0.5
        }
        
        key1 = (subj1, subj2)
        key2 = (subj2, subj1)
        
        if key1 in subject_similarity_matrix:
            return subject_similarity_matrix[key1]
        elif key2 in subject_similarity_matrix:
            return subject_similarity_matrix[key2]
        else:
            return 0.0
    
    def _generate_match_reasons(self, query_question: Dict[str, Any], 
                              matched_question: Dict[str, Any], 
                              similarity_breakdown: Dict[str, float]) -> List[str]:
        """ç”ŸæˆåŒ¹é…åŸå› """
        reasons = []
        
        if similarity_breakdown['text_similarity'] > 0.5:
            reasons.append("é¢˜ç›®å†…å®¹é«˜åº¦ç›¸ä¼¼")
        elif similarity_breakdown['text_similarity'] > 0.3:
            reasons.append("é¢˜ç›®å†…å®¹éƒ¨åˆ†ç›¸ä¼¼")
        
        if similarity_breakdown['type_similarity'] == 1.0:
            reasons.append("é¢˜å‹å®Œå…¨ä¸€è‡´")
        elif similarity_breakdown['type_similarity'] > 0.5:
            reasons.append("é¢˜å‹ç›¸å…³")
        
        if similarity_breakdown['difficulty_similarity'] == 1.0:
            reasons.append("éš¾åº¦ç­‰çº§ç›¸åŒ")
        elif similarity_breakdown['difficulty_similarity'] > 0.5:
            reasons.append("éš¾åº¦ç­‰çº§æ¥è¿‘")
        
        if similarity_breakdown['subject_similarity'] == 1.0:
            reasons.append("åŒä¸€å­¦ç§‘")
        elif similarity_breakdown['subject_similarity'] > 0.5:
            reasons.append("ç›¸å…³å­¦ç§‘")
        
        return reasons
    
    def _get_question_hash(self, question: Dict[str, Any]) -> str:
        """ç”Ÿæˆé¢˜ç›®å“ˆå¸Œå€¼"""
        key_content = f"{question.get('stem', '')}{question.get('correct_answer', '')}"
        return hashlib.md5(key_content.encode()).hexdigest()[:8]
    
    def get_search_statistics(self) -> Dict[str, Any]:
        """è·å–æœç´¢ç»Ÿè®¡ä¿¡æ¯"""
        cache_hit_rate = (
            self.search_stats['cache_hits'] / max(self.search_stats['total_searches'], 1)
        ) * 100
        
        return {
            'total_searches': self.search_stats['total_searches'],
            'cache_hits': self.search_stats['cache_hits'],
            'cache_hit_rate_percent': round(cache_hit_rate, 2),
            'vector_searches': self.search_stats['vector_searches'],
            'avg_search_time_ms': round(self.search_stats['avg_search_time'] * 1000, 2),
            'indexed_questions': self.search_stats['indexed_questions'],
            'cache_size': len(self.similarity_cache)
        }
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if cache_key not in self.similarity_cache:
            return False
        
        # æ£€æŸ¥ç¼“å­˜æ—¶é—´æˆ³
        if cache_key in self.cache_timestamps:
            cache_time = self.cache_timestamps[cache_key]
            current_time = datetime.now().timestamp()
            if current_time - cache_time > self.cache_ttl:
                # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                del self.similarity_cache[cache_key]
                del self.cache_timestamps[cache_key]
                return False
        
        return True
    
    def _cache_result(self, cache_key: str, result: List[Dict[str, Any]]):
        """ç¼“å­˜æœç´¢ç»“æœ"""
        # æ£€æŸ¥ç¼“å­˜å¤§å°é™åˆ¶
        if len(self.similarity_cache) >= self.max_cache_size:
            # åˆ é™¤æœ€æ—§çš„ç¼“å­˜æ¡ç›®
            oldest_key = min(self.cache_timestamps.keys(), 
                           key=lambda k: self.cache_timestamps[k])
            del self.similarity_cache[oldest_key]
            del self.cache_timestamps[oldest_key]
        
        # æ·»åŠ æ–°ç¼“å­˜
        self.similarity_cache[cache_key] = result
        self.cache_timestamps[cache_key] = datetime.now().timestamp()
    
    def clear_cache(self):
        """æ¸…ç©ºç›¸ä¼¼åº¦ç¼“å­˜"""
        self.similarity_cache.clear()
        self.cache_timestamps.clear()
        logger.info("ç›¸ä¼¼åº¦ç¼“å­˜å·²æ¸…ç©º")
    
    def save_model(self):
        """ä¿å­˜æ¨¡å‹å’Œç´¢å¼•"""
        model_data = {
            'similarity_weights': self.similarity_weights,
            'search_stats': self.search_stats,
            'question_index': self.question_index,
            'index_to_question': self.index_to_question,
            'version': '1.0.0',
            'created_at': datetime.now().isoformat()
        }
        
        save_path = os.path.join(self.model_path, 'similarity_search.json')
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(model_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç›¸ä¼¼åº¦æœç´¢æ¨¡å‹å·²ä¿å­˜: {save_path}")
        return save_path
    
    def _load_models(self):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        model_path = os.path.join(self.model_path, 'similarity_search.json')
        
        if os.path.exists(model_path):
            try:
                with open(model_path, 'r', encoding='utf-8') as f:
                    model_data = json.load(f)
                
                if 'similarity_weights' in model_data:
                    self.similarity_weights.update(model_data['similarity_weights'])
                
                if 'search_stats' in model_data:
                    self.search_stats.update(model_data['search_stats'])
                
                logger.info(f"å·²åŠ è½½ç›¸ä¼¼åº¦æœç´¢æ¨¡å‹: {model_path}")
            except Exception as e:
                logger.warning(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")


def test_similarity_search():
    """æµ‹è¯•ç›¸ä¼¼åº¦æœç´¢å¼•æ“"""
    print("ğŸ§ª æµ‹è¯•åŸºäºè¯­ä¹‰çš„é¢˜ç›®ç›¸ä¼¼åº¦è®¡ç®—ç³»ç»Ÿ...")
    
    # åˆ›å»ºæœç´¢å¼•æ“
    search_engine = SimilaritySearchEngine()
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_questions = [
        {
            'question_id': 'Q1',
            'stem': 'è§£ä¸€å…ƒä¸€æ¬¡æ–¹ç¨‹ï¼š2x + 3 = 7ï¼Œæ±‚xçš„å€¼',
            'correct_answer': 'x = 2',
            'question_type': 'calculation',
            'difficulty_level': 2,
            'subject': 'math'
        },
        {
            'question_id': 'Q2',
            'stem': 'è§£æ–¹ç¨‹ï¼š3x - 5 = 10ï¼Œæ±‚xçš„å€¼',
            'correct_answer': 'x = 5',
            'question_type': 'calculation',
            'difficulty_level': 2,
            'subject': 'math'
        },
        {
            'question_id': 'Q3',
            'stem': 'è®¡ç®—ä¸‰è§’å½¢çš„é¢ç§¯ï¼Œå·²çŸ¥åº•è¾¹ä¸º6ç±³ï¼Œé«˜ä¸º4ç±³',
            'correct_answer': '12å¹³æ–¹ç±³',
            'question_type': 'calculation',
            'difficulty_level': 2,
            'subject': 'math'
        },
        {
            'question_id': 'Q4',
            'stem': 'é€‰æ‹©æ­£ç¡®ç­”æ¡ˆï¼šä¸‹åˆ—å“ªä¸ªæ˜¯è´¨æ•°ï¼ŸA.4 B.6 C.7 D.8',
            'correct_answer': 'C',
            'question_type': 'single_choice',
            'difficulty_level': 1,
            'subject': 'math'
        },
        {
            'question_id': 'Q5',
            'stem': 'è§£ä¸€å…ƒäºŒæ¬¡æ–¹ç¨‹ï¼šxÂ² - 5x + 6 = 0',
            'correct_answer': 'x = 2 æˆ– x = 3',
            'question_type': 'calculation',
            'difficulty_level': 4,
            'subject': 'math'
        }
    ]
    
    # æ„å»ºç´¢å¼•
    print("ğŸ—ï¸ æ„å»ºé¢˜ç›®ç´¢å¼•...")
    index_result = search_engine.build_question_index(test_questions)
    if index_result['success']:
        print(f"âœ… æˆåŠŸç´¢å¼• {index_result['indexed_questions']} ä¸ªé¢˜ç›®")
        print(f"   æ„å»ºæ—¶é—´: {index_result['build_time_seconds']:.3f} ç§’")
        print(f"   å‘é‡ç»´åº¦: {index_result['vector_dimensions']}")
    else:
        print(f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {index_result['error']}")
        return
    
    # æµ‹è¯•ç›¸ä¼¼åº¦æœç´¢
    print("\nğŸ” æµ‹è¯•ç›¸ä¼¼åº¦æœç´¢...")
    
    query_question = {
        'stem': 'æ±‚è§£æ–¹ç¨‹ï¼š4x + 1 = 9ï¼Œxç­‰äºå¤šå°‘ï¼Ÿ',
        'correct_answer': 'x = 2',
        'question_type': 'calculation',
        'difficulty_level': 2,
        'subject': 'math'
    }
    
    print(f"æŸ¥è¯¢é¢˜ç›®: {query_question['stem']}")
    
    similar_questions = search_engine.find_similar_questions(
        query_question, 
        top_k=3, 
        similarity_threshold=0.2
    )
    
    print(f"\nğŸ“Š æ‰¾åˆ° {len(similar_questions)} ä¸ªç›¸ä¼¼é¢˜ç›®:")
    for result in similar_questions:
        print(f"\n{result['rank']}. ç›¸ä¼¼åº¦: {result['similarity_score']:.3f}")
        print(f"   é¢˜ç›®: {result['question']['stem']}")
        print(f"   ç­”æ¡ˆ: {result['question']['correct_answer']}")
        print(f"   åŒ¹é…åŸå› : {', '.join(result['match_reasons'])}")
        print(f"   ç›¸ä¼¼åº¦åˆ†è§£: {result['similarity_breakdown']}")
    
    # æ˜¾ç¤ºæœç´¢ç»Ÿè®¡
    stats = search_engine.get_search_statistics()
    print(f"\nğŸ“ˆ æœç´¢ç»Ÿè®¡:")
    print(f"   æ€»æœç´¢æ¬¡æ•°: {stats['total_searches']}")
    print(f"   å¹³å‡æœç´¢æ—¶é—´: {stats['avg_search_time_ms']:.2f} ms")
    print(f"   ç´¢å¼•é¢˜ç›®æ•°: {stats['indexed_questions']}")
    print(f"   ç¼“å­˜å‘½ä¸­ç‡: {stats['cache_hit_rate_percent']:.1f}%")
    
    print("âœ… æµ‹è¯•å®Œæˆ - ç›¸ä¼¼åº¦æœç´¢å¼•æ“å·¥ä½œæ­£å¸¸")
    return search_engine


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_similarity_search()
