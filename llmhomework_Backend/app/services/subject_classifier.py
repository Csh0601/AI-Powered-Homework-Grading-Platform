#!/usr/bin/env python3
"""
æ™ºèƒ½å­¦ç§‘åˆ†ç±»å™¨
åŸºäºæ–‡æœ¬å†…å®¹è‡ªåŠ¨è¯†åˆ«é¢˜ç›®æ‰€å±å­¦ç§‘

ä¸»è¦åŠŸèƒ½ï¼š
1. åŸºäºå…³é”®è¯å’Œè§„åˆ™çš„åˆ†ç±»
2. TextCNNæ·±åº¦å­¦ä¹ åˆ†ç±»æ¨¡å‹
3. åˆ†ç±»ç½®ä¿¡åº¦è¯„ä¼°
4. å¤šå±‚çº§åˆ†ç±»ç­–ç•¥

æ”¯æŒå­¦ç§‘ï¼š
- æ•°å­¦ (math)
- è¯­æ–‡ (chinese) 
- è‹±è¯­ (english)
- ç‰©ç† (physics)
- åŒ–å­¦ (chemistry)
- ç”Ÿç‰© (biology)
- å†å² (history)
- åœ°ç† (geography)
- æ”¿æ²» (politics)
"""

import re
import jieba
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional
from collections import Counter, defaultdict
import pickle
import os
import json
from datetime import datetime
import logging

# æ·±åº¦å­¦ä¹ ç›¸å…³
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.utils.data import DataLoader, Dataset
    from sklearn.model_selection import train_test_split
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics import classification_report, confusion_matrix
    DEEP_LEARNING_AVAILABLE = True
except ImportError:
    DEEP_LEARNING_AVAILABLE = False
    print("âš ï¸ æ·±åº¦å­¦ä¹ åº“æœªå®‰è£…ï¼Œä»…ä½¿ç”¨è§„åˆ™åˆ†ç±»")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def jieba_tokenizer(text):
    """ç‹¬ç«‹çš„åˆ†è¯å‡½æ•°ï¼Œç”¨äºTfidfVectorizer"""
    return list(jieba.cut(text))

class SubjectClassifier:
    """æ™ºèƒ½å­¦ç§‘åˆ†ç±»å™¨"""
    
    def __init__(self, model_path: str = None):
        """åˆå§‹åŒ–åˆ†ç±»å™¨"""
        self.model_path = model_path or os.path.join(
            os.path.dirname(__file__), '..', '..', 'models', 'subject_classifier'
        )
        
        # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
        os.makedirs(self.model_path, exist_ok=True)
        
        # å­¦ç§‘å®šä¹‰
        self.subjects = {
            'math': 'æ•°å­¦',
            'chinese': 'è¯­æ–‡', 
            'english': 'è‹±è¯­',
            'physics': 'ç‰©ç†',
            'chemistry': 'åŒ–å­¦',
            'biology': 'ç”Ÿç‰©',
            'history': 'å†å²',
            'geography': 'åœ°ç†',
            'politics': 'æ”¿æ²»'
        }
        
        # åˆå§‹åŒ–åˆ†ç±»å™¨ç»„ä»¶
        self._init_keyword_classifier()
        self._init_rule_patterns()
        
        # æ·±åº¦å­¦ä¹ æ¨¡å‹
        self.textcnn_model = None
        self.vectorizer = None
        self.label_encoder = None
        
        # å°è¯•åŠ è½½å·²è®­ç»ƒçš„TextCNNæ¨¡å‹
        self._load_textcnn_model()
        
        # åˆ†ç±»ç»Ÿè®¡
        self.classification_stats = {
            'total_classifications': 0,
            'method_usage': {
                'keyword': 0,
                'rule': 0,
                'textcnn': 0,
                'ensemble': 0,
                'fallback': 0
            },
            'confidence_distribution': defaultdict(int)
        }
    
    def _init_keyword_classifier(self):
        """åˆå§‹åŒ–å…³é”®è¯åˆ†ç±»å™¨"""
        self.subject_keywords = {
            'math': {
                # åŸºç¡€æ•°å­¦æ¦‚å¿µ
                'basic': ['æ•°å­¦', 'è®¡ç®—', 'ä»£æ•°', 'å‡ ä½•', 'å‡½æ•°', 'æ–¹ç¨‹', 'ä¸ç­‰å¼', 'æ¦‚ç‡', 'ç»Ÿè®¡'],
                # è¿ç®—ç¬¦å·
                'operators': ['åŠ ', 'å‡', 'ä¹˜', 'é™¤', 'ç­‰äº', 'å¤§äº', 'å°äº', 'å¹³æ–¹', 'å¼€æ–¹', 'ç»å¯¹å€¼'],
                # å‡ ä½•å›¾å½¢
                'geometry': ['ä¸‰è§’å½¢', 'å››è¾¹å½¢', 'åœ†', 'æ­£æ–¹å½¢', 'é•¿æ–¹å½¢', 'å¹³è¡Œå››è¾¹å½¢', 'æ¢¯å½¢', 
                           'ç‚¹', 'çº¿', 'é¢', 'è§’', 'å¼§', 'åŠå¾„', 'ç›´å¾„', 'å‘¨é•¿', 'é¢ç§¯', 'ä½“ç§¯'],
                # ä»£æ•°æ¦‚å¿µ
                'algebra': ['æœªçŸ¥æ•°', 'ç³»æ•°', 'å¸¸æ•°', 'å˜é‡', 'è§£', 'æ ¹', 'å› å¼åˆ†è§£', 'é…æ–¹'],
                # æ•°å­—å’Œå•ä½
                'numbers': ['æ•´æ•°', 'åˆ†æ•°', 'å°æ•°', 'ç™¾åˆ†æ¯”', 'æ¯”ä¾‹', 'ç‡'],
                # ç‰¹æ®Šç¬¦å·è¯†åˆ«
                'symbols': ['Ï€', 'âˆ', 'Â°', 'âˆ ', 'â–³', 'â–¡', 'â—‹', 'â‰ˆ', 'â‰ ', 'â‰¤', 'â‰¥']
            },
            
            'chinese': {
                'literary': ['æ–‡å­¦', 'è¯—æ­Œ', 'æ•£æ–‡', 'å°è¯´', 'æˆå‰§', 'å¤è¯—', 'ç°ä»£æ–‡', 'æ–‡è¨€æ–‡'],
                'grammar': ['è¯­æ³•', 'è¯æ±‡', 'å¥å¼', 'ä¿®è¾', 'è¯­è¨€', 'æ–‡å­—', 'æ±‰å­—', 'æ‹¼éŸ³'],
                'writing': ['ä½œæ–‡', 'å†™ä½œ', 'è¡¨è¾¾', 'é˜…è¯»', 'ç†è§£', 'åˆ†æ', 'é‰´èµ'],
                'classical': ['å¤ä»£', 'æ–‡è¨€', 'è¯—è¯', 'å”è¯—', 'å®‹è¯', 'å…ƒæ›²', 'å¤æ–‡'],
                'authors': ['é²è¿…', 'è€èˆ', 'å·´é‡‘', 'èŒ…ç›¾', 'æ²ˆä»æ–‡', 'æœ±è‡ªæ¸…', 'å†°å¿ƒ']
            },
            
            'english': {
                'grammar': ['grammar', 'tense', 'verb', 'noun', 'adjective', 'adverb'],
                'vocabulary': ['vocabulary', 'word', 'phrase', 'idiom', 'expression'],
                'skills': ['reading', 'writing', 'listening', 'speaking', 'translation'],
                'basic': ['english', 'è‹±è¯­', 'è‹±æ–‡', 'å•è¯', 'è¯­æ³•', 'æ—¶æ€', 'å¥å‹']
            },
            
            'physics': {
                'mechanics': ['åŠ›å­¦', 'è¿åŠ¨', 'é€Ÿåº¦', 'åŠ é€Ÿåº¦', 'åŠ›', 'è´¨é‡', 'é‡åŠ›', 'æ‘©æ“¦'],
                'electricity': ['ç”µå­¦', 'ç”µæµ', 'ç”µå‹', 'ç”µé˜»', 'åŠŸç‡', 'ç”µè·¯', 'ç£åœº'],
                'optics': ['å…‰å­¦', 'å…‰çº¿', 'åå°„', 'æŠ˜å°„', 'é€é•œ', 'å‡¸é€é•œ', 'å‡¹é€é•œ'],
                'thermodynamics': ['çƒ­å­¦', 'æ¸©åº¦', 'çƒ­é‡', 'æ¯”çƒ­å®¹', 'å†…èƒ½', 'çƒ­æœº'],
                'units': ['ç‰›é¡¿', 'ç„¦è€³', 'ç“¦ç‰¹', 'å®‰åŸ¹', 'ä¼ç‰¹', 'æ¬§å§†', 'ç±³/ç§’']
            },
            
            'chemistry': {
                'elements': ['å…ƒç´ ', 'åŸå­', 'åˆ†å­', 'ç¦»å­', 'è´¨å­', 'ä¸­å­', 'ç”µå­'],
                'compounds': ['åŒ–åˆç‰©', 'æ°§åŒ–ç‰©', 'é…¸', 'ç¢±', 'ç›', 'æœ‰æœºç‰©', 'æ— æœºç‰©'],
                'reactions': ['åŒ–å­¦ååº”', 'æ°§åŒ–', 'è¿˜åŸ', 'ç‡ƒçƒ§', 'ä¸­å’Œ', 'ç”µè§£'],
                'formulas': ['åŒ–å­¦å¼', 'åˆ†å­å¼', 'ç¦»å­æ–¹ç¨‹å¼', 'åŒ–å­¦æ–¹ç¨‹å¼'],
                'common': ['æ°¢', 'æ°§', 'ç¢³', 'æ°®', 'ç¡«', 'ç£·', 'æ°¯', 'é’ ', 'é’¾', 'é’™', 'é“']
            },
            
            'biology': {
                'cells': ['ç»†èƒ', 'ç»†èƒè†œ', 'ç»†èƒå£', 'ç»†èƒæ ¸', 'ç»†èƒè´¨', 'çº¿ç²’ä½“'],
                'organisms': ['ç”Ÿç‰©', 'æ¤ç‰©', 'åŠ¨ç‰©', 'å¾®ç”Ÿç‰©', 'ç»†èŒ', 'ç—…æ¯’'],
                'systems': ['æ¶ˆåŒ–ç³»ç»Ÿ', 'å‘¼å¸ç³»ç»Ÿ', 'å¾ªç¯ç³»ç»Ÿ', 'ç¥ç»ç³»ç»Ÿ', 'å†…åˆ†æ³Œç³»ç»Ÿ'],
                'genetics': ['é—ä¼ ', 'åŸºå› ', 'DNA', 'RNA', 'æŸ“è‰²ä½“', 'æ€§çŠ¶'],
                'ecology': ['ç”Ÿæ€', 'ç¯å¢ƒ', 'ç”Ÿæ€ç³»ç»Ÿ', 'é£Ÿç‰©é“¾', 'é£Ÿç‰©ç½‘']
            },
            
            'history': {
                'periods': ['å¤ä»£', 'è¿‘ä»£', 'ç°ä»£', 'æœä»£', 'ç‹æœ', 'å¸å›½', 'å…±å’Œ'],
                'events': ['æˆ˜äº‰', 'é©å‘½', 'æ”¹é©', 'èµ·ä¹‰', 'è¿åŠ¨', 'æ¡çº¦', 'åå®š'],
                'figures': ['çš‡å¸', 'å°†å†›', 'æ”¿æ²»å®¶', 'æ€æƒ³å®¶', 'æ–‡å­¦å®¶'],
                'chronology': ['å¹´ä»£', 'ä¸–çºª', 'å…¬å…ƒ', 'å†å²', 'å²å­¦', 'å²æ–™']
            },
            
            'geography': {
                'physical': ['åœ°ç†', 'åœ°å½¢', 'åœ°è²Œ', 'å±±è„‰', 'æ²³æµ', 'æ¹–æ³Š', 'æµ·æ´‹', 'å¹³åŸ'],
                'climate': ['æ°”å€™', 'æ¸©åº¦', 'é™æ°´', 'é£', 'å­£èŠ‚', 'æ°”æ¸©', 'æ¹¿åº¦'],
                'regions': ['äºšæ´²', 'æ¬§æ´²', 'éæ´²', 'ç¾æ´²', 'å¤§æ´‹æ´²', 'ä¸­å›½', 'çœä»½'],
                'resources': ['èµ„æº', 'çŸ¿äº§', 'çŸ³æ²¹', 'ç…¤ç‚­', 'æ°´èµ„æº', 'åœŸåœ°']
            },
            
            'politics': {
                'concepts': ['æ”¿æ²»', 'å›½å®¶', 'æ”¿åºœ', 'æ³•å¾‹', 'å®ªæ³•', 'æƒåˆ©', 'ä¹‰åŠ¡'],
                'systems': ['æ°‘ä¸»', 'ä¸“åˆ¶', 'å…±å’Œ', 'è”é‚¦', 'è®®ä¼š', 'é€‰ä¸¾'],
                'ideology': ['é©¬å…‹æ€ä¸»ä¹‰', 'ç¤¾ä¼šä¸»ä¹‰', 'èµ„æœ¬ä¸»ä¹‰', 'æ€æƒ³', 'ç†è®º'],
                'current': ['æ—¶äº‹', 'æ”¿ç­–', 'å¤–äº¤', 'å›½é™…å…³ç³»', 'è”åˆå›½']
            }
        }
    
    def _init_rule_patterns(self):
        """åˆå§‹åŒ–è§„åˆ™æ¨¡å¼"""
        self.rule_patterns = {
            'math': [
                r'è®¡ç®—.*?çš„å€¼',
                r'æ±‚.*?çš„.*?(é¢ç§¯|å‘¨é•¿|ä½“ç§¯|é•¿åº¦)',
                r'è§£.*?æ–¹ç¨‹',
                r'è¯æ˜.*?(ä¸‰è§’å½¢|å››è¾¹å½¢|åœ†)',
                r'\d+[+\-Ã—Ã·]\d+',
                r'è®¾.*?ä¸º.*?ï¼Œæ±‚',
                r'å·²çŸ¥.*?æ±‚.*?',
                r'å‡½æ•°.*?çš„.*?(å®šä¹‰åŸŸ|å€¼åŸŸ|å•è°ƒæ€§)',
                r'æ¦‚ç‡.*?æ˜¯å¤šå°‘'
            ],
            
            'chinese': [
                r'é˜…è¯».*?æ–‡ç« ',
                r'åˆ†æ.*?(äººç‰©|æƒ…èŠ‚|ä¸»é¢˜)',
                r'è¯—æ­Œ.*?(èµæ|ç†è§£)',
                r'æ–‡è¨€æ–‡.*?ç¿»è¯‘',
                r'ä½œè€….*?(æƒ³è¦è¡¨è¾¾|æƒ…æ„Ÿ)',
                r'ä¿®è¾.*?æ‰‹æ³•',
                r'è¯­è¨€.*?ç‰¹è‰²'
            ],
            
            'english': [
                r'translate.*?into',
                r'choose.*?correct.*?answer',
                r'complete.*?sentences?',
                r'what.*?does.*?mean',
                r'grammar.*?exercise',
                r'vocabulary.*?test'
            ],
            
            'physics': [
                r'ç‰©ä½“.*?è¿åŠ¨',
                r'ç”µè·¯.*?å›¾',
                r'å…‰.*?(åå°„|æŠ˜å°„)',
                r'åŠ›.*?ä½œç”¨',
                r'èƒ½é‡.*?è½¬æ¢',
                r'æ¸©åº¦.*?å˜åŒ–'
            ],
            
            'chemistry': [
                r'åŒ–å­¦.*?æ–¹ç¨‹å¼',
                r'å…ƒç´ .*?åŒ–åˆä»·',
                r'ååº”.*?äº§ç‰©',
                r'æº¶æ¶².*?æµ“åº¦',
                r'æ°”ä½“.*?ä½“ç§¯',
                r'åŸå­.*?ç»“æ„'
            ],
            
            'biology': [
                r'ç»†èƒ.*?ç»“æ„',
                r'ç”Ÿç‰©.*?ç‰¹å¾',
                r'é—ä¼ .*?è§„å¾‹',
                r'ç”Ÿæ€.*?ç³»ç»Ÿ',
                r'å™¨å®˜.*?åŠŸèƒ½',
                r'è¿›åŒ–.*?è¿‡ç¨‹'
            ],
            
            'history': [
                r'\d+å¹´.*?äº‹ä»¶',
                r'å†å².*?æ„ä¹‰',
                r'æœä»£.*?æ›´æ›¿',
                r'æˆ˜äº‰.*?ç»“æœ',
                r'æ”¹é©.*?å½±å“',
                r'æ–‡åŒ–.*?äº¤æµ'
            ],
            
            'geography': [
                r'åœ°ç†.*?ä½ç½®',
                r'æ°”å€™.*?ç‰¹å¾',
                r'åœ°å½¢.*?ç‰¹ç‚¹',
                r'èµ„æº.*?åˆ†å¸ƒ',
                r'äººå£.*?åˆ†å¸ƒ',
                r'ç»æµ.*?å‘å±•'
            ],
            
            'politics': [
                r'æ”¿æ²».*?åˆ¶åº¦',
                r'æ³•å¾‹.*?æ¡æ–‡',
                r'æƒåˆ©.*?ä¹‰åŠ¡',
                r'å›½å®¶.*?æ€§è´¨',
                r'æ”¿åºœ.*?èŒèƒ½',
                r'æ°‘ä¸».*?åˆ¶åº¦'
            ]
        }
    
    def classify_by_keywords(self, text: str) -> Tuple[str, float]:
        """åŸºäºå…³é”®è¯åˆ†ç±»"""
        text = text.lower()
        scores = defaultdict(float)
        
        for subject, categories in self.subject_keywords.items():
            for category, keywords in categories.items():
                for keyword in keywords:
                    keyword_lower = keyword.lower()
                    count = text.count(keyword_lower)
                    if count > 0:
                        # æ ¹æ®å…³é”®è¯ç±»åˆ«ç»™äºˆä¸åŒæƒé‡
                        weight = self._get_keyword_weight(category)
                        scores[subject] += count * weight
        
        if not scores:
            return 'unknown', 0.0
        
        best_subject = max(scores, key=scores.get)
        # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºæœ€é«˜åˆ†ä¸æ¬¡é«˜åˆ†çš„å·®è·ï¼‰
        sorted_scores = sorted(scores.values(), reverse=True)
        if len(sorted_scores) >= 2:
            confidence = (sorted_scores[0] - sorted_scores[1]) / sorted_scores[0]
        else:
            confidence = scores[best_subject] / (scores[best_subject] + 1)
        
        confidence = min(confidence, 0.95)  # æœ€å¤§ç½®ä¿¡åº¦é™åˆ¶
        return best_subject, confidence
    
    def classify_by_rules(self, text: str) -> Tuple[str, float]:
        """åŸºäºè§„åˆ™æ¨¡å¼åˆ†ç±»"""
        scores = defaultdict(int)
        
        for subject, patterns in self.rule_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                scores[subject] += len(matches)
        
        if not scores:
            return 'unknown', 0.0
        
        best_subject = max(scores, key=scores.get)
        max_score = scores[best_subject]
        total_score = sum(scores.values())
        
        confidence = max_score / total_score if total_score > 0 else 0.0
        confidence = min(confidence, 0.90)  # è§„åˆ™åˆ†ç±»çš„æœ€å¤§ç½®ä¿¡åº¦
        
        return best_subject, confidence
    
    def _get_keyword_weight(self, category: str) -> float:
        """è·å–å…³é”®è¯ç±»åˆ«æƒé‡"""
        weights = {
            'basic': 1.0,
            'operators': 1.2,
            'geometry': 1.1,
            'algebra': 1.1,
            'symbols': 1.5,
            'literary': 1.2,
            'grammar': 1.0,
            'classical': 1.3,
            'mechanics': 1.2,
            'electricity': 1.2,
            'elements': 1.3,
            'compounds': 1.1,
            'cells': 1.2,
            'organisms': 1.0,
            'periods': 1.1,
            'events': 1.2,
            'physical': 1.1,
            'climate': 1.2,
            'concepts': 1.0,
            'systems': 1.1
        }
        return weights.get(category, 1.0)
    
    def preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™æ•°å­¦ç¬¦å·ï¼‰
        math_symbols = ['Ï€', 'âˆ', 'Â°', 'âˆ ', 'â–³', 'â–¡', 'â—‹', 'â‰ˆ', 'â‰ ', 'â‰¤', 'â‰¥', '+', '-', 'Ã—', 'Ã·', '=']
        
        # æ ‡å‡†åŒ–æ–‡æœ¬
        text = re.sub(r'\s+', ' ', text)  # ç»Ÿä¸€ç©ºç™½å­—ç¬¦
        text = text.strip()
        
        # åˆ†è¯
        words = jieba.cut(text)
        processed_text = ' '.join(words)
        
        return processed_text
    
    def classify(self, text: str, use_ensemble: bool = True) -> Dict[str, Any]:
        """
        ä¸»åˆ†ç±»æ–¹æ³•
        
        Args:
            text: å¾…åˆ†ç±»æ–‡æœ¬
            use_ensemble: æ˜¯å¦ä½¿ç”¨é›†æˆæ–¹æ³•
            
        Returns:
            åˆ†ç±»ç»“æœå­—å…¸
        """
        if not text or not text.strip():
            return {
                'subject': 'unknown',
                'confidence': 0.0,
                'method': 'fallback',
                'details': {'error': 'æ–‡æœ¬ä¸ºç©º'}
            }
        
        # é¢„å¤„ç†æ–‡æœ¬
        processed_text = self.preprocess_text(text)
        
        # åˆ†ç±»ç»“æœ
        results = {}
        
        # 1. å…³é”®è¯åˆ†ç±»
        keyword_subject, keyword_conf = self.classify_by_keywords(processed_text)
        results['keyword'] = (keyword_subject, keyword_conf)
        
        # 2. è§„åˆ™åˆ†ç±»
        rule_subject, rule_conf = self.classify_by_rules(processed_text)
        results['rule'] = (rule_subject, rule_conf)
        
        # 3. TextCNNåˆ†ç±»ï¼ˆå¦‚æœæ¨¡å‹å¯ç”¨ï¼‰
        if DEEP_LEARNING_AVAILABLE and self.textcnn_model:
            try:
                textcnn_subject, textcnn_conf = self.classify_by_textcnn(processed_text)
                results['textcnn'] = (textcnn_subject, textcnn_conf)
            except Exception as e:
                logger.warning(f"TextCNNåˆ†ç±»å¤±è´¥: {e}")
                results['textcnn'] = ('unknown', 0.0)
        else:
            results['textcnn'] = ('unknown', 0.0)
        
        # 4. é›†æˆå†³ç­–
        if use_ensemble:
            final_subject, final_confidence, method = self._ensemble_decision(results)
        else:
            # ä½¿ç”¨æœ€é«˜ç½®ä¿¡åº¦çš„ç»“æœ
            best_result = max(results.items(), key=lambda x: x[1][1])
            final_subject, final_confidence = best_result[1]
            method = best_result[0]
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.classification_stats['total_classifications'] += 1
        self.classification_stats['method_usage'][method] += 1
        self.classification_stats['confidence_distribution'][round(final_confidence, 1)] += 1
        
        return {
            'subject': final_subject,
            'subject_name': self.subjects.get(final_subject, 'æœªçŸ¥'),
            'confidence': round(final_confidence, 3),
            'method': method,
            'details': {
                'keyword': {'subject': results['keyword'][0], 'confidence': round(results['keyword'][1], 3)},
                'rule': {'subject': results['rule'][0], 'confidence': round(results['rule'][1], 3)},
                'textcnn': {'subject': results['textcnn'][0], 'confidence': round(results['textcnn'][1], 3)},
                'text_length': len(text),
                'processed_length': len(processed_text)
            }
        }
    
    def _load_textcnn_model(self):
        """åŠ è½½å·²è®­ç»ƒçš„TextCNNæ¨¡å‹"""
        if not DEEP_LEARNING_AVAILABLE:
            return
        
        try:
            import pickle
            import torch
            
            model_dir = os.path.join(self.model_path, '..', '..', 'models', 'subject_classifier')
            model_path = os.path.join(model_dir, 'textcnn_model.pth')
            vectorizer_path = os.path.join(model_dir, 'vectorizer.pkl')
            label_encoder_path = os.path.join(model_dir, 'label_encoder.pkl')
            
            if all(os.path.exists(p) for p in [model_path, vectorizer_path, label_encoder_path]):
                # åŠ è½½å‘é‡åŒ–å™¨å’Œæ ‡ç­¾ç¼–ç å™¨
                with open(vectorizer_path, 'rb') as f:
                    self.vectorizer = pickle.load(f)
                
                with open(label_encoder_path, 'rb') as f:
                    self.label_encoder = pickle.load(f)
                
                # åˆå§‹åŒ–æ¨¡å‹ç»“æ„
                vocab_size = 5000
                embed_dim = 128
                num_classes = len(self.label_encoder.classes_)
                
                self.textcnn_model = TextCNN(
                    vocab_size=vocab_size,
                    embed_dim=embed_dim,
                    num_classes=num_classes,
                    kernel_sizes=[3, 4, 5],
                    num_filters=100
                )
                
                # åŠ è½½æ¨¡å‹æƒé‡
                self.textcnn_model.load_state_dict(torch.load(model_path, map_location='cpu'))
                self.textcnn_model.eval()
                
                logger.info("âœ… TextCNNæ¨¡å‹åŠ è½½æˆåŠŸ")
            else:
                logger.info("âš ï¸ TextCNNæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦å…ˆè®­ç»ƒæ¨¡å‹")
        
        except Exception as e:
            logger.warning(f"TextCNNæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.textcnn_model = None
            self.vectorizer = None
            self.label_encoder = None
    
    def classify_by_textcnn(self, text: str) -> Tuple[str, float]:
        """ä½¿ç”¨TextCNNæ¨¡å‹åˆ†ç±»"""
        if not DEEP_LEARNING_AVAILABLE or not self.textcnn_model or not self.vectorizer:
            return 'unknown', 0.0
        
        try:
            import torch
            import numpy as np
            
            # æ–‡æœ¬å‘é‡åŒ–
            text_vector = self.vectorizer.transform([text]).toarray()[0]
            
            # è½¬æ¢ä¸ºåºåˆ—ï¼ˆå–å‰128ä¸ªæœ€é‡è¦çš„ç‰¹å¾ï¼‰
            max_length = 128
            sequence = np.argsort(text_vector)[-max_length:][::-1].copy()  # æ·»åŠ .copy()è§£å†³è´Ÿæ­¥é•¿é—®é¢˜
            
            # å¡«å……æˆ–æˆªæ–­åˆ°å›ºå®šé•¿åº¦
            if len(sequence) < max_length:
                sequence = np.pad(sequence, (0, max_length - len(sequence)), 'constant', constant_values=0)
            
            # è½¬æ¢ä¸ºtensorå¹¶é¢„æµ‹
            input_tensor = torch.tensor(sequence, dtype=torch.long).unsqueeze(0)
            
            with torch.no_grad():
                output = self.textcnn_model(input_tensor)
                probabilities = torch.softmax(output, dim=1)
                confidence, predicted_idx = torch.max(probabilities, 1)
                
                # è½¬æ¢å›å­¦ç§‘æ ‡ç­¾
                predicted_label = self.label_encoder.inverse_transform([predicted_idx.item()])[0]
                confidence_score = confidence.item()
                
                return predicted_label, confidence_score
        
        except Exception as e:
            logger.error(f"TextCNNåˆ†ç±»é”™è¯¯: {e}")
            return 'unknown', 0.0

    def _ensemble_decision(self, results: Dict[str, Tuple[str, float]]) -> Tuple[str, float, str]:
        """é›†æˆå†³ç­–ç®—æ³•"""
        # æ–¹æ³•æƒé‡
        method_weights = {
            'keyword': 0.4,
            'rule': 0.3,
            'textcnn': 0.3
        }
        
        # è®¡ç®—åŠ æƒåˆ†æ•°
        subject_scores = defaultdict(float)
        total_weight = 0
        
        for method, (subject, confidence) in results.items():
            if subject != 'unknown' and confidence > 0:
                weight = method_weights.get(method, 0)
                subject_scores[subject] += confidence * weight
                total_weight += weight
        
        if not subject_scores:
            return 'unknown', 0.0, 'fallback'
        
        # æ‰¾å‡ºæœ€é«˜åˆ†å­¦ç§‘
        best_subject = max(subject_scores, key=subject_scores.get)
        best_score = subject_scores[best_subject]
        
        # è®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦
        if total_weight > 0:
            final_confidence = best_score / total_weight
        else:
            final_confidence = 0.0
        
        # ç¡®å®šä¸»è¦è´¡çŒ®æ–¹æ³•
        main_method = 'ensemble'
        for method, (subject, confidence) in results.items():
            if subject == best_subject and confidence > 0.5:
                main_method = method
                break
        
        return best_subject, final_confidence, main_method
    
    def batch_classify(self, texts: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡åˆ†ç±»"""
        results = []
        for text in texts:
            result = self.classify(text)
            results.append(result)
        return results
    
    def get_classification_stats(self) -> Dict[str, Any]:
        """è·å–åˆ†ç±»ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.classification_stats.copy()
        
        if stats['total_classifications'] > 0:
            for method in stats['method_usage']:
                percentage = (stats['method_usage'][method] / stats['total_classifications']) * 100
                stats['method_usage'][method] = {
                    'count': stats['method_usage'][method],
                    'percentage': round(percentage, 2)
                }
        
        return stats
    
    def save_model(self, model_name: str = 'subject_classifier'):
        """ä¿å­˜åˆ†ç±»å™¨æ¨¡å‹"""
        model_data = {
            'subjects': self.subjects,
            'subject_keywords': self.subject_keywords,
            'rule_patterns': self.rule_patterns,
            'classification_stats': self.classification_stats,
            'version': '1.0.0',
            'created_at': datetime.now().isoformat()
        }
        
        save_path = os.path.join(self.model_path, f'{model_name}.json')
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(model_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"åˆ†ç±»å™¨æ¨¡å‹å·²ä¿å­˜: {save_path}")
        return save_path
    
    def load_model(self, model_name: str = 'subject_classifier'):
        """åŠ è½½åˆ†ç±»å™¨æ¨¡å‹"""
        load_path = os.path.join(self.model_path, f'{model_name}.json')
        
        if not os.path.exists(load_path):
            logger.warning(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {load_path}")
            return False
        
        try:
            with open(load_path, 'r', encoding='utf-8') as f:
                model_data = json.load(f)
            
            self.subjects = model_data['subjects']
            self.subject_keywords = model_data['subject_keywords']
            self.rule_patterns = model_data['rule_patterns']
            self.classification_stats = model_data.get('classification_stats', self.classification_stats)
            
            logger.info(f"åˆ†ç±»å™¨æ¨¡å‹å·²åŠ è½½: {load_path}")
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False


# TextCNNæ¨¡å‹å®ç°ï¼ˆå¦‚æœæ·±åº¦å­¦ä¹ åº“å¯ç”¨ï¼‰
if DEEP_LEARNING_AVAILABLE:
    class TextCNN(nn.Module):
        """TextCNNæ¨¡å‹ç”¨äºæ–‡æœ¬åˆ†ç±»"""
        
        def __init__(self, vocab_size: int, embed_dim: int, num_classes: int, 
                     kernel_sizes: List[int] = [3, 4, 5], num_filters: int = 100):
            super(TextCNN, self).__init__()
            
            self.embedding = nn.Embedding(vocab_size, embed_dim)
            self.convs = nn.ModuleList([
                nn.Conv1d(embed_dim, num_filters, kernel_size)
                for kernel_size in kernel_sizes
            ])
            self.dropout = nn.Dropout(0.5)
            self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)
            
        def forward(self, x):
            # x: (batch_size, seq_len)
            x = self.embedding(x)  # (batch_size, seq_len, embed_dim)
            x = x.transpose(1, 2)  # (batch_size, embed_dim, seq_len)
            
            conv_outputs = []
            for conv in self.convs:
                conv_out = F.relu(conv(x))  # (batch_size, num_filters, conv_seq_len)
                pooled = F.max_pool1d(conv_out, conv_out.size(2))  # (batch_size, num_filters, 1)
                conv_outputs.append(pooled.squeeze(2))  # (batch_size, num_filters)
            
            x = torch.cat(conv_outputs, dim=1)  # (batch_size, len(kernel_sizes) * num_filters)
            x = self.dropout(x)
            x = self.fc(x)  # (batch_size, num_classes)
            
            return x


def create_training_data_from_keywords():
    """ä»å…³é”®è¯ç”Ÿæˆè®­ç»ƒæ•°æ®"""
    classifier = SubjectClassifier()
    training_data = []
    
    for subject, categories in classifier.subject_keywords.items():
        for category, keywords in categories.items():
            for keyword in keywords:
                # ç”ŸæˆåŒ…å«å…³é”®è¯çš„ç®€å•å¥å­
                sentences = [
                    f"è¯·è§£é‡Š{keyword}çš„æ¦‚å¿µ",
                    f"å…³äº{keyword}çš„é¢˜ç›®",
                    f"è¿™é“é¢˜è€ƒæŸ¥çš„æ˜¯{keyword}",
                    f"{keyword}ç›¸å…³çŸ¥è¯†ç‚¹",
                    f"è®¡ç®—{keyword}çš„å€¼",
                    f"åˆ†æ{keyword}çš„ç‰¹ç‚¹"
                ]
                
                for sentence in sentences:
                    training_data.append((sentence, subject))
    
    return training_data


def test_classifier():
    """æµ‹è¯•åˆ†ç±»å™¨åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•æ™ºèƒ½å­¦ç§‘åˆ†ç±»å™¨...")
    
    classifier = SubjectClassifier()
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        "è®¡ç®—ä¸‰è§’å½¢çš„é¢ç§¯å…¬å¼",
        "åˆ†æé²è¿…ä½œå“çš„è‰ºæœ¯ç‰¹è‰²",
        "Translate the following sentence into Chinese",
        "åˆ†æç”µè·¯å›¾ä¸­ç”µæµçš„æ–¹å‘",
        "å†™å‡ºæ°§æ°”çš„åŒ–å­¦å¼",
        "ç»†èƒåˆ†è£‚çš„è¿‡ç¨‹",
        "æ˜æœçš„æ”¿æ²»åˆ¶åº¦",
        "ä¸­å›½çš„åœ°ç†ä½ç½®ç‰¹ç‚¹",
        "é©¬å…‹æ€ä¸»ä¹‰åŸºæœ¬åŸç†",
        "è§£æ–¹ç¨‹ç»„ï¼šx+y=5, x-y=1"
    ]
    
    print("\nğŸ“Š åˆ†ç±»ç»“æœ:")
    print("-" * 80)
    
    for i, text in enumerate(test_cases, 1):
        result = classifier.classify(text)
        print(f"{i:2d}. æ–‡æœ¬: {text}")
        print(f"    å­¦ç§‘: {result['subject_name']} ({result['subject']})")
        print(f"    ç½®ä¿¡åº¦: {result['confidence']:.3f}")
        print(f"    æ–¹æ³•: {result['method']}")
        print(f"    è¯¦æƒ…: å…³é”®è¯={result['details']['keyword']['confidence']:.3f}, "
              f"è§„åˆ™={result['details']['rule']['confidence']:.3f}")
        print()
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = classifier.get_classification_stats()
    print("ğŸ“ˆ åˆ†ç±»ç»Ÿè®¡:")
    print(f"æ€»åˆ†ç±»æ¬¡æ•°: {stats['total_classifications']}")
    print("æ–¹æ³•ä½¿ç”¨æƒ…å†µ:")
    for method, data in stats['method_usage'].items():
        if isinstance(data, dict):
            print(f"  {method}: {data['count']} æ¬¡ ({data['percentage']:.1f}%)")
    
    # ä¿å­˜æ¨¡å‹
    model_path = classifier.save_model()
    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    return classifier


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_classifier()
