import json
import re
import requests
import time
from typing import List, Dict, Any
import logging
from ..config import Config

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class QwenService:
    def __init__(self, model_name: str = "qwen2.5-vl"):
        """
        åˆå§‹åŒ– Qwen æœåŠ¡ï¼ˆå·²å¼ƒç”¨ï¼‰- å…¼å®¹æ€§ä¿ç•™
        æ³¨æ„ï¼šæ­¤æœåŠ¡å·²è¢«QwenVLClientæ›¿ä»£ï¼Œå»ºè®®ä½¿ç”¨multimodal_client.py
        
        Args:
            model_name: Qwen æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º qwen2.5-vl
        """
        self.model_name = model_name
        
        # ğŸ¯ ç›´æ¥è°ƒç”¨LoRAé…ç½® - å…¼å®¹æ—§æ¥å£ï¼ˆå·²å¼ƒç”¨ï¼Œå»ºè®®ä½¿ç”¨qwen_vl_direct_serviceï¼‰
        self.use_huggingface = getattr(Config, 'USE_HUGGINGFACE', False)
        self.huggingface_url = getattr(Config, 'HUGGINGFACE_BASE_URL', getattr(Config, 'QWEN_VL_API_URL', 'http://172.31.179.77:8007'))
        self.ollama_url = getattr(Config, 'OLLAMA_BASE_URL', None)
        self.timeout = Config.TIMEOUT_SECONDS
        self.use_direct_service = getattr(Config, 'USE_DIRECT_LORA_SERVICE', True)
        
        if self.use_direct_service:
            logger.info(f"âœ… æ³¨æ„ï¼šæ­¤æœåŠ¡å·²å¼ƒç”¨ï¼Œå»ºè®®ä½¿ç”¨qwen_vl_direct_serviceè¿›è¡Œç›´æ¥è°ƒç”¨")
            logger.info(f"ğŸ¯ LoRAæœåŠ¡åœ°å€: {self.huggingface_url}")
        elif self.use_huggingface:
            logger.info(f"âœ… ä¼˜å…ˆä½¿ç”¨HuggingFaceæœåŠ¡: {self.huggingface_url}")
            logger.info(f"ğŸ”„ å¤‡ç”¨OllamaæœåŠ¡: {self.ollama_url}")
        else:
            # å¤‡ç”¨ï¼šé…ç½®Ollamaå®¢æˆ·ç«¯
            import ollama
            import httpx
            self.client = ollama.Client(
                host=self.ollama_url,
                timeout=httpx.Timeout(300.0)  # è®¾ç½®5åˆ†é’Ÿè¶…æ—¶
            )
            logger.info(f"é…ç½®OllamaæœåŠ¡å™¨: {self.ollama_url}")
            
        self._test_connection()
    
    def _test_connection(self):
        """æµ‹è¯•AIæœåŠ¡è¿æ¥"""
        if self.use_huggingface:
            # æµ‹è¯•HuggingFaceæœåŠ¡
            try:
                logger.info(f"ğŸ” æµ‹è¯•HuggingFaceè¿æ¥: {self.huggingface_url}")
                
                # æµ‹è¯•å¥åº·æ£€æŸ¥ç«¯ç‚¹
                health_url = f"{self.huggingface_url}/health"
                response = requests.get(health_url, timeout=30)
                
                if response.status_code == 200:
                    logger.info("âœ… HuggingFaceæœåŠ¡è¿æ¥æ­£å¸¸")
                    
                    # HuggingFaceæœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡ï¼Œä½†æˆ‘ä»¬ç°åœ¨ä½¿ç”¨å¤šæ¨¡æ€æœåŠ¡
                    logger.info("âœ… HuggingFaceæœåŠ¡å¥åº·ï¼Œå¤šæ¨¡æ€æœåŠ¡å¯ç”¨")
                    return  # å¥åº·æ£€æŸ¥é€šè¿‡ï¼Œæ— éœ€æµ‹è¯•Ollama
                else:
                    logger.warning(f"âš ï¸ HuggingFaceå¥åº·æ£€æŸ¥å¤±è´¥: {response.status_code}")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ HuggingFaceè¿æ¥å¤±è´¥: {e}")
                logger.info("ğŸ”„ å°†åˆ‡æ¢åˆ°Ollamaå¤‡ç”¨æœåŠ¡")
        
        # æµ‹è¯•Ollamaå¤‡ç”¨æœåŠ¡
        try:
            if not hasattr(self, 'client'):
                import ollama
                import httpx
                self.client = ollama.Client(
                    host=self.ollama_url,
                    timeout=httpx.Timeout(300.0)
                )
            
            logger.info(f"ğŸ” æµ‹è¯•Ollamaè¿æ¥: {self.ollama_url}")
            
            # å°è¯•è·å–æ¨¡å‹åˆ—è¡¨
            models = self.client.list()
            logger.info(f"åŸå§‹æ¨¡å‹å“åº”: {models}")
            
            available_models = []
            if 'models' in models and models['models']:
                available_models = [model.get('name', '') for model in models['models']]
                available_models = [name for name in available_models if name.strip()]
                logger.info(f"å¯ç”¨Ollamaæ¨¡å‹: {available_models}")
            else:
                logger.warning("æ¨¡å‹åˆ—è¡¨ä¸ºç©ºæˆ–æ ¼å¼å¼‚å¸¸")
                available_models = [self.model_name]  # å‡è®¾æ¨¡å‹å­˜åœ¨
                
            # æ£€æŸ¥ç›®æ ‡æ¨¡å‹æ˜¯å¦å­˜åœ¨
            if self.model_name not in available_models:
                qwen_models = [name for name in available_models if 'qwen' in name.lower()]
                if qwen_models:
                    logger.warning(f"æ¨¡å‹ {self.model_name} ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨: {qwen_models[0]}")
                    self.model_name = qwen_models[0]
                elif not available_models or available_models == ['']:
                    logger.info(f"æ¨¡å‹åˆ—è¡¨ä¸ºç©ºï¼Œç›´æ¥å°è¯•ä½¿ç”¨: {self.model_name}")
                else:
                    logger.error(f"æœªæ‰¾åˆ°ä»»ä½•qwenæ¨¡å‹ï¼Œå¯ç”¨æ¨¡å‹: {available_models}")
                    raise Exception(f"æœªæ‰¾åˆ°qwenæ¨¡å‹")
            
            # æµ‹è¯•æ¨¡å‹ç”ŸæˆåŠŸèƒ½
            logger.info(f"æµ‹è¯•Ollamaæ¨¡å‹ {self.model_name} ç”ŸæˆåŠŸèƒ½...")
            test_response = self.client.generate(
                model=self.model_name,
                prompt="Hi",
                options={'num_predict': 3, 'temperature': 0.1},
                stream=False
            )
            
            if test_response and 'response' in test_response:
                logger.info(f"âœ… Ollamaå¤‡ç”¨æœåŠ¡è¿æ¥æˆåŠŸ")
                logger.info(f"æµ‹è¯•å“åº”: {test_response['response'][:50]}...")
            else:
                raise Exception(f"Ollamaæ¨¡å‹å“åº”æ ¼å¼å¼‚å¸¸")
                
        except Exception as e:
            logger.error(f"âŒ æ‰€æœ‰AIæœåŠ¡è¿æ¥å¤±è´¥: {e}")
            if "502" in str(e):
                logger.error("502é”™è¯¯ï¼šå¯èƒ½æ˜¯æœåŠ¡æœªå¯åŠ¨æˆ–ç«¯å£æœªå¼€æ”¾")
            elif "timeout" in str(e).lower():
                logger.error("è¶…æ—¶é”™è¯¯ï¼šç½‘ç»œè¿æ¥è¾ƒæ…¢æˆ–æ¨¡å‹åŠ è½½æ—¶é—´è¿‡é•¿")
            
            raise Exception(f"æ— æ³•è¿æ¥åˆ°ä»»ä½•AIæœåŠ¡: {e}")
    
    def generate_response(self, prompt: str, max_tokens: int = 200000) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬å“åº” - ä¼˜å…ˆHuggingFaceï¼Œå¤‡ç”¨Ollama
        
        Args:
            prompt: è¾“å…¥æç¤º
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬å“åº”
        """
        # ğŸ”„ HuggingFaceæœåŠ¡ç°åœ¨ä¸“ç”¨äºå¤šæ¨¡æ€åˆ†æï¼Œç›´æ¥ä½¿ç”¨Ollamaè¿›è¡Œæ–‡æœ¬ç”Ÿæˆ
        if self.use_huggingface:
            logger.info("ğŸ”„ HuggingFaceæœåŠ¡ä¸“ç”¨äºå¤šæ¨¡æ€åˆ†æï¼Œä½¿ç”¨Ollamaè¿›è¡Œæ–‡æœ¬ç”Ÿæˆ")
        
        # ğŸ¥ˆ å¤‡ç”¨OllamaæœåŠ¡
        try:
            logger.info(f"ğŸ”„ ä½¿ç”¨Ollamaå¤‡ç”¨æœåŠ¡ç”Ÿæˆå“åº”")
            result = self._call_ollama_api(prompt, max_tokens)
            if result and result.strip():
                logger.info(f"âœ… Ollamaå“åº”æˆåŠŸï¼Œé•¿åº¦: {len(result)} å­—ç¬¦")
                return result
        except Exception as e:
            logger.error(f"âŒ Ollamaè°ƒç”¨å¤±è´¥: {e}")
            
        logger.error("âŒ æ‰€æœ‰AIæœåŠ¡å‡ä¸å¯ç”¨")
        return ""
    
    # HuggingFace APIç°åœ¨ä¸“ç”¨äºå¤šæ¨¡æ€åˆ†æï¼Œä¸å†æä¾›å•ç‹¬çš„æ–‡æœ¬ç”Ÿæˆæ¥å£
    # æ–‡æœ¬ç”Ÿæˆç»Ÿä¸€ä½¿ç”¨OllamaæœåŠ¡
    
    def _call_ollama_api(self, prompt: str, max_tokens: int) -> str:
        """è°ƒç”¨Ollama APIï¼ˆå¤‡ç”¨ï¼‰"""
        if not hasattr(self, 'client'):
            import ollama
            import httpx
            self.client = ollama.Client(
                host=self.ollama_url,
                timeout=httpx.Timeout(300.0)
            )
        
        # ä¸ºæ‰¹æ”¹ä»»åŠ¡ç§»é™¤stopå‚æ•°ï¼Œé¿å…æˆªæ–­å®Œæ•´å“åº”
        options = {
            'num_predict': max_tokens,
            'temperature': 0.1,
            'top_p': 0.9,
        }
        
        # åªåœ¨éæ‰¹æ”¹ä»»åŠ¡ä¸­ä½¿ç”¨stopå‚æ•°
        if max_tokens <= 100:
            options['stop'] = ['\n\n', 'é—®é¢˜:', 'é¢˜ç›®:']
        
        response = self.client.generate(
            model=self.model_name,
            prompt=prompt,
            options=options,
            stream=False
        )
        
        return response['response'].strip()
    
    def analyze_question_type(self, question: str) -> str:
        """
        åˆ†æé¢˜ç›®ç±»å‹
        
        Args:
            question: é¢˜ç›®æ–‡æœ¬
            
        Returns:
            é¢˜ç›®ç±»å‹
        """
        prompt = f"""è¯·åˆ†æä»¥ä¸‹é¢˜ç›®çš„ç±»å‹ï¼Œä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©æœ€åˆé€‚çš„ä¸€ä¸ªï¼š
é€‰æ‹©é¢˜ã€åˆ¤æ–­é¢˜ã€å¡«ç©ºé¢˜ã€è®¡ç®—é¢˜ã€åº”ç”¨é¢˜ã€å…¬å¼é¢˜ã€å£ç®—é¢˜

é¢˜ç›®ï¼š{question}

è¯·åªå›ç­”é¢˜ç›®ç±»å‹ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"""
        
        response = self.generate_response(prompt, max_tokens=50)
        
        # æå–é¢˜ç›®ç±»å‹
        valid_types = ['é€‰æ‹©é¢˜', 'åˆ¤æ–­é¢˜', 'å¡«ç©ºé¢˜', 'è®¡ç®—é¢˜', 'åº”ç”¨é¢˜', 'å…¬å¼é¢˜', 'å£ç®—é¢˜']
        for type_name in valid_types:
            if type_name in response:
                return type_name
        
        return 'æœªçŸ¥é¢˜å‹'
    
    def grade_question(self, question: str, student_answer: str, question_type: str) -> Dict[str, Any]:
        """
        æ‰¹æ”¹å•é“é¢˜ç›®
        
        Args:
            question: é¢˜ç›®å†…å®¹
            student_answer: å­¦ç”Ÿç­”æ¡ˆ
            question_type: é¢˜ç›®ç±»å‹
            
        Returns:
            æ‰¹æ”¹ç»“æœå­—å…¸
        """
        if question_type == 'è®¡ç®—é¢˜':
            return self._grade_calculation(question, student_answer)
        elif question_type == 'é€‰æ‹©é¢˜':
            return self._grade_multiple_choice(question, student_answer)
        elif question_type == 'åˆ¤æ–­é¢˜':
            return self._grade_true_false(question, student_answer)
        elif question_type == 'å¡«ç©ºé¢˜':
            return self._grade_fill_blank(question, student_answer)
        else:
            return self._grade_general(question, student_answer, question_type)
    
    def _grade_calculation(self, question: str, student_answer: str) -> Dict[str, Any]:
        """æ‰¹æ”¹è®¡ç®—é¢˜"""
        prompt = f"""ä½œä¸ºä¸€åä¸“ä¸šçš„æ•°å­¦è€å¸ˆï¼Œè¯·æ‰¹æ”¹ä»¥ä¸‹è®¡ç®—é¢˜ï¼š

é¢˜ç›®ï¼š{question}
å­¦ç”Ÿç­”æ¡ˆï¼š{student_answer}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
æ­£ç¡®æ€§ï¼šæ­£ç¡®/é”™è¯¯
åˆ†æ•°ï¼š0-5åˆ†
æ­£ç¡®ç­”æ¡ˆï¼š[è¯·æä¾›å®Œæ•´çš„æ ‡å‡†ç­”æ¡ˆ]
è§£é‡Šï¼šè¯¦ç»†çš„æ‰¹æ”¹è¯´æ˜ï¼ŒåŒ…æ‹¬å®Œæ•´çš„è§£é¢˜è¿‡ç¨‹ã€è®¡ç®—æ­¥éª¤å’Œæœ€ç»ˆç­”æ¡ˆ

è¯·åŠ¡å¿…ï¼š
1. æä¾›å®Œæ•´çš„æ­£ç¡®è§£é¢˜è¿‡ç¨‹
2. æŒ‡å‡ºå­¦ç”Ÿç­”æ¡ˆä¸­çš„é”™è¯¯ï¼ˆå¦‚æœæœ‰ï¼‰
3. ç»™å‡ºè¯¦ç»†çš„æ•°å­¦è®¡ç®—æ­¥éª¤
4. æä¾›æœ€ç»ˆçš„æ­£ç¡®ç­”æ¡ˆ"""

        response = self.generate_response(prompt, max_tokens=200000)  # æœ€å¤§tokenæ•°
        return self._parse_grading_response(response)
    
    def _grade_multiple_choice(self, question: str, student_answer: str) -> Dict[str, Any]:
        """æ‰¹æ”¹é€‰æ‹©é¢˜"""
        prompt = f"""ä½œä¸ºä¸€åä¸“ä¸šè€å¸ˆï¼Œè¯·æ‰¹æ”¹ä»¥ä¸‹é€‰æ‹©é¢˜ï¼š

é¢˜ç›®ï¼š{question}
å­¦ç”Ÿç­”æ¡ˆï¼š{student_answer}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
æ­£ç¡®æ€§ï¼šæ­£ç¡®/é”™è¯¯
åˆ†æ•°ï¼š0-2åˆ†ï¼ˆé€‰æ‹©é¢˜æ»¡åˆ†2åˆ†ï¼‰
è§£é‡Šï¼šç®€è¦è¯´æ˜æ­£ç¡®ç­”æ¡ˆå’ŒåŸå› 

è¯·åˆ†æé¢˜ç›®ä¸­çš„é€‰é¡¹ï¼Œç¡®å®šæ­£ç¡®ç­”æ¡ˆã€‚"""

        response = self.generate_response(prompt, max_tokens=200000)  # æœ€å¤§tokenæ•°
        return self._parse_grading_response(response)
    
    def _grade_true_false(self, question: str, student_answer: str) -> Dict[str, Any]:
        """æ‰¹æ”¹åˆ¤æ–­é¢˜"""
        prompt = f"""ä½œä¸ºä¸€åä¸“ä¸šè€å¸ˆï¼Œè¯·æ‰¹æ”¹ä»¥ä¸‹åˆ¤æ–­é¢˜ï¼š

é¢˜ç›®ï¼š{question}
å­¦ç”Ÿç­”æ¡ˆï¼š{student_answer}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
æ­£ç¡®æ€§ï¼šæ­£ç¡®/é”™è¯¯
åˆ†æ•°ï¼š0-2åˆ†ï¼ˆåˆ¤æ–­é¢˜æ»¡åˆ†2åˆ†ï¼‰
è§£é‡Šï¼šç®€è¦è¯´æ˜æ­£ç¡®ç­”æ¡ˆå’Œåˆ¤æ–­ä¾æ®"""

        response = self.generate_response(prompt, max_tokens=200000)  # æœ€å¤§tokenæ•°
        return self._parse_grading_response(response)
    
    def _grade_fill_blank(self, question: str, student_answer: str) -> Dict[str, Any]:
        """æ‰¹æ”¹å¡«ç©ºé¢˜"""
        prompt = f"""ä½œä¸ºä¸€åä¸“ä¸šè€å¸ˆï¼Œè¯·æ‰¹æ”¹ä»¥ä¸‹å¡«ç©ºé¢˜ï¼š

é¢˜ç›®ï¼š{question}
å­¦ç”Ÿç­”æ¡ˆï¼š{student_answer}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
æ­£ç¡®æ€§ï¼šæ­£ç¡®/é”™è¯¯/éƒ¨åˆ†æ­£ç¡®
åˆ†æ•°ï¼š0-3åˆ†ï¼ˆå¡«ç©ºé¢˜æ»¡åˆ†3åˆ†ï¼‰
è§£é‡Šï¼šè¯¦ç»†è¯´æ˜æ­£ç¡®ç­”æ¡ˆï¼Œå¦‚æœå­¦ç”Ÿç­”æ¡ˆéƒ¨åˆ†æ­£ç¡®åˆ™è¯´æ˜å“ªéƒ¨åˆ†æ­£ç¡®"""

        response = self.generate_response(prompt, max_tokens=200000)  # æœ€å¤§tokenæ•°
        return self._parse_grading_response(response)
    
    def _grade_general(self, question: str, student_answer: str, question_type: str) -> Dict[str, Any]:
        """é€šç”¨æ‰¹æ”¹æ–¹æ³•"""
        prompt = f"""ä½œä¸ºä¸€åä¸“ä¸šè€å¸ˆï¼Œè¯·æ‰¹æ”¹ä»¥ä¸‹{question_type}ï¼š

é¢˜ç›®ï¼š{question}
å­¦ç”Ÿç­”æ¡ˆï¼š{student_answer}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
æ­£ç¡®æ€§ï¼šæ­£ç¡®/é”™è¯¯/éƒ¨åˆ†æ­£ç¡®
åˆ†æ•°ï¼š0-5åˆ†
æ­£ç¡®ç­”æ¡ˆï¼š[è¯·æä¾›æ ‡å‡†ç­”æ¡ˆ]
è§£é‡Šï¼šè¯¦ç»†çš„æ‰¹æ”¹è¯´æ˜ï¼ŒåŒ…æ‹¬è§£é¢˜è¿‡ç¨‹å’ŒçŸ¥è¯†ç‚¹åˆ†æ

è¯·åŠ¡å¿…æä¾›å®Œæ•´çš„æ­£ç¡®ç­”æ¡ˆå’Œè¯¦ç»†çš„è§£é¢˜æ­¥éª¤ã€‚"""

        response = self.generate_response(prompt, max_tokens=200000)  # æœ€å¤§tokenæ•°
        return self._parse_grading_response(response)
    
    def _parse_grading_response(self, response: str) -> Dict[str, Any]:
        """è§£ææ‰¹æ”¹å“åº”"""
        try:
            # æå–æ­£ç¡®æ€§
            correct = False
            if 'æ­£ç¡®æ€§ï¼šæ­£ç¡®' in response or 'æ­£ç¡®æ€§: æ­£ç¡®' in response:
                correct = True
            elif 'æ­£ç¡®æ€§ï¼šéƒ¨åˆ†æ­£ç¡®' in response or 'æ­£ç¡®æ€§: éƒ¨åˆ†æ­£ç¡®' in response:
                correct = True  # éƒ¨åˆ†æ­£ç¡®ä¹Ÿç®—æ­£ç¡®
            
            # æå–åˆ†æ•°
            score_match = re.search(r'åˆ†æ•°[ï¼š:]\s*(\d+(?:\.\d+)?)', response)
            score = float(score_match.group(1)) if score_match else 0
            
            # æå–æ­£ç¡®ç­”æ¡ˆ
            correct_answer_match = re.search(r'æ­£ç¡®ç­”æ¡ˆ[ï¼š:]\s*(.*?)(?=\nè§£é‡Š|$)', response, re.DOTALL)
            correct_answer = correct_answer_match.group(1).strip() if correct_answer_match else "å‚è€ƒç­”æ¡ˆ"
            
            # æå–è§£é‡Šï¼ˆåŒ…å«å®Œæ•´çš„å“åº”å†…å®¹ï¼‰
            explanation_match = re.search(r'è§£é‡Š[ï¼š:]\s*(.*)', response, re.DOTALL)
            if explanation_match:
                explanation = explanation_match.group(1).strip()
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°"è§£é‡Š"æ ‡è®°ï¼Œä½¿ç”¨æ•´ä¸ªå“åº”
                explanation = response.strip()
            
            # ç¡®ä¿è§£é‡ŠåŒ…å«æ­£ç¡®ç­”æ¡ˆä¿¡æ¯
            if correct_answer and correct_answer != "å‚è€ƒç­”æ¡ˆ":
                explanation = f"æ­£ç¡®ç­”æ¡ˆï¼š{correct_answer}\n\n{explanation}"
            
            return {
                'correct': correct,
                'score': score,
                'explanation': explanation,
                'correct_answer': correct_answer
            }
        
        except Exception as e:
            logger.error(f"è§£ææ‰¹æ”¹å“åº”å¤±è´¥: {e}")
            return {
                'correct': False,
                'score': 0,
                'explanation': 'æ‰¹æ”¹å¤±è´¥ï¼Œè¯·é‡è¯•'
            }
    
    def analyze_knowledge_points(self, wrong_questions: List[Dict]) -> List[str]:
        """
        åˆ†æé”™é¢˜çŸ¥è¯†ç‚¹
        
        Args:
            wrong_questions: é”™é¢˜åˆ—è¡¨
            
        Returns:
            çŸ¥è¯†ç‚¹åˆ—è¡¨
        """
        if not wrong_questions:
            return []
        
        questions_text = "\n".join([f"é¢˜ç›®{i+1}: {q.get('question', '')}" 
                                  for i, q in enumerate(wrong_questions)])
        
        prompt = f"""è¯·åˆ†æä»¥ä¸‹é”™é¢˜æ¶‰åŠçš„ä¸»è¦çŸ¥è¯†ç‚¹ï¼Œé€‚åˆåˆä¸­ç”Ÿæ°´å¹³ï¼š

{questions_text}

è¯·åˆ—å‡º3-5ä¸ªä¸»è¦çŸ¥è¯†ç‚¹ï¼Œæ¯ä¸ªçŸ¥è¯†ç‚¹å•ç‹¬ä¸€è¡Œï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
1. çŸ¥è¯†ç‚¹åç§°
2. çŸ¥è¯†ç‚¹åç§°
...

åªè¿”å›çŸ¥è¯†ç‚¹åˆ—è¡¨ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"""

        response = self.generate_response(prompt, max_tokens=200000)
        
        # æå–çŸ¥è¯†ç‚¹
        knowledge_points = []
        lines = response.split('\n')
        for line in lines:
            line = line.strip()
            if line and (line[0].isdigit() or line.startswith('â€¢') or line.startswith('-')):
                # ç§»é™¤ç¼–å·å’Œç¬¦å·
                point = re.sub(r'^\d+\.\s*|^[â€¢\-]\s*', '', line).strip()
                if point:
                    knowledge_points.append(point)
        
        return knowledge_points[:5]  # æœ€å¤šè¿”å›5ä¸ªçŸ¥è¯†ç‚¹
    
    def generate_practice_questions(self, knowledge_points: List[str], count: int = 3) -> List[Dict]:
        """
        æ ¹æ®çŸ¥è¯†ç‚¹ç”Ÿæˆç»ƒä¹ é¢˜
        
        Args:
            knowledge_points: çŸ¥è¯†ç‚¹åˆ—è¡¨
            count: ç”Ÿæˆé¢˜ç›®æ•°é‡
            
        Returns:
            ç»ƒä¹ é¢˜åˆ—è¡¨
        """
        if not knowledge_points:
            return []
        
        knowledge_text = "ã€".join(knowledge_points)
        
        prompt = f"""åŸºäºä»¥ä¸‹çŸ¥è¯†ç‚¹ï¼Œä¸ºåˆä¸­ç”Ÿç”Ÿæˆ{count}é“ç»ƒä¹ é¢˜ï¼š

çŸ¥è¯†ç‚¹ï¼š{knowledge_text}

è¦æ±‚ï¼š
1. é¢˜ç›®åº”è¯¥é€‚åˆåˆä¸­ç”Ÿæ°´å¹³
2. åŒ…å«ä¸åŒç±»å‹çš„é¢˜ç›®ï¼ˆé€‰æ‹©é¢˜ã€å¡«ç©ºé¢˜ã€è®¡ç®—é¢˜ç­‰ï¼‰
3. é¢˜ç›®è¦æœ‰ä¸€å®šçš„æ¢¯åº¦ï¼Œä»åŸºç¡€åˆ°æé«˜

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç”Ÿæˆé¢˜ç›®ï¼š

é¢˜ç›®1ï¼š
ç±»å‹ï¼š[é€‰æ‹©é¢˜/å¡«ç©ºé¢˜/è®¡ç®—é¢˜]
é¢˜å¹²ï¼šé¢˜ç›®å†…å®¹
é€‰é¡¹ï¼šA. xxx B. xxx C. xxx D. xxxï¼ˆå¦‚æœæ˜¯é€‰æ‹©é¢˜ï¼‰
ç­”æ¡ˆï¼šæ­£ç¡®ç­”æ¡ˆ
è§£æï¼šç®€è¦è§£æ

é¢˜ç›®2ï¼š
...

åªè¿”å›é¢˜ç›®å†…å®¹ï¼Œä¸è¦åŒ…å«å…¶ä»–è¯´æ˜ã€‚"""

        response = self.generate_response(prompt, max_tokens=200000)
        
        # è§£æç”Ÿæˆçš„é¢˜ç›®
        return self._parse_practice_questions(response)
    
    def _parse_practice_questions(self, response: str) -> List[Dict]:
        """è§£æç”Ÿæˆçš„ç»ƒä¹ é¢˜"""
        questions = []
        
        try:
            # æŒ‰é¢˜ç›®åˆ†å‰²
            question_blocks = re.split(r'é¢˜ç›®\d+[ï¼š:]', response)
            
            for block in question_blocks[1:]:  # è·³è¿‡ç¬¬ä¸€ä¸ªç©ºå—
                question_data = {}
                
                # æå–ç±»å‹
                type_match = re.search(r'ç±»å‹[ï¼š:]\s*([^\n]+)', block)
                if type_match:
                    question_data['type'] = type_match.group(1).strip()
                
                # æå–é¢˜å¹²
                stem_match = re.search(r'é¢˜å¹²[ï¼š:]\s*([^\n]+)', block)
                if stem_match:
                    question_data['stem'] = stem_match.group(1).strip()
                
                # æå–é€‰é¡¹ï¼ˆå¦‚æœæ˜¯é€‰æ‹©é¢˜ï¼‰
                options_match = re.search(r'é€‰é¡¹[ï¼š:]\s*([^ç­”]+?)(?=ç­”æ¡ˆ|è§£æ|$)', block, re.DOTALL)
                if options_match:
                    question_data['options'] = options_match.group(1).strip()
                
                # æå–ç­”æ¡ˆ
                answer_match = re.search(r'ç­”æ¡ˆ[ï¼š:]\s*([^\n]+)', block)
                if answer_match:
                    question_data['answer'] = answer_match.group(1).strip()
                
                # æå–è§£æ
                explanation_match = re.search(r'è§£æ[ï¼š:]\s*([^é¢˜]+?)(?=é¢˜ç›®|$)', block, re.DOTALL)
                if explanation_match:
                    question_data['explanation'] = explanation_match.group(1).strip()
                
                if question_data.get('stem') and question_data.get('answer'):
                    questions.append(question_data)
        
        except Exception as e:
            logger.error(f"è§£æç»ƒä¹ é¢˜å¤±è´¥: {e}")
        
        return questions
    
    def multimodal_analyze(self, image_text: str, question_context: str = "") -> Dict[str, Any]:
        """
        å¤šæ¨¡æ€åˆ†æï¼šç»“åˆå›¾åƒOCRæ–‡æœ¬å’Œä¸Šä¸‹æ–‡è¿›è¡Œåˆ†æ
        
        Args:
            image_text: OCRè¯†åˆ«çš„å›¾åƒæ–‡æœ¬
            question_context: é¢˜ç›®ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            åˆ†æç»“æœ
        """
        prompt = f"""ä½œä¸ºä¸€åä¸“ä¸šçš„æ•™è‚²AIåŠ©æ‰‹ï¼Œè¯·åˆ†æä»¥ä¸‹ä»è¯•å·å›¾ç‰‡ä¸­è¯†åˆ«çš„æ–‡æœ¬å†…å®¹ï¼š

OCRè¯†åˆ«æ–‡æœ¬ï¼š
{image_text}

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{question_context}

è¯·è¿›è¡Œä»¥ä¸‹åˆ†æï¼š
1. æ–‡æœ¬å†…å®¹è´¨é‡è¯„ä¼°ï¼ˆè¯†åˆ«å‡†ç¡®æ€§ï¼‰
2. é¢˜ç›®ç»“æ„è¯†åˆ«ï¼ˆé¢˜å¹²ã€é€‰é¡¹ã€ç­”æ¡ˆåŒºåŸŸï¼‰
3. å¯èƒ½çš„OCRé”™è¯¯ä¿®æ­£å»ºè®®
4. é¢˜ç›®ç±»å‹åˆæ­¥åˆ¤æ–­

è¯·ä»¥ç»“æ„åŒ–çš„æ–¹å¼å›ç­”ï¼Œä¾¿äºç¨‹åºå¤„ç†ã€‚"""

        response = self.generate_response(prompt, max_tokens=200000)
        
        return {
            'analysis': response,
            'text_quality': self._assess_text_quality(image_text),
            'structure_detected': self._detect_question_structure(image_text)
        }
    
    def _assess_text_quality(self, text: str) -> str:
        """è¯„ä¼°OCRæ–‡æœ¬è´¨é‡"""
        if not text or len(text.strip()) < 10:
            return "æ–‡æœ¬è¿‡çŸ­ï¼Œå¯èƒ½è¯†åˆ«ä¸å®Œæ•´"
        
        # æ£€æŸ¥å¸¸è§OCRé”™è¯¯æ¨¡å¼
        error_indicators = ['ï¼Ÿï¼Ÿ', 'â–¡â–¡', '||', '@@', '##']
        if any(indicator in text for indicator in error_indicators):
            return "æ£€æµ‹åˆ°å¯èƒ½çš„OCRé”™è¯¯æ ‡è®°"
        
        # æ£€æŸ¥ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        total_chars = len(text.strip())
        
        if total_chars > 0 and chinese_chars / total_chars > 0.3:
            return "æ–‡æœ¬è´¨é‡è‰¯å¥½"
        else:
            return "æ–‡æœ¬å¯èƒ½å­˜åœ¨è¯†åˆ«é—®é¢˜"
    
    def _detect_question_structure(self, text: str) -> Dict[str, bool]:
        """æ£€æµ‹é¢˜ç›®ç»“æ„"""
        return {
            'has_question_number': bool(re.search(r'\d+[ï¼\.\)ï¼‰]', text)),
            'has_options': bool(re.search(r'[ABCD][\.ï¼\)ï¼‰]', text)),
            'has_blank': bool(re.search(r'_{2,}|ï¼ˆ\s*ï¼‰', text)),
            'has_calculation': bool(re.search(r'[+\-Ã—Ã·=]|\d+\s*[+\-Ã—Ã·]\s*\d+', text))
        }
