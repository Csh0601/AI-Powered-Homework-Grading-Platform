#!/usr/bin/env python3
"""
é«˜ç²¾åº¦çŸ¥è¯†ç‚¹æå–ç³»ç»Ÿ
Day8ä»»åŠ¡: åŸºäºBERT embeddingsçš„è¯­ä¹‰æå–ï¼Œå®ç°ä»é¢˜ç›®ä¸­å‡†ç¡®æå–çŸ¥è¯†ç‚¹æ ‡ç­¾

ä¸»è¦åŠŸèƒ½ï¼š
1. åŸºäºBERT embeddingsçš„è¯­ä¹‰æå–
2. çŸ¥è¯†ç‚¹æƒé‡è¯„åˆ†æœºåˆ¶
3. å¤šçŸ¥è¯†ç‚¹è‡ªåŠ¨æ ‡æ³¨
4. ç»“åˆè§„åˆ™åŒ¹é…å’Œæœºå™¨å­¦ä¹ 
5. æ”¯æŒæ‰¹é‡å¤„ç†

æŠ€æœ¯è¦ç‚¹ï¼š
- æ·±åº¦å­¦ä¹ å’Œè¯­ä¹‰ç†è§£
- å¤šç»´åº¦ç‰¹å¾æå–
- æœºå™¨å­¦ä¹ åˆ†ç±»
- ç»“æœå¯è§£é‡Šæ€§

ç›®æ ‡ï¼šçŸ¥è¯†ç‚¹æå–å‡†ç¡®ç‡è¾¾80%+
"""

import re
import jieba
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional, Set
from collections import defaultdict, Counter
import json
import os
import logging
from datetime import datetime
import pickle
import hashlib

# æœºå™¨å­¦ä¹ å’ŒNLPåº“
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.multiclass import MultiLabelBinarizer
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report, accuracy_score
    import gensim
    from gensim.models import Word2Vec
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    print("âš ï¸ æœºå™¨å­¦ä¹ åº“æœªå®Œå…¨å®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½å—é™")

# BERTå’Œæ·±åº¦å­¦ä¹ åº“
try:
    from transformers import AutoTokenizer, AutoModel
    from sentence_transformers import SentenceTransformer
    import torch
    BERT_AVAILABLE = True
except ImportError as e:
    BERT_AVAILABLE = False
    print(f"âš ï¸ BERT/Transformersåº“æœªå®‰è£…ï¼Œè¯­ä¹‰æå–åŠŸèƒ½å°†å—é™: {e}")
except Exception as e:
    BERT_AVAILABLE = False
    print(f"âš ï¸ BERT/Transformersåº“åˆå§‹åŒ–å¤±è´¥ï¼Œè¯­ä¹‰æå–åŠŸèƒ½å°†å—é™: {e}")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class KnowledgeExtractor:
    """é«˜ç²¾åº¦çŸ¥è¯†ç‚¹æå–å™¨"""
    
    def __init__(self, model_path: str = None):
        """åˆå§‹åŒ–æå–å™¨"""
        self.model_path = model_path or os.path.join(
            os.path.dirname(__file__), '..', '..', 'models', 'knowledge_matcher'
        )
        
        # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
        os.makedirs(self.model_path, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.tfidf_vectorizer = None
        self.word2vec_model = None
        self.ml_classifier = None
        self.label_encoder = MultiLabelBinarizer() if ML_AVAILABLE else None
        
        # BERTæ¨¡å‹ç»„ä»¶
        self.bert_tokenizer = None
        self.bert_model = None
        self.sentence_transformer = None
        self.knowledge_embeddings = None
        
        # çŸ¥è¯†ç‚¹ä½“ç³»å’Œè§„åˆ™
        self._init_knowledge_rules()
        
        # æå–ç»Ÿè®¡
        self.extraction_stats = {
            'total_extractions': 0,
            'successful_extractions': 0,
            'method_accuracy': {
                'rule_based': [],
                'tfidf_based': [],
                'bert_based': [],
                'ensemble': []
            },
            'knowledge_point_frequency': defaultdict(int),
            'subject_distribution': defaultdict(int)
        }
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        self._load_models()
        
        # åˆå§‹åŒ–BERTæ¨¡å‹
        self._init_bert_models()
    
    def _init_knowledge_rules(self):
        """åˆå§‹åŒ–çŸ¥è¯†ç‚¹è§„åˆ™åº“"""
        self.knowledge_rules = {
            # æ•°å­¦å­¦ç§‘è§„åˆ™
            'math': {
                'equations': {
                    'keywords': ['æ–¹ç¨‹', 'è§£', 'æ±‚è§£', 'æœªçŸ¥æ•°', 'ç­‰å¼', 'æ¶ˆå…ƒ', 'ä»£å…¥'],
                    'patterns': [
                        r'è§£.*?æ–¹ç¨‹',
                        r'æ±‚.*?çš„å€¼',
                        r'è®¾.*?ä¸º.*?',
                        r'[xyz]\s*=',
                        r'\d+x\s*[+\-=]',
                        r'ä¸€å…ƒ.*?æ–¹ç¨‹',
                        r'äºŒå…ƒ.*?æ–¹ç¨‹'
                    ],
                    'negative_patterns': [r'ä¸ç­‰å¼', r'å‡½æ•°'],
                    'weight': 1.0,
                    'confidence_base': 0.8
                },
                'inequalities': {
                    'keywords': ['ä¸ç­‰å¼', 'å¤§äº', 'å°äº', 'ä¸ç­‰å·', 'è§£é›†', 'å–å€¼èŒƒå›´'],
                    'patterns': [
                        r'ä¸ç­‰å¼.*?è§£',
                        r'[>â‰¥<â‰¤]',
                        r'è§£é›†',
                        r'å–å€¼èŒƒå›´',
                        r'æ»¡è¶³.*?ä¸ç­‰'
                    ],
                    'negative_patterns': [r'æ–¹ç¨‹', r'ç­‰å¼'],
                    'weight': 1.0,
                    'confidence_base': 0.9
                },
                'geometry_triangle': {
                    'keywords': ['ä¸‰è§’å½¢', 'è¾¹é•¿', 'è§’åº¦', 'é¢ç§¯', 'å‘¨é•¿', 'å‹¾è‚¡å®šç†', 'å†…è§’', 'å¤–è§’'],
                    'patterns': [
                        r'ä¸‰è§’å½¢.*?(é¢ç§¯|å‘¨é•¿|è¾¹é•¿)',
                        r'â–³.*?ABC',
                        r'å‹¾è‚¡å®šç†',
                        r'ç›´è§’ä¸‰è§’å½¢',
                        r'ç­‰è…°ä¸‰è§’å½¢',
                        r'ç­‰è¾¹ä¸‰è§’å½¢'
                    ],
                    'negative_patterns': [],
                    'weight': 1.2,
                    'confidence_base': 0.85
                },
                'geometry_circle': {
                    'keywords': ['åœ†', 'åŠå¾„', 'ç›´å¾„', 'å‘¨é•¿', 'é¢ç§¯', 'å¼§é•¿', 'æ‰‡å½¢'],
                    'patterns': [
                        r'åœ†.*?(é¢ç§¯|å‘¨é•¿|åŠå¾„|ç›´å¾„)',
                        r'åŠå¾„.*?ä¸º',
                        r'â—‹.*?[ABC]',
                        r'å¼§é•¿',
                        r'æ‰‡å½¢.*?é¢ç§¯'
                    ],
                    'negative_patterns': [],
                    'weight': 1.1,
                    'confidence_base': 0.85
                },
                'functions_linear': {
                    'keywords': ['ä¸€æ¬¡å‡½æ•°', 'æ–œç‡', 'æˆªè·', 'å›¾åƒ', 'æ­£æ¯”ä¾‹', 'åæ¯”ä¾‹'],
                    'patterns': [
                        r'ä¸€æ¬¡å‡½æ•°',
                        r'y\s*=.*?x',
                        r'æ–œç‡',
                        r'k.*?å€¼',
                        r'å‡½æ•°.*?å›¾åƒ'
                    ],
                    'negative_patterns': [r'äºŒæ¬¡å‡½æ•°'],
                    'weight': 1.1,
                    'confidence_base': 0.8
                },
                'functions_quadratic': {
                    'keywords': ['äºŒæ¬¡å‡½æ•°', 'æŠ›ç‰©çº¿', 'é¡¶ç‚¹', 'å¯¹ç§°è½´', 'å¼€å£æ–¹å‘', 'æœ€å€¼'],
                    'patterns': [
                        r'äºŒæ¬¡å‡½æ•°',
                        r'y\s*=.*?xÂ²',
                        r'y\s*=.*?x\^2',
                        r'æŠ›ç‰©çº¿',
                        r'é¡¶ç‚¹.*?åæ ‡',
                        r'å¯¹ç§°è½´'
                    ],
                    'negative_patterns': [r'ä¸€æ¬¡å‡½æ•°'],
                    'weight': 1.3,
                    'confidence_base': 0.9
                }
            },
            
            # è¯­æ–‡å­¦ç§‘è§„åˆ™
            'chinese': {
                'poetry_analysis': {
                    'keywords': ['è¯—æ­Œ', 'éŸµå¾‹', 'æ„è±¡', 'æƒ…æ„Ÿ', 'ä¿®è¾', 'å¤è¯—', 'è¯—è¯', 'èµæ'],
                    'patterns': [
                        r'è¯—æ­Œ.*?èµæ',
                        r'å¤è¯—.*?ç†è§£',
                        r'è¯—è¯.*?åˆ†æ',
                        r'è¡¨è¾¾.*?æƒ…æ„Ÿ',
                        r'ä¿®è¾.*?æ‰‹æ³•'
                    ],
                    'negative_patterns': [r'æ•£æ–‡', r'å°è¯´'],
                    'weight': 1.0,
                    'confidence_base': 0.8
                },
                'prose_analysis': {
                    'keywords': ['æ•£æ–‡', 'æŠ’æƒ…', 'å™äº‹', 'æå†™', 'è®®è®º', 'æ–‡ç« ', 'ä½œè€…'],
                    'patterns': [
                        r'æ•£æ–‡.*?åˆ†æ',
                        r'ä½œè€….*?æƒ…æ„Ÿ',
                        r'æ–‡ç« .*?ä¸»é¢˜',
                        r'æå†™.*?æ–¹æ³•'
                    ],
                    'negative_patterns': [r'è¯—æ­Œ', r'å¤è¯—'],
                    'weight': 1.0,
                    'confidence_base': 0.75
                },
                'grammar': {
                    'keywords': ['ä¸»è¯­', 'è°“è¯­', 'å®¾è¯­', 'å®šè¯­', 'çŠ¶è¯­', 'è¡¥è¯­', 'å¥å­', 'æˆåˆ†'],
                    'patterns': [
                        r'å¥å­.*?æˆåˆ†',
                        r'è¯­æ³•.*?åˆ†æ',
                        r'ä¸»è°“å®¾',
                        r'å®šçŠ¶è¡¥'
                    ],
                    'negative_patterns': [],
                    'weight': 1.0,
                    'confidence_base': 0.85
                }
            },
            
            # ç‰©ç†å­¦ç§‘è§„åˆ™
            'physics': {
                'mechanics_motion': {
                    'keywords': ['é€Ÿåº¦', 'åŠ é€Ÿåº¦', 'ä½ç§»', 'æ—¶é—´', 'è¿åŠ¨', 'åŒ€é€Ÿ', 'å˜é€Ÿ'],
                    'patterns': [
                        r'é€Ÿåº¦.*?è®¡ç®—',
                        r'è¿åŠ¨.*?æ—¶é—´',
                        r'ä½ç§».*?å…¬å¼',
                        r'åŒ€é€Ÿ.*?è¿åŠ¨',
                        r'åŠ é€Ÿåº¦.*?ä¸º'
                    ],
                    'negative_patterns': [],
                    'weight': 1.1,
                    'confidence_base': 0.8
                },
                'mechanics_force': {
                    'keywords': ['åŠ›', 'é‡åŠ›', 'æ‘©æ“¦åŠ›', 'å‹åŠ›', 'æ”¯æŒåŠ›', 'åˆåŠ›', 'åˆ†åŠ›'],
                    'patterns': [
                        r'åŠ›.*?åˆ†æ',
                        r'å—åŠ›.*?å›¾',
                        r'é‡åŠ›.*?ä¸º',
                        r'æ‘©æ“¦åŠ›.*?å¤§å°',
                        r'åˆåŠ›.*?æ–¹å‘'
                    ],
                    'negative_patterns': [],
                    'weight': 1.1,
                    'confidence_base': 0.8
                }
            }
        }
    
    def preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'\s+', ' ', text.strip())
        
        # ä¿ç•™é‡è¦çš„æ•°å­¦ç¬¦å·å’Œæ ‡ç‚¹
        important_symbols = ['=', '+', '-', 'Ã—', 'Ã·', '>', '<', 'â‰¥', 'â‰¤', 'â‰ ', 'â‰ˆ', 
                           'âˆš', '^', 'Â²', 'Â³', 'Ï€', 'âˆ', 'Â°', 'âˆ ', 'â–³', 'â–¡', 'â—‹', 
                           '(', ')', '[', ']', '{', '}', 'ï¼Œ', 'ã€‚', 'ï¼Ÿ', 'ï¼']
        
        # åˆ†è¯å¹¶ä¿ç•™é‡è¦ç¬¦å·
        words = []
        for word in jieba.cut(text):
            word = word.strip()
            if word and (word.isalnum() or word in important_symbols or len(word) > 1):
                words.append(word)
        
        return ' '.join(words)
    
    def extract_by_rules(self, question_text: str, subject_hint: str = None) -> List[Dict[str, Any]]:
        """åŸºäºè§„åˆ™çš„çŸ¥è¯†ç‚¹æå–"""
        processed_text = self.preprocess_text(question_text).lower()
        
        extractions = []
        
        # ç¡®å®šæœç´¢èŒƒå›´
        search_subjects = [subject_hint] if subject_hint and subject_hint in self.knowledge_rules else self.knowledge_rules.keys()
        
        for subject in search_subjects:
            subject_rules = self.knowledge_rules[subject]
            
            for knowledge_point, rules in subject_rules.items():
                score = 0.0
                matched_keywords = []
                matched_patterns = []
                
                # å…³é”®è¯åŒ¹é…
                for keyword in rules['keywords']:
                    if keyword.lower() in processed_text:
                        score += 1.0
                        matched_keywords.append(keyword)
                
                # æ¨¡å¼åŒ¹é…
                for pattern in rules['patterns']:
                    if re.search(pattern, processed_text, re.IGNORECASE):
                        score += 1.5  # æ¨¡å¼åŒ¹é…æƒé‡æ›´é«˜
                        matched_patterns.append(pattern)
                
                # è´Ÿæ¨¡å¼æ£€æŸ¥ï¼ˆæ’é™¤ä¸ç›¸å…³çš„çŸ¥è¯†ç‚¹ï¼‰
                negative_match = False
                for neg_pattern in rules.get('negative_patterns', []):
                    if re.search(neg_pattern, processed_text, re.IGNORECASE):
                        negative_match = True
                        break
                
                # è®¡ç®—ç½®ä¿¡åº¦
                if score > 0 and not negative_match:
                    # åŸºç¡€ç½®ä¿¡åº¦
                    base_confidence = rules.get('confidence_base', 0.5)
                    
                    # åŸºäºåŒ¹é…å¼ºåº¦è°ƒæ•´ç½®ä¿¡åº¦
                    keyword_bonus = min(len(matched_keywords) * 0.1, 0.3)
                    pattern_bonus = min(len(matched_patterns) * 0.15, 0.4)
                    
                    final_confidence = min(base_confidence + keyword_bonus + pattern_bonus, 1.0)
                    
                    # åº”ç”¨æƒé‡
                    weighted_score = score * rules.get('weight', 1.0)
                    
                    extractions.append({
                        'knowledge_point': knowledge_point,
                        'subject': subject,
                        'confidence': round(final_confidence, 3),
                        'raw_score': score,
                        'weighted_score': weighted_score,
                        'matched_keywords': matched_keywords,
                        'matched_patterns': matched_patterns,
                        'extraction_method': 'rule_based'
                    })
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        extractions.sort(key=lambda x: x['confidence'], reverse=True)
        
        return extractions
    
    def extract_by_tfidf(self, question_text: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """åŸºäºTF-IDFçš„çŸ¥è¯†ç‚¹æå–"""
        if not ML_AVAILABLE:
            return []
        
        try:
            # é¢„å¤„ç†æ–‡æœ¬
            processed_text = self.preprocess_text(question_text)
            
            # æ„å»ºçŸ¥è¯†ç‚¹æ–‡æ¡£åº“
            knowledge_docs = []
            knowledge_labels = []
            
            for subject, subject_rules in self.knowledge_rules.items():
                for kp, rules in subject_rules.items():
                    # åˆå¹¶å…³é”®è¯å’Œæ¨¡å¼ä½œä¸ºæ–‡æ¡£
                    doc_text = ' '.join(rules['keywords'])
                    # ä»æ¨¡å¼ä¸­æå–æœ‰æ„ä¹‰çš„è¯æ±‡
                    for pattern in rules['patterns']:
                        # ç®€å•æå–æ¨¡å¼ä¸­çš„ä¸­æ–‡è¯æ±‡
                        pattern_words = re.findall(r'[\u4e00-\u9fa5]+', pattern)
                        doc_text += ' ' + ' '.join(pattern_words)
                    
                    knowledge_docs.append(doc_text)
                    knowledge_labels.append((subject, kp))
            
            if not knowledge_docs:
                return []
            
            # åˆå§‹åŒ–TF-IDFå‘é‡åŒ–å™¨
            if not hasattr(self, '_tfidf_extractor'):
                self._tfidf_extractor = TfidfVectorizer(
                    tokenizer=lambda x: list(jieba.cut(x)),
                    lowercase=True,
                    max_features=1000,
                    min_df=1,
                    max_df=0.95
                )
                # æ‹ŸåˆçŸ¥è¯†ç‚¹æ–‡æ¡£
                self._tfidf_extractor.fit(knowledge_docs)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            question_vector = self._tfidf_extractor.transform([processed_text])
            knowledge_vectors = self._tfidf_extractor.transform(knowledge_docs)
            
            similarities = cosine_similarity(question_vector, knowledge_vectors).flatten()
            
            # è·å–Top-Kç»“æœ
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            
            extractions = []
            for idx in top_indices:
                if similarities[idx] > 0.1:  # æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼
                    subject, kp = knowledge_labels[idx]
                    extractions.append({
                        'knowledge_point': kp,
                        'subject': subject,
                        'confidence': round(similarities[idx], 3),
                        'similarity_score': similarities[idx],
                        'extraction_method': 'tfidf_based'
                    })
            
            return extractions
            
        except Exception as e:
            logger.error(f"TF-IDFçŸ¥è¯†ç‚¹æå–å¤±è´¥: {e}")
            return []
    
    def extract_by_ensemble(self, question_text: str, subject_hint: str = None, 
                           top_k: int = 5) -> List[Dict[str, Any]]:
        """é›†æˆæ–¹æ³•çš„çŸ¥è¯†ç‚¹æå–"""
        
        # 1. åŸºäºè§„åˆ™çš„æå–
        rule_extractions = self.extract_by_rules(question_text, subject_hint)
        
        # 2. åŸºäºTF-IDFçš„æå–
        tfidf_extractions = self.extract_by_tfidf(question_text, top_k * 2)
        
        # 3. åŸºäºBERTçš„è¯­ä¹‰æå–
        bert_extractions = self.extract_by_bert(question_text, top_k * 2)
        
        # 4. åˆå¹¶ç»“æœ
        combined_results = {}
        
        # å¤„ç†è§„åˆ™æå–ç»“æœ
        for extraction in rule_extractions:
            key = f"{extraction['subject']}.{extraction['knowledge_point']}"
            if key not in combined_results:
                combined_results[key] = {
                    'knowledge_point': extraction['knowledge_point'],
                    'subject': extraction['subject'],
                    'scores': {},
                    'methods': [],
                    'details': {}
                }
            
            combined_results[key]['scores']['rule_based'] = extraction['confidence']
            combined_results[key]['methods'].append('rule_based')
            combined_results[key]['details']['rule_based'] = {
                'matched_keywords': extraction.get('matched_keywords', []),
                'matched_patterns': extraction.get('matched_patterns', [])
            }
        
        # å¤„ç†TF-IDFæå–ç»“æœ
        for extraction in tfidf_extractions:
            key = f"{extraction['subject']}.{extraction['knowledge_point']}"
            if key not in combined_results:
                combined_results[key] = {
                    'knowledge_point': extraction['knowledge_point'],
                    'subject': extraction['subject'],
                    'scores': {},
                    'methods': [],
                    'details': {}
                }
            
            combined_results[key]['scores']['tfidf_based'] = extraction['confidence']
            combined_results[key]['methods'].append('tfidf_based')
            combined_results[key]['details']['tfidf_based'] = {
                'similarity_score': extraction.get('similarity_score', 0)
            }
        
        # å¤„ç†BERTæå–ç»“æœ
        for extraction in bert_extractions:
            key = f"{extraction['subject']}.{extraction['knowledge_point']}"
            if key not in combined_results:
                combined_results[key] = {
                    'knowledge_point': extraction['knowledge_point'],
                    'subject': extraction['subject'],
                    'scores': {},
                    'methods': [],
                    'details': {}
                }
            
            combined_results[key]['scores']['bert_based'] = extraction['confidence']
            combined_results[key]['methods'].append('bert_based')
            combined_results[key]['details']['bert_based'] = {
                'semantic_similarity': extraction.get('semantic_similarity', 0),
                'knowledge_description': extraction.get('knowledge_description', '')
            }
        
        # 5. è®¡ç®—ç»¼åˆå¾—åˆ†
        method_weights = {
            'rule_based': 0.4,    # è§„åˆ™æ–¹æ³•æƒé‡
            'tfidf_based': 0.2,   # TF-IDFä½œä¸ºè¡¥å……
            'bert_based': 0.4     # BERTè¯­ä¹‰ç†è§£æƒé‡
        }
        
        final_extractions = []
        for key, data in combined_results.items():
            # è®¡ç®—åŠ æƒå¹³å‡åˆ†
            total_score = 0.0
            total_weight = 0.0
            
            for method, weight in method_weights.items():
                if method in data['scores']:
                    total_score += data['scores'][method] * weight
                    total_weight += weight
            
            if total_weight > 0:
                avg_score = total_score / total_weight
                
                # å¤šæ–¹æ³•ä¸€è‡´æ€§åŠ æˆ
                consistency_bonus = 0.1 * (len(data['methods']) - 1)
                final_score = min(avg_score + consistency_bonus, 1.0)
                
                final_extractions.append({
                    'knowledge_point': data['knowledge_point'],
                    'subject': data['subject'],
                    'confidence': round(final_score, 3),
                    'extraction_methods': data['methods'],
                    'method_scores': {k: round(v, 3) for k, v in data['scores'].items()},
                    'extraction_details': data['details'],
                    'consistency_score': len(data['methods'])
                })
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        final_extractions.sort(key=lambda x: x['confidence'], reverse=True)
        
        # æ›´æ–°ç»Ÿè®¡
        self.extraction_stats['total_extractions'] += 1
        if final_extractions:
            self.extraction_stats['successful_extractions'] += 1
            for extraction in final_extractions[:top_k]:
                kp = extraction['knowledge_point']
                subject = extraction['subject']
                self.extraction_stats['knowledge_point_frequency'][kp] += 1
                self.extraction_stats['subject_distribution'][subject] += 1
        
        return final_extractions[:top_k]
    
    def batch_extract(self, questions: List[str], subject_hints: List[str] = None, 
                     top_k: int = 3) -> List[List[Dict[str, Any]]]:
        """æ‰¹é‡çŸ¥è¯†ç‚¹æå–"""
        results = []
        subject_hints = subject_hints or [None] * len(questions)
        
        for i, question in enumerate(questions):
            subject_hint = subject_hints[i] if i < len(subject_hints) else None
            extractions = self.extract_by_ensemble(question, subject_hint, top_k)
            results.append(extractions)
        
        return results
    
    def evaluate_extraction_accuracy(self, test_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è¯„ä¼°æå–å‡†ç¡®æ€§"""
        if not test_data:
            return {'error': 'æµ‹è¯•æ•°æ®ä¸ºç©º'}
        
        total_questions = len(test_data)
        correct_predictions = 0
        method_accuracies = defaultdict(list)
        
        for test_case in test_data:
            question = test_case['question']
            true_knowledge_points = set(test_case['knowledge_points'])
            subject_hint = test_case.get('subject')
            
            # æå–çŸ¥è¯†ç‚¹
            extractions = self.extract_by_ensemble(question, subject_hint)
            predicted_knowledge_points = set([ext['knowledge_point'] for ext in extractions])
            
            # è®¡ç®—å‡†ç¡®æ€§
            intersection = true_knowledge_points.intersection(predicted_knowledge_points)
            if intersection:
                accuracy = len(intersection) / len(true_knowledge_points)
                if accuracy >= 0.5:  # è‡³å°‘50%åŒ¹é…æ‰ç®—æ­£ç¡®
                    correct_predictions += 1
                
                # è®°å½•å„æ–¹æ³•çš„å‡†ç¡®æ€§
                for extraction in extractions:
                    if extraction['knowledge_point'] in true_knowledge_points:
                        for method in extraction['extraction_methods']:
                            method_accuracies[method].append(1.0)
                    else:
                        for method in extraction['extraction_methods']:
                            method_accuracies[method].append(0.0)
        
        # è®¡ç®—æ€»ä½“å‡†ç¡®ç‡
        overall_accuracy = correct_predictions / total_questions
        
        # è®¡ç®—å„æ–¹æ³•å‡†ç¡®ç‡
        method_accuracy_summary = {}
        for method, accuracies in method_accuracies.items():
            if accuracies:
                method_accuracy_summary[method] = {
                    'accuracy': np.mean(accuracies),
                    'samples': len(accuracies)
                }
        
        return {
            'overall_accuracy': overall_accuracy,
            'correct_predictions': correct_predictions,
            'total_questions': total_questions,
            'method_accuracies': method_accuracy_summary,
            'extraction_stats': self.extraction_stats
        }
    
    def get_extraction_statistics(self) -> Dict[str, Any]:
        """è·å–æå–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.extraction_stats.copy()
        
        # è®¡ç®—æˆåŠŸç‡
        if stats['total_extractions'] > 0:
            stats['success_rate'] = stats['successful_extractions'] / stats['total_extractions']
        else:
            stats['success_rate'] = 0.0
        
        # çŸ¥è¯†ç‚¹çƒ­åº¦æ’è¡Œ
        stats['top_knowledge_points'] = dict(
            sorted(stats['knowledge_point_frequency'].items(), 
                  key=lambda x: x[1], reverse=True)[:10]
        )
        
        # å­¦ç§‘åˆ†å¸ƒ
        stats['subject_distribution_summary'] = dict(stats['subject_distribution'])
        
        return stats
    
    def save_model(self):
        """ä¿å­˜æ¨¡å‹å’Œç»Ÿè®¡æ•°æ®"""
        model_data = {
            'knowledge_rules': self.knowledge_rules,
            'extraction_stats': self.extraction_stats,
            'version': '1.0.0',
            'created_at': datetime.now().isoformat()
        }
        
        save_path = os.path.join(self.model_path, 'knowledge_extractor.json')
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(model_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"çŸ¥è¯†ç‚¹æå–å™¨æ¨¡å‹å·²ä¿å­˜: {save_path}")
        return save_path
    
    def _load_models(self):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        model_path = os.path.join(self.model_path, 'knowledge_extractor.json')
        
        if os.path.exists(model_path):
            try:
                with open(model_path, 'r', encoding='utf-8') as f:
                    model_data = json.load(f)
                
                if 'knowledge_rules' in model_data:
                    self.knowledge_rules.update(model_data['knowledge_rules'])
                
                if 'extraction_stats' in model_data:
                    saved_stats = model_data['extraction_stats']
                    self.extraction_stats.update(saved_stats)
                
                logger.info(f"å·²åŠ è½½çŸ¥è¯†ç‚¹æå–å™¨æ¨¡å‹: {model_path}")
            except Exception as e:
                logger.warning(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
    
    def _init_bert_models(self):
        """åˆå§‹åŒ–BERTæ¨¡å‹"""
        if not BERT_AVAILABLE:
            logger.warning("BERTåº“ä¸å¯ç”¨ï¼Œè·³è¿‡BERTæ¨¡å‹åˆå§‹åŒ–")
            return
        
        try:
            # ä½¿ç”¨è½»é‡çº§çš„ä¸­æ–‡BERTæ¨¡å‹
            model_name = "distilbert-base-multilingual-cased"
            
            # åˆå§‹åŒ–SentenceTransformerï¼ˆæ›´é€‚åˆè¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—ï¼‰
            self.sentence_transformer = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            logger.info("âœ… SentenceTransformeræ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # é¢„è®¡ç®—çŸ¥è¯†ç‚¹embeddings
            self._precompute_knowledge_embeddings()
            
        except Exception as e:
            logger.error(f"BERTæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.sentence_transformer = None
    
    def _precompute_knowledge_embeddings(self):
        """é¢„è®¡ç®—çŸ¥è¯†ç‚¹embeddings"""
        if not self.sentence_transformer:
            return
        
        try:
            knowledge_texts = []
            knowledge_labels = []
            
            for subject, subject_rules in self.knowledge_rules.items():
                for kp, rules in subject_rules.items():
                    # æ„å»ºçŸ¥è¯†ç‚¹çš„æ–‡æœ¬æè¿°
                    kp_text = f"{subject} {kp} {' '.join(rules['keywords'])}"
                    knowledge_texts.append(kp_text)
                    knowledge_labels.append((subject, kp))
            
            if knowledge_texts:
                # è®¡ç®—embeddings
                self.knowledge_embeddings = self.sentence_transformer.encode(knowledge_texts)
                self.knowledge_labels = knowledge_labels
                logger.info(f"âœ… é¢„è®¡ç®—äº† {len(knowledge_texts)} ä¸ªçŸ¥è¯†ç‚¹çš„embeddings")
            
        except Exception as e:
            logger.error(f"é¢„è®¡ç®—çŸ¥è¯†ç‚¹embeddingså¤±è´¥: {e}")
            self.knowledge_embeddings = None
    
    def extract_by_bert(self, question_text: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """åŸºäºBERT embeddingsçš„è¯­ä¹‰æå–"""
        if not BERT_AVAILABLE or not self.sentence_transformer or self.knowledge_embeddings is None:
            logger.warning("BERTæ¨¡å‹ä¸å¯ç”¨ï¼Œè·³è¿‡è¯­ä¹‰æå–")
            return []
        
        try:
            # é¢„å¤„ç†é—®é¢˜æ–‡æœ¬
            processed_text = self.preprocess_text(question_text)
            
            # è®¡ç®—é—®é¢˜æ–‡æœ¬çš„embedding
            question_embedding = self.sentence_transformer.encode([processed_text])
            
            # è®¡ç®—ä¸æ‰€æœ‰çŸ¥è¯†ç‚¹çš„ç›¸ä¼¼åº¦
            similarities = cosine_similarity(question_embedding, self.knowledge_embeddings).flatten()
            
            # è·å–Top-Kç»“æœ
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            
            extractions = []
            for idx in top_indices:
                if similarities[idx] > 0.3:  # è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼
                    subject, kp = self.knowledge_labels[idx]
                    
                    # è·å–å¯¹åº”çš„è§„åˆ™ä¿¡æ¯
                    rules = self.knowledge_rules[subject][kp]
                    
                    extractions.append({
                        'knowledge_point': kp,
                        'subject': subject,
                        'confidence': round(similarities[idx], 3),
                        'semantic_similarity': similarities[idx],
                        'extraction_method': 'bert_based',
                        'knowledge_description': f"{subject} {kp} {' '.join(rules['keywords'])}"
                    })
            
            return extractions
            
        except Exception as e:
            logger.error(f"BERTè¯­ä¹‰æå–å¤±è´¥: {e}")
            return []


def test_knowledge_extractor():
    """æµ‹è¯•çŸ¥è¯†ç‚¹æå–å™¨"""
    print("ğŸ§ª æµ‹è¯•é«˜ç²¾åº¦çŸ¥è¯†ç‚¹æå–ç³»ç»Ÿ...")
    
    extractor = KnowledgeExtractor()
    
    # æµ‹è¯•ç”¨ä¾‹
    test_questions = [
        {
            'question': 'è§£ä¸€å…ƒä¸€æ¬¡æ–¹ç¨‹ï¼š2x + 3 = 7ï¼Œæ±‚xçš„å€¼',
            'expected_subject': 'math',
            'expected_kp': ['equations']
        },
        {
            'question': 'å·²çŸ¥ä¸‰è§’å½¢ABCçš„ä¸‰è¾¹é•¿åˆ†åˆ«ä¸º3ã€4ã€5ï¼Œæ±‚è¯¥ä¸‰è§’å½¢çš„é¢ç§¯',
            'expected_subject': 'math',
            'expected_kp': ['geometry_triangle']
        },
        {
            'question': 'å·²çŸ¥äºŒæ¬¡å‡½æ•°y = xÂ² - 2x + 1ï¼Œæ±‚å…¶é¡¶ç‚¹åæ ‡å’Œå¯¹ç§°è½´',
            'expected_subject': 'math',
            'expected_kp': ['functions_quadratic']
        },
        {
            'question': 'åˆ†æã€Šæ˜¥æ™“ã€‹è¿™é¦–å¤è¯—ä¸­è¯—äººè¡¨è¾¾çš„æƒ…æ„Ÿå’Œè¿ç”¨çš„æ„è±¡',
            'expected_subject': 'chinese',
            'expected_kp': ['poetry_analysis']
        },
        {
            'question': 'ä¸€ç‰©ä½“ä»¥10m/sçš„åˆé€Ÿåº¦åšåŒ€åŠ é€Ÿè¿åŠ¨ï¼ŒåŠ é€Ÿåº¦ä¸º2m/sÂ²ï¼Œæ±‚5ç§’åçš„é€Ÿåº¦',
            'expected_subject': 'physics',
            'expected_kp': ['mechanics_motion']
        }
    ]
    
    print(f"æµ‹è¯• {len(test_questions)} ä¸ªé—®é¢˜...")
    
    for i, test_case in enumerate(test_questions, 1):
        question = test_case['question']
        expected_subject = test_case['expected_subject']
        
        print(f"\n{i}. {question}")
        
        # æå–çŸ¥è¯†ç‚¹
        extractions = extractor.extract_by_ensemble(question, expected_subject, top_k=3)
        
        if extractions:
            print("   æå–ç»“æœ:")
            for j, extraction in enumerate(extractions, 1):
                print(f"   {j}. {extraction['knowledge_point']} ({extraction['subject']})")
                print(f"      ç½®ä¿¡åº¦: {extraction['confidence']:.3f}")
                print(f"      æ–¹æ³•: {', '.join(extraction['extraction_methods'])}")
                
                # æ˜¾ç¤ºå„æ–¹æ³•å¾—åˆ†
                if 'method_scores' in extraction:
                    scores = extraction['method_scores']
                    score_str = ', '.join([f"{method}: {score:.3f}" for method, score in scores.items()])
                    print(f"      å„æ–¹æ³•å¾—åˆ†: {score_str}")
        else:
            print("   â†’ æœªæå–åˆ°çŸ¥è¯†ç‚¹")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = extractor.get_extraction_statistics()
    print(f"\nğŸ“Š æå–ç»Ÿè®¡:")
    print(f"   æ€»æå–æ¬¡æ•°: {stats['total_extractions']}")
    print(f"   æˆåŠŸæå–æ¬¡æ•°: {stats['successful_extractions']}")
    print(f"   æˆåŠŸç‡: {stats['success_rate']:.2%}")
    
    if stats['top_knowledge_points']:
        print(f"   çƒ­é—¨çŸ¥è¯†ç‚¹: {list(stats['top_knowledge_points'].keys())[:3]}")
    
    print("âœ… æµ‹è¯•å®Œæˆ")
    return extractor


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_knowledge_extractor()
