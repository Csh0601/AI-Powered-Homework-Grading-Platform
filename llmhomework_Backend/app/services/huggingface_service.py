"""
HuggingFaceæœåŠ¡å®¢æˆ·ç«¯ - ä¼˜å…ˆè¿æ¥æœåŠ¡å™¨HuggingFaceæ¨¡å‹
æ™ºèƒ½åˆ‡æ¢ï¼šHuggingFace (ä¸»è¦) â†’ Ollama (å¤‡ç”¨)
"""
import requests
import json
import logging
from typing import Dict, Any, Optional
from app.config import Config

logger = logging.getLogger(__name__)

class HuggingFaceService:
    """æ™ºèƒ½HuggingFaceå®¢æˆ·ç«¯ - ä¼˜å…ˆHuggingFaceï¼Œå¤‡ç”¨Ollama"""
    
    def __init__(self):
        # ç›´æ¥è°ƒç”¨LoRAæœåŠ¡é…ç½®
        self.qwen_vl_url = getattr(Config, 'QWEN_VL_API_URL', 'http://172.31.179.77:8007')
        self.llm_provider = getattr(Config, 'LLM_PROVIDER', 'qwen_vl_lora_direct')
        self.multimodal_enabled = getattr(Config, 'MULTIMODAL_ENABLED', True)
        self.use_direct_service = getattr(Config, 'USE_DIRECT_LORA_SERVICE', True)
        self.bypass_rag = getattr(Config, 'BYPASS_RAG_SERVICE', True)
        
        # å…¼å®¹æ—§é…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self.huggingface_url = getattr(Config, 'HUGGINGFACE_BASE_URL', self.qwen_vl_url)
        self.huggingface_model = getattr(Config, 'HUGGINGFACE_MODEL', 'Qwen2.5-VL-LoRA')
        self.use_huggingface = getattr(Config, 'USE_HUGGINGFACE', False)
        
        # Ollamaé…ç½® (å·²å¼ƒç”¨ï¼Œä¿ç•™å…¼å®¹æ€§)
        self.ollama_url = getattr(Config, 'OLLAMA_BASE_URL', None)
        self.ollama_model = getattr(Config, 'QWEN_MODEL_NAME', None)
        
        # é€šç”¨é…ç½®
        self.max_tokens = Config.MAX_TOKENS
        self.timeout = Config.TIMEOUT_SECONDS
        self.retry_attempts = Config.RETRY_ATTEMPTS
        
        logger.info(f"AIæœåŠ¡åˆå§‹åŒ– (ç›´æ¥è°ƒç”¨LoRAé…ç½®):")
        logger.info(f"  LoRAæœåŠ¡: {self.qwen_vl_url}")
        logger.info(f"  LLMæä¾›å•†: {self.llm_provider}")
        logger.info(f"  å¤šæ¨¡æ€å¯ç”¨: {self.multimodal_enabled}")
        logger.info(f"  ç›´æ¥è°ƒç”¨: {self.use_direct_service}")
        logger.info(f"  ç»•è¿‡RAG: {self.bypass_rag}")
        if self.ollama_url:
            logger.info(f"  å¤‡ç”¨æœåŠ¡: {self.ollama_url} (æ¨¡å‹: {self.ollama_model})")
        
    def chat_completion(self, prompt: str, max_tokens: Optional[int] = None) -> Dict[str, Any]:
        """æ™ºèƒ½èŠå¤©å®Œæˆ - è‡ªåŠ¨åˆ‡æ¢æœåŠ¡"""
        if max_tokens is None:
            max_tokens = self.max_tokens
            
        logger.info(f"å¼€å§‹AIå¯¹è¯ï¼ŒPrompté•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        # ğŸ¥‡ ä¼˜å…ˆå°è¯•HuggingFace
        if self.use_huggingface:
            try:
                logger.info("ğŸ¤– å°è¯•è¿æ¥HuggingFace Qwen3-30B...")
                response = self._call_huggingface(prompt, max_tokens)
                if response and response.get("success"):
                    logger.info("âœ… HuggingFaceè°ƒç”¨æˆåŠŸ")
                    return response
            except Exception as e:
                logger.warning(f"âš ï¸ HuggingFaceè°ƒç”¨å¤±è´¥: {e}")
        
        # ğŸ¥ˆ å¤‡ç”¨OllamaæœåŠ¡ 
        try:
            logger.info("ğŸ”„ åˆ‡æ¢åˆ°Ollamaå¤‡ç”¨æœåŠ¡...")
            response = self._call_ollama(prompt, max_tokens)
            if response and response.get("success"):
                logger.info("âœ… Ollamaè°ƒç”¨æˆåŠŸ")
                return response
        except Exception as e:
            logger.error(f"âŒ Ollamaè°ƒç”¨å¤±è´¥: {e}")
            
        # ğŸš¨ æ‰€æœ‰æœåŠ¡éƒ½å¤±è´¥
        logger.error("âŒ æ‰€æœ‰LLMæœåŠ¡å‡ä¸å¯ç”¨")
        return {
            "success": False,
            "error": "æ‰€æœ‰LLMæœåŠ¡å‡ä¸å¯ç”¨",
            "response": "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚",
            "model_used": "none"
        }
    
    def _call_huggingface(self, prompt: str, max_tokens: int) -> Dict[str, Any]:
        """è°ƒç”¨HuggingFaceæœåŠ¡ - ä½¿ç”¨å·²éªŒè¯çš„APIæ ¼å¼"""
        # âœ… ä½¿ç”¨å·²éªŒè¯å·¥ä½œçš„ç«¯ç‚¹
        url = f"{self.huggingface_url}/api/v1/chat"
        
        # âœ… ä½¿ç”¨å·²éªŒè¯çš„è¯·æ±‚æ ¼å¼
        payload = {
            "query": prompt,  # æœåŠ¡å™¨æœŸæœ›çš„å­—æ®µå
            "max_new_tokens": max_tokens,
            "temperature": 0.7
        }
        
        logger.debug(f"è°ƒç”¨HuggingFace API: {url}")
        logger.debug(f"è¯·æ±‚æ•°æ®: {json.dumps(payload, ensure_ascii=False)[:100]}...")
        
        try:
            response = requests.post(
                url, 
                json=payload, 
                timeout=self.timeout,
                headers={"Content-Type": "application/json"}
            )
            
            if response.status_code == 200:
                data = response.json()
                
                # æ£€æŸ¥æœåŠ¡å™¨é”™è¯¯
                if "error" in data:
                    return {"success": False, "error": data["error"]}
                
                # âœ… ä½¿ç”¨å·²éªŒè¯çš„å“åº”æ ¼å¼
                response_text = data.get("response", "")
                
                if response_text and response_text.strip():
                    return {
                        "success": True,
                        "response": response_text,
                        "model_used": f"HuggingFace-{data.get('model', self.huggingface_model)}",
                        "tokens_used": data.get("max_tokens_used", len(response_text.split()))
                    }
                else:
                    return {"success": False, "error": "ç©ºå“åº”"}
            else:
                return {
                    "success": False, 
                    "error": f"HTTP {response.status_code}: {response.text}"
                }
        
        except Exception as e:
            logger.error(f"HuggingFace APIè°ƒç”¨å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def _call_ollama(self, prompt: str, max_tokens: int) -> Dict[str, Any]:
        """è°ƒç”¨Ollamaå¤‡ç”¨æœåŠ¡"""
        url = f"{self.ollama_url}/api/generate"
        
        payload = {
            "model": self.ollama_model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "num_predict": max_tokens,
                "temperature": 0.7,
                "top_p": 0.9,
                "top_k": 40
            }
        }
        
        logger.debug(f"Ollamaè¯·æ±‚: {url}")
        
        response = requests.post(
            url,
            json=payload,
            timeout=self.timeout,
            headers={"Content-Type": "application/json"}
        )
        response.raise_for_status()
        
        data = response.json()
        
        return {
            "success": True, 
            "response": data.get("response", ""),
            "model_used": f"Ollama-{self.ollama_model}",
            "tokens_used": len(data.get("response", "").split()),
            "server_info": {
                "model": data.get("model", ""),
                "created_at": data.get("created_at", "")
            }
        }
    
    def test_connection(self) -> Dict[str, Any]:
        """æµ‹è¯•æœåŠ¡å™¨è¿æ¥çŠ¶æ€"""
        results = {
            "huggingface": {"available": False, "error": None},
            "ollama": {"available": False, "error": None},
            "recommended_service": None
        }
        
        # æµ‹è¯•HuggingFace
        try:
            url = f"{self.huggingface_url}/"
            response = requests.get(url, timeout=30)
            if response.status_code == 200:
                results["huggingface"]["available"] = True
                results["huggingface"]["status"] = response.json()
                logger.info("âœ… HuggingFaceæœåŠ¡è¿æ¥æ­£å¸¸")
            else:
                results["huggingface"]["error"] = f"HTTP {response.status_code}"
        except Exception as e:
            results["huggingface"]["error"] = str(e)
            logger.warning(f"âŒ HuggingFaceè¿æ¥å¤±è´¥: {e}")
        
        # æµ‹è¯•Ollama
        try:
            url = f"{self.ollama_url}/api/tags"
            response = requests.get(url, timeout=30)
            if response.status_code == 200:
                results["ollama"]["available"] = True
                results["ollama"]["models"] = response.json()
                logger.info("âœ… OllamaæœåŠ¡è¿æ¥æ­£å¸¸")
            else:
                results["ollama"]["error"] = f"HTTP {response.status_code}"
        except Exception as e:
            results["ollama"]["error"] = str(e)
            logger.warning(f"âŒ Ollamaè¿æ¥å¤±è´¥: {e}")
        
        # æ¨èæœåŠ¡
        if results["huggingface"]["available"]:
            results["recommended_service"] = "huggingface"
        elif results["ollama"]["available"]:
            results["recommended_service"] = "ollama"
        else:
            results["recommended_service"] = "none"
        
        return results

# å…¨å±€å®ä¾‹
_huggingface_service = None

def get_huggingface_service() -> HuggingFaceService:
    """è·å–HuggingFaceæœåŠ¡å•ä¾‹"""
    global _huggingface_service
    if _huggingface_service is None:
        _huggingface_service = HuggingFaceService()
    return _huggingface_service

def get_ai_service_status() -> Dict[str, Any]:
    """è·å–AIæœåŠ¡çŠ¶æ€"""
    service = get_huggingface_service()
    return service.test_connection()

# å…¼å®¹æ€§å‡½æ•°
def grade_homework_with_ai(questions, ocr_text: str = "") -> Dict[str, Any]:
    """ä½¿ç”¨AIè¿›è¡Œä½œä¸šæ‰¹æ”¹ - å…¼å®¹åŸæ¥å£"""
    service = get_huggingface_service()
    
    # æ„å»ºæ‰¹æ”¹æç¤º
    prompt = f"""è¯·ä½œä¸ºä¸€åèµ„æ·±æ•™å¸ˆï¼Œå¯¹ä»¥ä¸‹å­¦ç”Ÿä½œä¸šè¿›è¡Œè¯¦ç»†æ‰¹æ”¹ï¼š

OCRè¯†åˆ«ç»“æœ:
{ocr_text}

é¢˜ç›®ä¿¡æ¯:
{json.dumps(questions, ensure_ascii=False, indent=2)}

è¯·æä¾›:
1. æ¯é¢˜çš„è¯¦ç»†æ‰¹æ”¹æ„è§
2. é”™è¯¯ç‚¹åˆ†æ
3. æ”¹è¿›å»ºè®®
4. æ€»ä½“è¯„ä»·

è¯·ç”¨ä¸­æ–‡å›å¤ï¼Œæ ¼å¼æ¸…æ™°ã€‚"""
    
    result = service.chat_completion(prompt)
    
    return {
        "knowledge_analysis": result.get("response", ""),
        "practice_questions": "",
        "multimodal_analysis": f"ä½¿ç”¨æ¨¡å‹: {result.get('model_used', 'unknown')}",
        "ai_service_status": result.get("success", False),
        "error": result.get("error", None)
    }
