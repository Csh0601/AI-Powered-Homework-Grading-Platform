#!/usr/bin/env python3
"""
é¢˜ç›®åˆ°çŸ¥è¯†ç‚¹çš„æ™ºèƒ½åŒ¹é…ç®—æ³•
åŸºäºTF-IDFå…³é”®è¯åŒ¹é…å’Œè¯å‘é‡è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—

ä¸»è¦åŠŸèƒ½ï¼š
1. TF-IDFå…³é”®è¯åŒ¹é…ç®—æ³•
2. è¯å‘é‡è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
3. å¤šæ ‡ç­¾åˆ†ç±»æœºåˆ¶
4. çŸ¥è¯†ç‚¹æƒé‡è¯„åˆ†
5. æ™ºèƒ½æ¨èç³»ç»Ÿ

æŠ€æœ¯è¦ç‚¹ï¼š
- ä¿¡æ¯æ£€ç´¢å’Œè¯­ä¹‰åŒ¹é…
- å¤šç»´åº¦ç‰¹å¾æå–
- æœºå™¨å­¦ä¹ åˆ†ç±»
- ç»“æœå¯è§£é‡Šæ€§
"""

import re
import jieba
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional, Set
from collections import defaultdict, Counter
import json
import os
import logging
from datetime import datetime
import pickle

# æœºå™¨å­¦ä¹ å’ŒNLPåº“
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.decomposition import LatentDirichletAllocation
    from sklearn.cluster import KMeans
    from sklearn.multiclass import MultiLabelBinarizer
    import gensim
    from gensim.models import Word2Vec
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    print("âš ï¸ æœºå™¨å­¦ä¹ åº“æœªå®Œå…¨å®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½å—é™")

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class KnowledgeMatcher:
    """é¢˜ç›®åˆ°çŸ¥è¯†ç‚¹çš„æ™ºèƒ½åŒ¹é…å™¨"""
    
    def __init__(self, model_path: str = None):
        """åˆå§‹åŒ–åŒ¹é…å™¨"""
        self.model_path = model_path or os.path.join(
            os.path.dirname(__file__), '..', '..', 'models', 'knowledge_matcher'
        )
        
        # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
        os.makedirs(self.model_path, exist_ok=True)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.tfidf_vectorizer = None
        self.word2vec_model = None
        self.knowledge_vectors = {}
        self.knowledge_keywords = {}
        self.label_binarizer = MultiLabelBinarizer() if ML_AVAILABLE else None
        
        # é¢„å®šä¹‰çŸ¥è¯†ç‚¹ä½“ç³»
        self._init_knowledge_system()
        
        # åŒ¹é…ç»Ÿè®¡
        self.matching_stats = {
            'total_matches': 0,
            'method_usage': {
                'tfidf': 0,
                'semantic': 0,
                'keyword': 0,
                'ensemble': 0
            },
            'accuracy_distribution': defaultdict(int),
            'knowledge_point_coverage': defaultdict(int)
        }
        
        # å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        self._load_models()
    
    def _init_knowledge_system(self):
        """åˆå§‹åŒ–çŸ¥è¯†ç‚¹ä½“ç³»"""
        self.knowledge_system = {
            'math': {
                'algebra': {
                    'linear_equations': {
                        'name': 'ä¸€æ¬¡æ–¹ç¨‹',
                        'keywords': ['æ–¹ç¨‹', 'æœªçŸ¥æ•°', 'è§£', 'ç­‰å¼', 'æ¶ˆå…ƒ', 'ä»£å…¥'],
                        'patterns': [r'è§£.*?æ–¹ç¨‹', r'æ±‚.*?çš„å€¼', r'è®¾.*?ä¸º.*?'],
                        'difficulty': 2,
                        'grade_level': 7
                    },
                    'quadratic_equations': {
                        'name': 'äºŒæ¬¡æ–¹ç¨‹',
                        'keywords': ['äºŒæ¬¡æ–¹ç¨‹', 'åˆ¤åˆ«å¼', 'æ±‚æ ¹å…¬å¼', 'é¡¶ç‚¹', 'å¼€å£'],
                        'patterns': [r'äºŒæ¬¡.*?æ–¹ç¨‹', r'æŠ›ç‰©çº¿', r'å¼€å£.*?å‘'],
                        'difficulty': 3,
                        'grade_level': 8
                    },
                    'inequalities': {
                        'name': 'ä¸ç­‰å¼',
                        'keywords': ['ä¸ç­‰å¼', 'å¤§äº', 'å°äº', 'ä¸ç­‰å·', 'è§£é›†'],
                        'patterns': [r'ä¸ç­‰å¼.*?è§£', r'[>â‰¥<â‰¤]', r'è§£é›†'],
                        'difficulty': 2,
                        'grade_level': 7
                    }
                },
                'geometry': {
                    'triangles': {
                        'name': 'ä¸‰è§’å½¢',
                        'keywords': ['ä¸‰è§’å½¢', 'è¾¹é•¿', 'è§’åº¦', 'é¢ç§¯', 'å‘¨é•¿', 'å‹¾è‚¡å®šç†'],
                        'patterns': [r'ä¸‰è§’å½¢.*?(é¢ç§¯|å‘¨é•¿)', r'â–³', r'å‹¾è‚¡å®šç†'],
                        'difficulty': 2,
                        'grade_level': 7
                    },
                    'circles': {
                        'name': 'åœ†',
                        'keywords': ['åœ†', 'åŠå¾„', 'ç›´å¾„', 'å‘¨é•¿', 'é¢ç§¯', 'å¼§é•¿'],
                        'patterns': [r'åœ†.*?(é¢ç§¯|å‘¨é•¿)', r'åŠå¾„.*?ä¸º', r'â—‹'],
                        'difficulty': 3,
                        'grade_level': 8
                    },
                    'quadrilaterals': {
                        'name': 'å››è¾¹å½¢',
                        'keywords': ['å››è¾¹å½¢', 'çŸ©å½¢', 'æ­£æ–¹å½¢', 'å¹³è¡Œå››è¾¹å½¢', 'æ¢¯å½¢'],
                        'patterns': [r'å››è¾¹å½¢', r'çŸ©å½¢.*?(é¢ç§¯|å‘¨é•¿)', r'â–¡'],
                        'difficulty': 2,
                        'grade_level': 7
                    }
                },
                'functions': {
                    'linear_functions': {
                        'name': 'ä¸€æ¬¡å‡½æ•°',
                        'keywords': ['ä¸€æ¬¡å‡½æ•°', 'æ–œç‡', 'æˆªè·', 'å›¾åƒ', 'æ­£æ¯”ä¾‹'],
                        'patterns': [r'ä¸€æ¬¡å‡½æ•°', r'y=.*?x', r'æ–œç‡'],
                        'difficulty': 3,
                        'grade_level': 8
                    },
                    'quadratic_functions': {
                        'name': 'äºŒæ¬¡å‡½æ•°',
                        'keywords': ['äºŒæ¬¡å‡½æ•°', 'æŠ›ç‰©çº¿', 'é¡¶ç‚¹', 'å¯¹ç§°è½´', 'å¼€å£æ–¹å‘'],
                        'patterns': [r'äºŒæ¬¡å‡½æ•°', r'y=.*?xÂ²', r'æŠ›ç‰©çº¿'],
                        'difficulty': 4,
                        'grade_level': 9
                    }
                }
            },
            'chinese': {
                'literature': {
                    'poetry': {
                        'name': 'è¯—æ­Œ',
                        'keywords': ['è¯—æ­Œ', 'éŸµå¾‹', 'æ„è±¡', 'æƒ…æ„Ÿ', 'ä¿®è¾', 'å¤è¯—'],
                        'patterns': [r'è¯—æ­Œ.*?èµæ', r'å¤è¯—.*?ç†è§£'],
                        'difficulty': 3,
                        'grade_level': 8
                    },
                    'prose': {
                        'name': 'æ•£æ–‡',
                        'keywords': ['æ•£æ–‡', 'æŠ’æƒ…', 'å™äº‹', 'æå†™', 'è®®è®º'],
                        'patterns': [r'æ•£æ–‡.*?åˆ†æ', r'ä½œè€….*?æƒ…æ„Ÿ'],
                        'difficulty': 3,
                        'grade_level': 8
                    }
                },
                'grammar': {
                    'sentence_structure': {
                        'name': 'å¥å¼ç»“æ„',
                        'keywords': ['ä¸»è¯­', 'è°“è¯­', 'å®¾è¯­', 'å®šè¯­', 'çŠ¶è¯­', 'è¡¥è¯­'],
                        'patterns': [r'å¥å­.*?æˆåˆ†', r'è¯­æ³•.*?åˆ†æ'],
                        'difficulty': 2,
                        'grade_level': 7
                    }
                }
            },
            'physics': {
                'mechanics': {
                    'motion': {
                        'name': 'è¿åŠ¨å­¦',
                        'keywords': ['é€Ÿåº¦', 'åŠ é€Ÿåº¦', 'ä½ç§»', 'æ—¶é—´', 'è¿åŠ¨'],
                        'patterns': [r'é€Ÿåº¦.*?è®¡ç®—', r'è¿åŠ¨.*?æ—¶é—´'],
                        'difficulty': 3,
                        'grade_level': 8
                    },
                    'force': {
                        'name': 'åŠ›å­¦',
                        'keywords': ['åŠ›', 'é‡åŠ›', 'æ‘©æ“¦åŠ›', 'å‹åŠ›', 'æ”¯æŒåŠ›'],
                        'patterns': [r'åŠ›.*?åˆ†æ', r'å—åŠ›.*?å›¾'],
                        'difficulty': 3,
                        'grade_level': 8
                    }
                }
            }
        }
        
        # æ‰å¹³åŒ–çŸ¥è¯†ç‚¹ä¾¿äºæŸ¥æ‰¾
        self.flat_knowledge_points = {}
        self._flatten_knowledge_system()
    
    def _flatten_knowledge_system(self):
        """æ‰å¹³åŒ–çŸ¥è¯†ç‚¹ä½“ç³»"""
        def traverse(node, path=""):
            for key, value in node.items():
                current_path = f"{path}.{key}" if path else key
                if isinstance(value, dict) and 'name' in value:
                    # è¿™æ˜¯ä¸€ä¸ªçŸ¥è¯†ç‚¹
                    self.flat_knowledge_points[current_path] = value
                elif isinstance(value, dict):
                    # è¿™æ˜¯ä¸€ä¸ªåˆ†ç±»ï¼Œç»§ç»­éå†
                    traverse(value, current_path)
        
        traverse(self.knowledge_system)
        
        logger.info(f"å·²åŠ è½½ {len(self.flat_knowledge_points)} ä¸ªçŸ¥è¯†ç‚¹")
    
    def preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'\s+', ' ', text.strip())
        
        # ä¿ç•™é‡è¦çš„æ•°å­¦ç¬¦å·
        math_symbols = ['=', '+', '-', 'Ã—', 'Ã·', '>', '<', 'â‰¥', 'â‰¤', 'â‰ ', 'â‰ˆ', 
                       'âˆš', '^', 'Â²', 'Â³', 'Ï€', 'âˆ', 'Â°', 'âˆ ', 'â–³', 'â–¡', 'â—‹']
        
        # åˆ†è¯å¹¶ä¿ç•™æ•°å­¦ç¬¦å·
        words = []
        for word in jieba.cut(text):
            word = word.strip()
            if word and (word.isalnum() or word in math_symbols or len(word) > 1):
                words.append(word)
        
        return ' '.join(words)
    
    def extract_keywords(self, text: str, top_k: int = 10) -> List[Tuple[str, float]]:
        """æå–å…³é”®è¯"""
        if not ML_AVAILABLE:
            # ç®€å•çš„å…³é”®è¯æå–
            words = jieba.cut(text)
            word_freq = Counter(words)
            return [(word, freq) for word, freq in word_freq.most_common(top_k)]
        
        # ä½¿ç”¨TF-IDFæå–å…³é”®è¯
        if not hasattr(self, '_keyword_vectorizer'):
            self._keyword_vectorizer = TfidfVectorizer(
                max_features=1000,
                stop_words=None,
                tokenizer=lambda x: list(jieba.cut(x)),
                lowercase=True
            )
            
            # æ„å»ºè¯­æ–™åº“ï¼ˆä½¿ç”¨çŸ¥è¯†ç‚¹æè¿°ï¼‰
            corpus = []
            for kp_id, kp_info in self.flat_knowledge_points.items():
                corpus.append(' '.join(kp_info['keywords']))
            
            if corpus:
                self._keyword_vectorizer.fit(corpus)
        
        try:
            tfidf_matrix = self._keyword_vectorizer.transform([text])
            feature_names = self._keyword_vectorizer.get_feature_names_out()
            scores = tfidf_matrix.toarray()[0]
            
            # è·å–Top-Kå…³é”®è¯
            top_indices = np.argsort(scores)[-top_k:][::-1]
            keywords = [(feature_names[i], scores[i]) for i in top_indices if scores[i] > 0]
            
            return keywords
        except:
            # é™çº§åˆ°ç®€å•å…³é”®è¯æå–
            words = jieba.cut(text)
            word_freq = Counter(words)
            return [(word, freq) for word, freq in word_freq.most_common(top_k)]
    
    def match_by_tfidf(self, question_text: str, top_k: int = 5) -> List[Tuple[str, float, Dict]]:
        """åŸºäºTF-IDFçš„çŸ¥è¯†ç‚¹åŒ¹é…"""
        if not ML_AVAILABLE:
            return []
        
        try:
            # é¢„å¤„ç†é—®é¢˜æ–‡æœ¬
            processed_text = self.preprocess_text(question_text)
            
            # æ„å»ºçŸ¥è¯†ç‚¹è¯­æ–™åº“
            knowledge_corpus = []
            knowledge_ids = []
            
            for kp_id, kp_info in self.flat_knowledge_points.items():
                # åˆå¹¶çŸ¥è¯†ç‚¹çš„å…³é”®è¯ã€åç§°ç­‰ä¿¡æ¯
                kp_text = ' '.join([
                    kp_info['name'],
                    ' '.join(kp_info['keywords']),
                    ' '.join(kp_info.get('patterns', []))
                ])
                knowledge_corpus.append(kp_text)
                knowledge_ids.append(kp_id)
            
            if not knowledge_corpus:
                return []
            
            # åˆå§‹åŒ–TF-IDFå‘é‡åŒ–å™¨
            vectorizer = TfidfVectorizer(
                tokenizer=lambda x: list(jieba.cut(x)),
                lowercase=True,
                max_features=1000
            )
            
            # æ‹Ÿåˆè¯­æ–™åº“
            tfidf_matrix = vectorizer.fit_transform(knowledge_corpus + [processed_text])
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            question_vector = tfidf_matrix[-1]  # æœ€åä¸€ä¸ªæ˜¯é—®é¢˜å‘é‡
            knowledge_vectors = tfidf_matrix[:-1]  # å‰é¢çš„æ˜¯çŸ¥è¯†ç‚¹å‘é‡
            
            similarities = cosine_similarity(question_vector, knowledge_vectors).flatten()
            
            # è·å–Top-KåŒ¹é…ç»“æœ
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            
            results = []
            for idx in top_indices:
                if similarities[idx] > 0.1:  # è®¾ç½®æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼
                    kp_id = knowledge_ids[idx]
                    kp_info = self.flat_knowledge_points[kp_id]
                    results.append((kp_id, similarities[idx], kp_info))
            
            # æ›´æ–°ç»Ÿè®¡
            self.matching_stats['method_usage']['tfidf'] += 1
            
            return results
            
        except Exception as e:
            logger.error(f"TF-IDFåŒ¹é…å¤±è´¥: {e}")
            return []
    
    def match_by_semantic_similarity(self, question_text: str, top_k: int = 5) -> List[Tuple[str, float, Dict]]:
        """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„çŸ¥è¯†ç‚¹åŒ¹é…"""
        if not ML_AVAILABLE:
            return self._simple_semantic_match(question_text, top_k)
        
        try:
            # é¢„å¤„ç†æ–‡æœ¬
            processed_text = self.preprocess_text(question_text)
            question_words = processed_text.split()
            
            if not question_words:
                return []
            
            # è®¡ç®—ä¸æ¯ä¸ªçŸ¥è¯†ç‚¹çš„è¯­ä¹‰ç›¸ä¼¼åº¦
            similarities = []
            
            for kp_id, kp_info in self.flat_knowledge_points.items():
                # è®¡ç®—è¯æ±‡é‡å åº¦
                kp_words = set(kp_info['keywords'] + [kp_info['name']])
                question_words_set = set(question_words)
                
                # Jaccardç›¸ä¼¼åº¦
                intersection = len(kp_words.intersection(question_words_set))
                union = len(kp_words.union(question_words_set))
                jaccard_sim = intersection / union if union > 0 else 0
                
                # å…³é”®è¯åŒ¹é…åº¦
                keyword_matches = sum(1 for word in question_words if word in kp_words)
                keyword_sim = keyword_matches / len(question_words) if question_words else 0
                
                # ç»¼åˆç›¸ä¼¼åº¦
                combined_sim = 0.6 * jaccard_sim + 0.4 * keyword_sim
                
                if combined_sim > 0.1:
                    similarities.append((kp_id, combined_sim, kp_info))
            
            # æ’åºå¹¶è¿”å›Top-K
            similarities.sort(key=lambda x: x[1], reverse=True)
            
            # æ›´æ–°ç»Ÿè®¡
            self.matching_stats['method_usage']['semantic'] += 1
            
            return similarities[:top_k]
            
        except Exception as e:
            logger.error(f"è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…å¤±è´¥: {e}")
            return []
    
    def _simple_semantic_match(self, question_text: str, top_k: int = 5) -> List[Tuple[str, float, Dict]]:
        """ç®€å•çš„è¯­ä¹‰åŒ¹é…ï¼ˆæ— éœ€æœºå™¨å­¦ä¹ åº“ï¼‰"""
        processed_text = self.preprocess_text(question_text).lower()
        question_words = set(processed_text.split())
        
        similarities = []
        
        for kp_id, kp_info in self.flat_knowledge_points.items():
            # å…³é”®è¯åŒ¹é…
            kp_keywords = set([word.lower() for word in kp_info['keywords']])
            
            # è®¡ç®—åŒ¹é…åº¦
            matches = len(question_words.intersection(kp_keywords))
            total_keywords = len(kp_keywords)
            
            if matches > 0:
                similarity = matches / max(len(question_words), total_keywords)
                similarities.append((kp_id, similarity, kp_info))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]
    
    def match_by_keyword_rules(self, question_text: str, top_k: int = 5) -> List[Tuple[str, float, Dict]]:
        """åŸºäºå…³é”®è¯è§„åˆ™çš„åŒ¹é…"""
        processed_text = self.preprocess_text(question_text).lower()
        
        matches = []
        
        for kp_id, kp_info in self.flat_knowledge_points.items():
            score = 0.0
            reasons = []
            
            # å…³é”®è¯åŒ¹é…
            for keyword in kp_info['keywords']:
                if keyword.lower() in processed_text:
                    score += 1.0
                    reasons.append(f"å…³é”®è¯:{keyword}")
            
            # æ¨¡å¼åŒ¹é…
            for pattern in kp_info.get('patterns', []):
                if re.search(pattern, processed_text, re.IGNORECASE):
                    score += 1.5  # æ¨¡å¼åŒ¹é…æƒé‡æ›´é«˜
                    reasons.append(f"æ¨¡å¼:{pattern}")
            
            # åç§°åŒ¹é…
            if kp_info['name'].lower() in processed_text:
                score += 2.0  # åç§°åŒ¹é…æƒé‡æœ€é«˜
                reasons.append(f"åç§°:{kp_info['name']}")
            
            if score > 0:
                # æ ‡å‡†åŒ–è¯„åˆ†
                normalized_score = min(score / 5.0, 1.0)  # æœ€é«˜åˆ†é™åˆ¶ä¸º1.0
                
                # æ·»åŠ é¢å¤–ä¿¡æ¯
                kp_info_extended = kp_info.copy()
                kp_info_extended['match_reasons'] = reasons
                kp_info_extended['raw_score'] = score
                
                matches.append((kp_id, normalized_score, kp_info_extended))
        
        # æ’åº
        matches.sort(key=lambda x: x[1], reverse=True)
        
        # æ›´æ–°ç»Ÿè®¡
        self.matching_stats['method_usage']['keyword'] += 1
        
        return matches[:top_k]
    
    def ensemble_match(self, question_text: str, top_k: int = 5, 
                      use_tfidf: bool = True, use_semantic: bool = True, 
                      use_keyword: bool = True) -> List[Dict[str, Any]]:
        """é›†æˆåŒ¹é…ç®—æ³•"""
        all_matches = {}
        
        # æƒé‡é…ç½®
        method_weights = {
            'tfidf': 0.4,
            'semantic': 0.3,
            'keyword': 0.3
        }
        
        # TF-IDFåŒ¹é…
        if use_tfidf:
            tfidf_matches = self.match_by_tfidf(question_text, top_k * 2)
            for kp_id, score, kp_info in tfidf_matches:
                if kp_id not in all_matches:
                    all_matches[kp_id] = {
                        'kp_id': kp_id,
                        'kp_info': kp_info,
                        'scores': {},
                        'total_score': 0.0,
                        'methods': []
                    }
                all_matches[kp_id]['scores']['tfidf'] = score
                all_matches[kp_id]['methods'].append('tfidf')
        
        # è¯­ä¹‰åŒ¹é…
        if use_semantic:
            semantic_matches = self.match_by_semantic_similarity(question_text, top_k * 2)
            for kp_id, score, kp_info in semantic_matches:
                if kp_id not in all_matches:
                    all_matches[kp_id] = {
                        'kp_id': kp_id,
                        'kp_info': kp_info,
                        'scores': {},
                        'total_score': 0.0,
                        'methods': []
                    }
                all_matches[kp_id]['scores']['semantic'] = score
                all_matches[kp_id]['methods'].append('semantic')
        
        # å…³é”®è¯åŒ¹é…
        if use_keyword:
            keyword_matches = self.match_by_keyword_rules(question_text, top_k * 2)
            for kp_id, score, kp_info in keyword_matches:
                if kp_id not in all_matches:
                    all_matches[kp_id] = {
                        'kp_id': kp_id,
                        'kp_info': kp_info,
                        'scores': {},
                        'total_score': 0.0,
                        'methods': []
                    }
                all_matches[kp_id]['scores']['keyword'] = score
                all_matches[kp_id]['methods'].append('keyword')
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        for match_data in all_matches.values():
            total_score = 0.0
            total_weight = 0.0
            
            for method, weight in method_weights.items():
                if method in match_data['scores']:
                    total_score += match_data['scores'][method] * weight
                    total_weight += weight
            
            if total_weight > 0:
                match_data['total_score'] = total_score / total_weight
            
            # å¤šæ–¹æ³•ä¸€è‡´æ€§åŠ æˆ
            if len(match_data['methods']) > 1:
                consistency_bonus = 0.1 * (len(match_data['methods']) - 1)
                match_data['total_score'] = min(match_data['total_score'] + consistency_bonus, 1.0)
        
        # æ’åºå¹¶è¿”å›ç»“æœ
        sorted_matches = sorted(all_matches.values(), key=lambda x: x['total_score'], reverse=True)
        
        # æ ¼å¼åŒ–ç»“æœ
        results = []
        for i, match in enumerate(sorted_matches[:top_k]):
            result = {
                'rank': i + 1,
                'knowledge_point_id': match['kp_id'],
                'knowledge_point_name': match['kp_info']['name'],
                'confidence_score': round(match['total_score'], 3),
                'matching_methods': match['methods'],
                'method_scores': {k: round(v, 3) for k, v in match['scores'].items()},
                'knowledge_point_info': {
                    'difficulty': match['kp_info'].get('difficulty', 1),
                    'grade_level': match['kp_info'].get('grade_level', 7),
                    'keywords': match['kp_info']['keywords'],
                    'category_path': match['kp_id']
                },
                'match_details': match['kp_info'].get('match_reasons', [])
            }
            results.append(result)
        
        # æ›´æ–°ç»Ÿè®¡
        self.matching_stats['total_matches'] += 1
        self.matching_stats['method_usage']['ensemble'] += 1
        
        for result in results:
            kp_id = result['knowledge_point_id']
            if kp_id not in self.matching_stats['knowledge_point_coverage']:
                self.matching_stats['knowledge_point_coverage'][kp_id] = 0
            self.matching_stats['knowledge_point_coverage'][kp_id] += 1
        
        return results
    
    def analyze_question_difficulty(self, question_text: str, matched_knowledge_points: List[Dict]) -> Dict[str, Any]:
        """åˆ†æé¢˜ç›®éš¾åº¦"""
        if not matched_knowledge_points:
            return {'difficulty_level': 1, 'confidence': 0.0, 'explanation': 'æ— æ³•åŒ¹é…åˆ°çŸ¥è¯†ç‚¹'}
        
        # åŸºäºåŒ¹é…çš„çŸ¥è¯†ç‚¹è®¡ç®—éš¾åº¦
        difficulties = []
        weights = []
        
        for kp in matched_knowledge_points:
            difficulty = kp['knowledge_point_info']['difficulty']
            confidence = kp['confidence_score']
            difficulties.append(difficulty)
            weights.append(confidence)
        
        # åŠ æƒå¹³å‡éš¾åº¦
        if weights:
            weighted_difficulty = sum(d * w for d, w in zip(difficulties, weights)) / sum(weights)
        else:
            weighted_difficulty = sum(difficulties) / len(difficulties) if difficulties else 1
        
        # æ–‡æœ¬å¤æ‚åº¦åˆ†æ
        text_complexity = self._analyze_text_complexity(question_text)
        
        # ç»¼åˆéš¾åº¦è¯„ä¼°
        final_difficulty = 0.7 * weighted_difficulty + 0.3 * text_complexity
        final_difficulty = max(1, min(5, round(final_difficulty)))
        
        return {
            'difficulty_level': int(final_difficulty),
            'confidence': sum(weights) / len(weights) if weights else 0.0,
            'explanation': f'åŸºäº{len(matched_knowledge_points)}ä¸ªåŒ¹é…çŸ¥è¯†ç‚¹çš„éš¾åº¦åˆ†æ',
            'text_complexity': text_complexity,
            'knowledge_point_difficulties': difficulties
        }
    
    def _analyze_text_complexity(self, text: str) -> float:
        """åˆ†ææ–‡æœ¬å¤æ‚åº¦"""
        # ç®€å•çš„æ–‡æœ¬å¤æ‚åº¦æŒ‡æ ‡
        word_count = len(text.split())
        char_count = len(text)
        
        # å¤æ‚åº¦è¯„åˆ†ï¼ˆ1-5ï¼‰
        if word_count < 10:
            complexity = 1
        elif word_count < 20:
            complexity = 2
        elif word_count < 40:
            complexity = 3
        elif word_count < 80:
            complexity = 4
        else:
            complexity = 5
        
        # åŸºäºå­—ç¬¦æ•°çš„è°ƒæ•´
        if char_count > 200:
            complexity = min(5, complexity + 1)
        
        return float(complexity)
    
    def get_knowledge_point_recommendations(self, user_weak_points: List[str], 
                                          difficulty_preference: int = 2,
                                          limit: int = 10) -> List[Dict[str, Any]]:
        """åŸºäºè–„å¼±çŸ¥è¯†ç‚¹æ¨èç›¸å…³ç»ƒä¹ å†…å®¹"""
        recommendations = []
        
        for weak_point in user_weak_points:
            # æŸ¥æ‰¾ç›¸å…³çŸ¥è¯†ç‚¹
            if weak_point in self.flat_knowledge_points:
                kp_info = self.flat_knowledge_points[weak_point]
                
                # ç”Ÿæˆæ¨èå†…å®¹
                recommendation = {
                    'knowledge_point_id': weak_point,
                    'knowledge_point_name': kp_info['name'],
                    'difficulty_level': kp_info.get('difficulty', difficulty_preference),
                    'recommended_practice': self._generate_practice_suggestions(kp_info),
                    'prerequisite_points': self._find_prerequisite_points(weak_point),
                    'related_points': self._find_related_points(weak_point),
                    'learning_priority': self._calculate_learning_priority(kp_info, difficulty_preference)
                }
                recommendations.append(recommendation)
        
        # æŒ‰å­¦ä¹ ä¼˜å…ˆçº§æ’åº
        recommendations.sort(key=lambda x: x['learning_priority'], reverse=True)
        
        return recommendations[:limit]
    
    def _generate_practice_suggestions(self, kp_info: Dict) -> List[str]:
        """ç”Ÿæˆç»ƒä¹ å»ºè®®"""
        suggestions = []
        
        # åŸºäºçŸ¥è¯†ç‚¹ç±»å‹ç”Ÿæˆå»ºè®®
        if 'equation' in kp_info['name'].lower():
            suggestions.extend([
                'å¤šåšä¸åŒç±»å‹çš„æ–¹ç¨‹æ±‚è§£ç»ƒä¹ ',
                'é‡ç‚¹ç†è§£è§£é¢˜æ­¥éª¤å’Œæ–¹æ³•',
                'ç»ƒä¹ æ£€éªŒè§£çš„æ­£ç¡®æ€§'
            ])
        elif 'geometry' in kp_info.get('category', '').lower():
            suggestions.extend([
                'ç»˜åˆ¶å‡ ä½•å›¾å½¢å¸®åŠ©ç†è§£',
                'è®°å¿†ç›¸å…³å…¬å¼å’Œå®šç†',
                'ç»ƒä¹ å‡ ä½•è¯æ˜é¢˜'
            ])
        else:
            suggestions.extend([
                'åŠ å¼ºåŸºç¡€æ¦‚å¿µç†è§£',
                'å¤šåšç›¸å…³ç»ƒä¹ é¢˜',
                'æ€»ç»“è§£é¢˜è§„å¾‹'
            ])
        
        return suggestions
    
    def _find_prerequisite_points(self, kp_id: str) -> List[str]:
        """æŸ¥æ‰¾å‰ç½®çŸ¥è¯†ç‚¹"""
        # ç®€å•çš„å‰ç½®å…³ç³»æ¨æ–­ï¼ˆå¯ä»¥æ‰©å±•ä¸ºæ›´å¤æ‚çš„å›¾ç»“æ„ï¼‰
        prerequisites = []
        
        # åŸºäºè·¯å¾„ç»“æ„æ¨æ–­
        path_parts = kp_id.split('.')
        if len(path_parts) > 2:
            # æŸ¥æ‰¾åŒä¸€åˆ†ç±»ä¸‹çš„åŸºç¡€çŸ¥è¯†ç‚¹
            base_path = '.'.join(path_parts[:-1])
            for other_kp_id, other_kp_info in self.flat_knowledge_points.items():
                if (other_kp_id.startswith(base_path) and 
                    other_kp_id != kp_id and
                    other_kp_info.get('difficulty', 5) < self.flat_knowledge_points[kp_id].get('difficulty', 1)):
                    prerequisites.append(other_kp_id)
        
        return prerequisites[:3]  # æœ€å¤šè¿”å›3ä¸ªå‰ç½®çŸ¥è¯†ç‚¹
    
    def _find_related_points(self, kp_id: str) -> List[str]:
        """æŸ¥æ‰¾ç›¸å…³çŸ¥è¯†ç‚¹"""
        related = []
        current_kp = self.flat_knowledge_points[kp_id]
        
        # åŸºäºå…³é”®è¯ç›¸ä¼¼åº¦æŸ¥æ‰¾ç›¸å…³çŸ¥è¯†ç‚¹
        current_keywords = set(current_kp['keywords'])
        
        for other_kp_id, other_kp_info in self.flat_knowledge_points.items():
            if other_kp_id != kp_id:
                other_keywords = set(other_kp_info['keywords'])
                # è®¡ç®—å…³é”®è¯é‡å åº¦
                overlap = len(current_keywords.intersection(other_keywords))
                if overlap >= 2:  # è‡³å°‘æœ‰2ä¸ªå…³é”®è¯é‡å 
                    related.append(other_kp_id)
        
        return related[:5]  # æœ€å¤šè¿”å›5ä¸ªç›¸å…³çŸ¥è¯†ç‚¹
    
    def _calculate_learning_priority(self, kp_info: Dict, difficulty_preference: int) -> float:
        """è®¡ç®—å­¦ä¹ ä¼˜å…ˆçº§"""
        base_priority = 0.5
        
        # éš¾åº¦åŒ¹é…åº¦
        difficulty_diff = abs(kp_info.get('difficulty', 2) - difficulty_preference)
        difficulty_score = max(0, 1 - difficulty_diff * 0.2)
        
        # é‡è¦ç¨‹åº¦ï¼ˆå¯ä»¥åŸºäºè€ƒè¯•é¢‘ç‡ç­‰å› ç´ ï¼‰
        importance_score = kp_info.get('exam_frequency', 0.5)
        
        # ç»¼åˆä¼˜å…ˆçº§
        priority = base_priority + 0.3 * difficulty_score + 0.2 * importance_score
        
        return min(1.0, priority)
    
    def batch_match(self, questions: List[str], **kwargs) -> List[List[Dict[str, Any]]]:
        """æ‰¹é‡åŒ¹é…é¢˜ç›®åˆ°çŸ¥è¯†ç‚¹"""
        results = []
        for question in questions:
            matches = self.ensemble_match(question, **kwargs)
            results.append(matches)
        return results
    
    def get_matching_statistics(self) -> Dict[str, Any]:
        """è·å–åŒ¹é…ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.matching_stats.copy()
        
        # è®¡ç®—æ–¹æ³•ä½¿ç”¨ç‡
        total_matches = stats['total_matches']
        if total_matches > 0:
            # åˆ›å»ºæ–°çš„method_usageå­—å…¸é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            new_method_usage = {}
            for method in stats['method_usage']:
                count = stats['method_usage'][method]
                percentage = (count / total_matches) * 100
                new_method_usage[method] = {
                    'count': count,
                    'percentage': round(percentage, 2)
                }
            stats['method_usage'] = new_method_usage
        
        # æ·»åŠ çŸ¥è¯†ç‚¹è¦†ç›–ç»Ÿè®¡
        stats['knowledge_point_coverage_summary'] = {
            'total_knowledge_points': len(self.flat_knowledge_points),
            'covered_knowledge_points': len(stats['knowledge_point_coverage']),
            'coverage_rate': len(stats['knowledge_point_coverage']) / len(self.flat_knowledge_points) * 100
        }
        
        return stats
    
    def save_models(self):
        """ä¿å­˜æ¨¡å‹å’Œé…ç½®"""
        model_data = {
            'knowledge_system': self.knowledge_system,
            'flat_knowledge_points': self.flat_knowledge_points,
            'matching_stats': self.matching_stats,
            'version': '1.0.0',
            'created_at': datetime.now().isoformat()
        }
        
        save_path = os.path.join(self.model_path, 'knowledge_matcher.json')
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(model_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"çŸ¥è¯†åŒ¹é…å™¨æ¨¡å‹å·²ä¿å­˜: {save_path}")
        return save_path
    
    def _load_models(self):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        model_path = os.path.join(self.model_path, 'knowledge_matcher.json')
        
        if os.path.exists(model_path):
            try:
                with open(model_path, 'r', encoding='utf-8') as f:
                    model_data = json.load(f)
                
                if 'knowledge_system' in model_data:
                    self.knowledge_system = model_data['knowledge_system']
                    self._flatten_knowledge_system()
                
                if 'matching_stats' in model_data:
                    # ç¡®ä¿method_usageä¿æŒä¸ºæ•´æ•°è®¡æ•°å™¨æ ¼å¼
                    saved_stats = model_data['matching_stats']
                    if 'method_usage' in saved_stats:
                        for method, value in saved_stats['method_usage'].items():
                            if isinstance(value, dict) and 'count' in value:
                                # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œæå–countå€¼
                                self.matching_stats['method_usage'][method] = value['count']
                            else:
                                # å¦‚æœæ˜¯æ•´æ•°æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                                self.matching_stats['method_usage'][method] = value
                    
                    # æ›´æ–°å…¶ä»–ç»Ÿè®¡ä¿¡æ¯
                    self.matching_stats['total_matches'] = saved_stats.get('total_matches', 0)
                    self.matching_stats['accuracy_distribution'] = saved_stats.get('accuracy_distribution', defaultdict(int))
                    self.matching_stats['knowledge_point_coverage'] = saved_stats.get('knowledge_point_coverage', defaultdict(int))
                
                logger.info(f"å·²åŠ è½½çŸ¥è¯†åŒ¹é…å™¨æ¨¡å‹: {model_path}")
            except Exception as e:
                logger.warning(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")


def test_knowledge_matcher():
    """ç®€å•æµ‹è¯•çŸ¥è¯†ç‚¹åŒ¹é…å™¨"""
    print("ğŸ§ª æµ‹è¯•çŸ¥è¯†ç‚¹åŒ¹é…ç®—æ³•...")
    
    matcher = KnowledgeMatcher()
    
    # ç®€å•æµ‹è¯•ç”¨ä¾‹
    test_questions = [
        "è§£ä¸€å…ƒä¸€æ¬¡æ–¹ç¨‹ï¼š2x + 3 = 7",
        "è®¡ç®—ä¸‰è§’å½¢çš„é¢ç§¯",
        "æ±‚äºŒæ¬¡å‡½æ•°çš„é¡¶ç‚¹åæ ‡"
    ]
    
    for i, question in enumerate(test_questions, 1):
        matches = matcher.ensemble_match(question, top_k=2)
        print(f"{i}. {question}")
        if matches:
            for match in matches:
                print(f"   â†’ {match['knowledge_point_name']} (ç½®ä¿¡åº¦: {match['confidence_score']:.3f})")
        else:
            print("   â†’ æœªæ‰¾åˆ°åŒ¹é…")
    
    print("âœ… æµ‹è¯•å®Œæˆ")
    return matcher


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_knowledge_matcher()
