#!/usr/bin/env python3
"""
æ•°æ®æ”¶é›†è´¨é‡éªŒè¯è„šæœ¬
éªŒè¯æ”¶é›†çš„çŸ¥è¯†ç‚¹å’Œé¢˜ç›®æ•°æ®çš„å®Œæ•´æ€§å’Œè´¨é‡
"""

import pandas as pd
import json
import os
from typing import Dict, List, Tuple, Any
import re
from datetime import datetime

class DataValidator:
    """æ•°æ®éªŒè¯å™¨"""
    
    def __init__(self):
        self.data_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'data_collection', 'processed')
        self.validation_results = {
            'timestamp': datetime.now().isoformat(),
            'knowledge_points': {'total': 0, 'valid': 0, 'errors': []},
            'exam_questions': {'total': 0, 'valid': 0, 'errors': []},
            'mock_questions': {'total': 0, 'valid': 0, 'errors': []},
            'overall_quality': 'unknown'
        }
    
    def validate_knowledge_points(self) -> Dict[str, Any]:
        """éªŒè¯çŸ¥è¯†ç‚¹æ•°æ®"""
        print("ğŸ” éªŒè¯çŸ¥è¯†ç‚¹æ•°æ®...")
        
        csv_path = os.path.join(self.data_dir, 'knowledge_points.csv')
        if not os.path.exists(csv_path):
            print("âŒ çŸ¥è¯†ç‚¹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
            return {'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}
        
        try:
            df = pd.read_csv(csv_path)
            total_count = len(df)
            valid_count = 0
            errors = []
            
            # éªŒè¯å¿…å¡«å­—æ®µ
            required_fields = ['grade', 'subject', 'chapter', 'name', 'description', 
                             'difficulty_level', 'importance_level', 'exam_frequency']
            
            for index, row in df.iterrows():
                row_errors = []
                
                # æ£€æŸ¥å¿…å¡«å­—æ®µ
                for field in required_fields:
                    if pd.isna(row[field]) or str(row[field]).strip() == '':
                        row_errors.append(f"å­—æ®µ {field} ä¸ºç©º")
                
                # éªŒè¯æ•°å€¼èŒƒå›´
                if not pd.isna(row['difficulty_level']):
                    if not (1 <= row['difficulty_level'] <= 5):
                        row_errors.append("difficulty_level åº”åœ¨1-5èŒƒå›´å†…")
                
                if not pd.isna(row['importance_level']):
                    if not (1 <= row['importance_level'] <= 5):
                        row_errors.append("importance_level åº”åœ¨1-5èŒƒå›´å†…")
                
                if not pd.isna(row['exam_frequency']):
                    if not (0.0 <= row['exam_frequency'] <= 1.0):
                        row_errors.append("exam_frequency åº”åœ¨0.0-1.0èŒƒå›´å†…")
                
                # éªŒè¯æ–‡æœ¬é•¿åº¦
                if not pd.isna(row['name']) and len(str(row['name'])) > 200:
                    row_errors.append("name é•¿åº¦è¶…è¿‡200å­—ç¬¦")
                
                if not pd.isna(row['description']) and len(str(row['description'])) < 10:
                    row_errors.append("description é•¿åº¦å¤ªçŸ­ï¼Œåº”è‡³å°‘10å­—ç¬¦")
                
                # éªŒè¯å…³é”®è¯
                if not pd.isna(row['keywords']):
                    keywords = str(row['keywords']).split('|')
                    if len(keywords) < 2:
                        row_errors.append("keywords åº”è‡³å°‘åŒ…å«2ä¸ªå…³é”®è¯")
                
                if not row_errors:
                    valid_count += 1
                else:
                    errors.append({
                        'row': index + 1,
                        'name': row['name'],
                        'errors': row_errors
                    })
            
            self.validation_results['knowledge_points'] = {
                'total': total_count,
                'valid': valid_count,
                'errors': errors,
                'validation_rate': round(valid_count / total_count * 100, 2) if total_count > 0 else 0
            }
            
            print(f"âœ… çŸ¥è¯†ç‚¹éªŒè¯å®Œæˆ: {valid_count}/{total_count} æ¡è®°å½•æœ‰æ•ˆ")
            return self.validation_results['knowledge_points']
            
        except Exception as e:
            error_msg = f"éªŒè¯çŸ¥è¯†ç‚¹æ•°æ®æ—¶å‡ºé”™: {str(e)}"
            print(f"âŒ {error_msg}")
            self.validation_results['knowledge_points']['errors'].append(error_msg)
            return {'error': error_msg}
    
    def validate_questions(self, filename: str, question_type: str) -> Dict[str, Any]:
        """éªŒè¯é¢˜ç›®æ•°æ®"""
        print(f"ğŸ” éªŒè¯{question_type}æ•°æ®...")
        
        csv_path = os.path.join(self.data_dir, filename)
        if not os.path.exists(csv_path):
            print(f"âŒ {question_type}æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
            return {'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}
        
        try:
            df = pd.read_csv(csv_path)
            total_count = len(df)
            valid_count = 0
            errors = []
            
            # éªŒè¯å¿…å¡«å­—æ®µ
            required_fields = ['question_id', 'subject', 'type', 'stem', 'correct_answer', 
                             'difficulty_level', 'source']
            
            for index, row in df.iterrows():
                row_errors = []
                
                # æ£€æŸ¥å¿…å¡«å­—æ®µ
                for field in required_fields:
                    if pd.isna(row[field]) or str(row[field]).strip() == '':
                        row_errors.append(f"å­—æ®µ {field} ä¸ºç©º")
                
                # éªŒè¯é¢˜ç›®IDå”¯ä¸€æ€§
                if not pd.isna(row['question_id']):
                    if len(str(row['question_id'])) > 100:
                        row_errors.append("question_id é•¿åº¦è¶…è¿‡100å­—ç¬¦")
                
                # éªŒè¯é¢˜ç›®ç±»å‹
                valid_types = ['choice', 'true_false', 'fill_blank', 'application', 'calculation']
                if not pd.isna(row['type']) and row['type'] not in valid_types:
                    row_errors.append(f"type åº”ä¸ºä»¥ä¸‹ä¹‹ä¸€: {', '.join(valid_types)}")
                
                # éªŒè¯éš¾åº¦ç­‰çº§
                if not pd.isna(row['difficulty_level']):
                    if not (1 <= row['difficulty_level'] <= 5):
                        row_errors.append("difficulty_level åº”åœ¨1-5èŒƒå›´å†…")
                
                # éªŒè¯é€‰æ‹©é¢˜é€‰é¡¹
                if not pd.isna(row['type']) and row['type'] == 'choice':
                    if pd.isna(row.get('options')):
                        row_errors.append("é€‰æ‹©é¢˜ç¼ºå°‘optionså­—æ®µ")
                    else:
                        try:
                            # å‡è®¾optionsæ˜¯JSONæ ¼å¼å­˜å‚¨
                            options = eval(str(row['options'])) if isinstance(row['options'], str) else row['options']
                            if not isinstance(options, list) or len(options) < 2:
                                row_errors.append("é€‰æ‹©é¢˜åº”è‡³å°‘æœ‰2ä¸ªé€‰é¡¹")
                        except:
                            row_errors.append("options æ ¼å¼é”™è¯¯")
                
                # éªŒè¯é¢˜ç›®å†…å®¹é•¿åº¦
                if not pd.isna(row['stem']) and len(str(row['stem'])) < 5:
                    row_errors.append("stem å†…å®¹å¤ªçŸ­")
                
                if not pd.isna(row['correct_answer']) and len(str(row['correct_answer'])) == 0:
                    row_errors.append("correct_answer ä¸ºç©º")
                
                if not row_errors:
                    valid_count += 1
                else:
                    errors.append({
                        'row': index + 1,
                        'question_id': row.get('question_id', 'unknown'),
                        'errors': row_errors
                    })
            
            result = {
                'total': total_count,
                'valid': valid_count,
                'errors': errors,
                'validation_rate': round(valid_count / total_count * 100, 2) if total_count > 0 else 0
            }
            
            # ä¿å­˜åˆ°éªŒè¯ç»“æœ
            if question_type == 'ä¸­è€ƒåŸé¢˜':
                self.validation_results['exam_questions'] = result
            elif question_type == 'æ¨¡æ‹Ÿé¢˜':
                self.validation_results['mock_questions'] = result
            
            print(f"âœ… {question_type}éªŒè¯å®Œæˆ: {valid_count}/{total_count} æ¡è®°å½•æœ‰æ•ˆ")
            return result
            
        except Exception as e:
            error_msg = f"éªŒè¯{question_type}æ•°æ®æ—¶å‡ºé”™: {str(e)}"
            print(f"âŒ {error_msg}")
            return {'error': error_msg}
    
    def check_data_completeness(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        print("ğŸ“Š æ£€æŸ¥æ•°æ®å®Œæ•´æ€§...")
        
        # ç›®æ ‡æ•°é‡
        targets = {
            'æ•°å­¦': {'knowledge_points': 50, 'exam_questions': 20, 'mock_questions': 15},
            'è¯­æ–‡': {'knowledge_points': 50, 'exam_questions': 20, 'mock_questions': 15},
            'è‹±è¯­': {'knowledge_points': 50, 'exam_questions': 20, 'mock_questions': 15},
            'ç‰©ç†': {'knowledge_points': 40, 'exam_questions': 20, 'mock_questions': 15},
            'åŒ–å­¦': {'knowledge_points': 40, 'exam_questions': 20, 'mock_questions': 15},
            'ç”Ÿç‰©': {'knowledge_points': 30, 'exam_questions': 20, 'mock_questions': 15},
            'å†å²': {'knowledge_points': 30, 'exam_questions': 20, 'mock_questions': 15},
            'åœ°ç†': {'knowledge_points': 30, 'exam_questions': 20, 'mock_questions': 15},
            'æ”¿æ²»': {'knowledge_points': 30, 'exam_questions': 20, 'mock_questions': 15}
        }
        
        completeness = {}
        
        # æ£€æŸ¥çŸ¥è¯†ç‚¹å®Œæ•´æ€§
        try:
            kp_path = os.path.join(self.data_dir, 'knowledge_points.csv')
            if os.path.exists(kp_path):
                df_kp = pd.read_csv(kp_path)
                subject_counts = df_kp['subject'].value_counts()
                
                for subject in targets.keys():
                    current_count = subject_counts.get(subject, 0)
                    target_count = targets[subject]['knowledge_points']
                    completeness[f'{subject}_çŸ¥è¯†ç‚¹'] = {
                        'current': current_count,
                        'target': target_count,
                        'progress': f"{current_count}/{target_count}",
                        'completion_rate': round(current_count / target_count * 100, 2)
                    }
        except Exception as e:
            print(f"âŒ æ£€æŸ¥çŸ¥è¯†ç‚¹å®Œæ•´æ€§æ—¶å‡ºé”™: {e}")
        
        # æ£€æŸ¥é¢˜ç›®å®Œæ•´æ€§
        for file_info in [('exam_questions.csv', 'ä¸­è€ƒåŸé¢˜'), ('mock_questions.csv', 'æ¨¡æ‹Ÿé¢˜')]:
            filename, question_type = file_info
            try:
                q_path = os.path.join(self.data_dir, filename)
                if os.path.exists(q_path):
                    df_q = pd.read_csv(q_path)
                    subject_counts = df_q['subject'].value_counts()
                    
                    for subject in targets.keys():
                        current_count = subject_counts.get(subject, 0)
                        target_key = 'exam_questions' if 'ä¸­è€ƒ' in question_type else 'mock_questions'
                        target_count = targets[subject][target_key]
                        completeness[f'{subject}_{question_type}'] = {
                            'current': current_count,
                            'target': target_count,
                            'progress': f"{current_count}/{target_count}",
                            'completion_rate': round(current_count / target_count * 100, 2)
                        }
            except Exception as e:
                print(f"âŒ æ£€æŸ¥{question_type}å®Œæ•´æ€§æ—¶å‡ºé”™: {e}")
        
        return completeness
    
    def calculate_overall_quality(self) -> str:
        """è®¡ç®—æ€»ä½“æ•°æ®è´¨é‡"""
        quality_scores = []
        
        # çŸ¥è¯†ç‚¹è´¨é‡å¾—åˆ†
        if self.validation_results['knowledge_points']['total'] > 0:
            kp_score = self.validation_results['knowledge_points']['valid'] / self.validation_results['knowledge_points']['total']
            quality_scores.append(kp_score)
        
        # é¢˜ç›®è´¨é‡å¾—åˆ†
        for question_type in ['exam_questions', 'mock_questions']:
            if self.validation_results[question_type]['total'] > 0:
                q_score = self.validation_results[question_type]['valid'] / self.validation_results[question_type]['total']
                quality_scores.append(q_score)
        
        if not quality_scores:
            return 'unknown'
        
        avg_score = sum(quality_scores) / len(quality_scores)
        
        if avg_score >= 0.9:
            return 'excellent'
        elif avg_score >= 0.8:
            return 'good'
        elif avg_score >= 0.7:
            return 'fair'
        else:
            return 'poor'
    
    def generate_validation_report(self) -> Dict[str, Any]:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        print("ğŸ“‹ ç”ŸæˆéªŒè¯æŠ¥å‘Š...")
        
        # è®¡ç®—æ€»ä½“è´¨é‡
        self.validation_results['overall_quality'] = self.calculate_overall_quality()
        
        # æ£€æŸ¥å®Œæ•´æ€§
        completeness = self.check_data_completeness()
        
        # ç”Ÿæˆå»ºè®®
        suggestions = []
        
        # åŸºäºéªŒè¯ç»“æœçš„å»ºè®®
        if self.validation_results['knowledge_points']['errors']:
            suggestions.append("ä¿®å¤çŸ¥è¯†ç‚¹æ•°æ®ä¸­çš„é”™è¯¯å­—æ®µ")
        
        if self.validation_results['exam_questions']['errors']:
            suggestions.append("ä¿®å¤ä¸­è€ƒåŸé¢˜æ•°æ®ä¸­çš„é”™è¯¯å­—æ®µ")
        
        if self.validation_results['mock_questions']['errors']:
            suggestions.append("ä¿®å¤æ¨¡æ‹Ÿé¢˜æ•°æ®ä¸­çš„é”™è¯¯å­—æ®µ")
        
        # åŸºäºå®Œæ•´æ€§çš„å»ºè®®
        for key, value in completeness.items():
            if value['completion_rate'] < 50:
                suggestions.append(f"å¢åŠ {key}çš„æ•°æ®æ”¶é›†ï¼Œå½“å‰å®Œæˆåº¦ä»…{value['completion_rate']:.1f}%")
        
        if not suggestions:
            suggestions.append("æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥è¿›è¡Œä¸‹ä¸€æ­¥å¤„ç†")
        
        report = {
            'validation_summary': {
                'overall_quality': self.validation_results['overall_quality'],
                'total_records_validated': (
                    self.validation_results['knowledge_points']['total'] +
                    self.validation_results['exam_questions']['total'] +
                    self.validation_results['mock_questions']['total']
                ),
                'validation_timestamp': self.validation_results['timestamp']
            },
            'detailed_results': self.validation_results,
            'completeness_analysis': completeness,
            'recommendations': suggestions,
            'next_actions': [
                "ä¿®å¤å‘ç°çš„æ•°æ®é”™è¯¯",
                "è¡¥å……ç¼ºå¤±çš„æ•°æ®é¡¹",
                "è¿›è¡Œæ•°æ®æ ‡å‡†åŒ–å¤„ç†",
                "å¯¼å…¥æ•°æ®åº“è¿›è¡Œæœ€ç»ˆéªŒè¯"
            ]
        }
        
        # ä¿å­˜æŠ¥å‘Šï¼ˆå¤„ç†numpyç±»å‹ï¼‰
        def convert_numpy_types(obj):
            """è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
            if hasattr(obj, 'item'):
                return obj.item()
            elif hasattr(obj, 'tolist'):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_numpy_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(v) for v in obj]
            return obj
        
        report_converted = convert_numpy_types(report)
        report_path = os.path.join(self.data_dir, 'validation_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report_converted, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report
    
    def run_validation(self) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„æ•°æ®éªŒè¯æµç¨‹"""
        print("ğŸ” å¼€å§‹æ•°æ®è´¨é‡éªŒè¯...")
        print("=" * 60)
        
        # éªŒè¯çŸ¥è¯†ç‚¹æ•°æ®
        self.validate_knowledge_points()
        
        # éªŒè¯é¢˜ç›®æ•°æ®
        self.validate_questions('exam_questions.csv', 'ä¸­è€ƒåŸé¢˜')
        self.validate_questions('mock_questions.csv', 'æ¨¡æ‹Ÿé¢˜')
        
        # ç”ŸæˆéªŒè¯æŠ¥å‘Š
        report = self.generate_validation_report()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ•°æ®éªŒè¯å®Œæˆï¼")
        print(f"ğŸ“Š æ€»ä½“è´¨é‡ç­‰çº§: {report['validation_summary']['overall_quality']}")
        print(f"ğŸ“ˆ éªŒè¯è®°å½•æ€»æ•°: {report['validation_summary']['total_records_validated']}")
        print(f"ğŸ’¡ å»ºè®®æ•°é‡: {len(report['recommendations'])}")
        print("=" * 60)
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    validator = DataValidator()
    report = validator.run_validation()
    
    # è¾“å‡ºéªŒè¯æ€»ç»“
    print("\nğŸ“‹ éªŒè¯æ€»ç»“:")
    
    # æ˜¾ç¤ºå„ç±»æ•°æ®çš„éªŒè¯ç»“æœ
    for data_type in ['knowledge_points', 'exam_questions', 'mock_questions']:
        if data_type in report['detailed_results']:
            result = report['detailed_results'][data_type]
            if result['total'] > 0:
                print(f"  {data_type}: {result['valid']}/{result['total']} æœ‰æ•ˆ ({result.get('validation_rate', 0):.1f}%)")
    
    # æ˜¾ç¤ºä¸»è¦å»ºè®®
    print("\nğŸ’¡ ä¸»è¦å»ºè®®:")
    for i, suggestion in enumerate(report['recommendations'][:5], 1):
        print(f"  {i}. {suggestion}")
    
    # æ˜¾ç¤ºæ•°æ®è´¨é‡ç­‰çº§
    quality_levels = {
        'excellent': 'ä¼˜ç§€ ğŸŒŸ',
        'good': 'è‰¯å¥½ âœ…',
        'fair': 'ä¸€èˆ¬ âš ï¸',
        'poor': 'éœ€æ”¹è¿› âŒ',
        'unknown': 'æœªçŸ¥ â“'
    }
    quality = report['validation_summary']['overall_quality']
    print(f"\nğŸ† æ•°æ®è´¨é‡: {quality_levels.get(quality, quality)}")

if __name__ == "__main__":
    main()
