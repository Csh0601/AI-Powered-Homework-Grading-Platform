from flask import Blueprint, request, jsonify, current_app
import os
import logging
import uuid
from datetime import datetime
import json
from typing import Dict, Any, List, Optional

from app.config import Config
from app.services.multimodal_client import analyze_homework_with_direct_service
from app.services.image_processing import preprocess_image

logger = logging.getLogger(__name__)

upload_bp = Blueprint('upload', __name__)

def _ensure_list(value):
    """ç¡®ä¿å€¼æ˜¯åˆ—è¡¨æ ¼å¼"""
    if value is None:
        return []
    if isinstance(value, str):
        # å¤„ç†ç”¨åˆ†éš”ç¬¦åˆ†å‰²çš„å­—ç¬¦ä¸²
        if ',' in value:
            return [item.strip() for item in value.split(',') if item.strip()]
        elif 'ã€' in value:
            return [item.strip() for item in value.split('ã€') if item.strip()]
        else:
            return [value.strip()] if value.strip() else []
    if isinstance(value, list):
        return value
    return [str(value)]

def _enhance_grading_payload(grading_payload: Dict[str, Any]) -> Dict[str, Any]:
    """å¢å¼ºæ‰¹æ”¹ç»“æœçš„æ•°æ®ç»“æ„"""
    results = grading_payload.get("grading_result") or []
    questions = grading_payload.get("questions") or []
    
    # ç¡®ä¿resultsæ˜¯åˆ—è¡¨
    if not isinstance(results, list):
        results = [results] if results else []
    
    # ä¸ºæ¯ä¸ªç»“æœæ·»åŠ é»˜è®¤å€¼å’Œå¢å¼ºå­—æ®µ
    correct_count = 0
    aggregated_knowledge_points = set()
    main_issues = []
    
    for i, result in enumerate(results):
        if not isinstance(result, dict):
            continue
            
        # åŸºç¡€å­—æ®µå¤„ç†
        result.setdefault("correct", False)
        result.setdefault("score", 0)
        result.setdefault("explanation", "")
        result.setdefault("knowledge_points", [])
        result.setdefault("type", "æœªçŸ¥é¢˜å‹")
        
        # ç¡®ä¿çŸ¥è¯†ç‚¹æ˜¯åˆ—è¡¨
        result["knowledge_points"] = _ensure_list(result["knowledge_points"])
        
        # ä¿ç•™æ–°å­—æ®µï¼ˆlearning_suggestions, similar_questionï¼‰
        if "learning_suggestions" in result:
            result["learning_suggestions"] = _ensure_list(result["learning_suggestions"])
        if "similar_question" in result:
            # similar_question ä¿æŒåŸæ ·
            pass
        
        # ç»Ÿè®¡æ•°æ®
        if result["correct"]:
            correct_count += 1
        aggregated_knowledge_points.update(result.get("knowledge_points") or [])
        if not result["correct"]:
            question_text = result.get("question", "")
            if question_text:
                main_issues.append(f"é¢˜ç›®: {question_text[:40]}...")

    # å¤„ç†questionsæ•°ç»„
    for question in questions:
        if isinstance(question, dict):
            # ä¿ç•™ questions ä¸­çš„æ–°å­—æ®µ
            if "similar_question" in question:
                # similar_question ä¿æŒåŸæ ·
                pass

    # å¤„ç†summary
    summary = grading_payload.get("summary") or {}
    summary.setdefault("total_questions", len(results) or len(questions))
    summary["correct_count"] = correct_count
    total_questions = summary.get("total_questions") or len(results) or 1
    summary["accuracy_rate"] = correct_count / total_questions if total_questions else 0
    summary["main_issues"] = _ensure_list(summary.get("main_issues")) or main_issues
    summary["knowledge_points"] = _ensure_list(summary.get("knowledge_points")) or list(aggregated_knowledge_points)
    
    # ä¿ç•™ summary ä¸­çš„æ–°å­—æ®µ
    if "learning_suggestions" in summary:
        summary["learning_suggestions"] = _ensure_list(summary["learning_suggestions"])
    if "similar_question" in summary:
        # similar_question ä¿æŒåŸæ ·
        pass

    grading_payload["grading_result"] = results
    grading_payload["summary"] = summary

    # çŸ¥è¯†ç‚¹åˆ†æå…œåº•
    knowledge_analysis = grading_payload.get("knowledge_analysis") or {}
    wrong_points = _ensure_list(knowledge_analysis.get("wrong_knowledge_points"))
    study_recommendations = _ensure_list(knowledge_analysis.get("study_recommendations"))

    if not wrong_points:
        wrong_points = [kp for kp in aggregated_knowledge_points if kp]
    if not study_recommendations and wrong_points:
        study_recommendations = [f"å¤ä¹ ç›¸å…³çŸ¥è¯†ç‚¹ï¼š{kp}" for kp in wrong_points]

    knowledge_analysis["wrong_knowledge_points"] = wrong_points
    knowledge_analysis["study_recommendations"] = study_recommendations
    grading_payload["knowledge_analysis"] = knowledge_analysis

    return grading_payload

@upload_bp.route('/upload_image', methods=['POST'])
def upload_image():
    """
    å›¾ç‰‡ä¸Šä¼ å’ŒAIæ‰¹æ”¹ç«¯ç‚¹ - ç®€åŒ–ç‰ˆï¼ˆä»…ä½¿ç”¨ç›´æ¥LoRAè°ƒç”¨ï¼‰
    """
    try:
        logger.info("æ”¶åˆ°ä¸Šä¼ è¯·æ±‚")
        
        # æ£€æŸ¥æ–‡ä»¶
        if 'file' not in request.files:
            return jsonify({'error': 'No file part'}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'No selected file'}), 400

        if not allowed_file(file.filename):
            return jsonify({'error': 'File type not allowed'}), 400
        
        # ç”Ÿæˆä»»åŠ¡IDå’Œæ—¶é—´æˆ³
        task_id = str(uuid.uuid4())
        timestamp = datetime.now().isoformat()
        cache_bust_id = str(uuid.uuid4())
        
        logger.info(f"ğŸ†” ä»»åŠ¡ID: {task_id}")
        logger.info(f"ğŸ”„ ç¼“å­˜ç ´åID: {cache_bust_id}")

        # ä¿å­˜æ–‡ä»¶
        filename = f"{task_id}_{file.filename}"
        save_path = os.path.join(Config.UPLOAD_FOLDER, filename)
        os.makedirs(Config.UPLOAD_FOLDER, exist_ok=True)
        file.save(save_path)
        
        logger.info(f"ğŸ“ æ–‡ä»¶ä¿å­˜åˆ°: {save_path}")

        # ğŸ¯ ä½¿ç”¨Qwen VL LoRAç›´æ¥è°ƒç”¨æœåŠ¡è¿›è¡Œåˆ†æ
        logger.info("ğŸ¯ å¼€å§‹ä½¿ç”¨Qwen VL LoRAç›´æ¥è°ƒç”¨æœåŠ¡...")
        
        # ä¸ºå¤šæ¨¡æ€AIå¤„ç†å›¾ç‰‡ï¼ˆä¿ç•™å½©è‰²ï¼‰
        multimodal_processed_path = preprocess_image(save_path, for_multimodal=True)
        logger.info(f"ğŸ“ å¤šæ¨¡æ€é¢„å¤„ç†åå›¾ç‰‡: {multimodal_processed_path}")
        
        # éªŒè¯æ–‡ä»¶è·¯å¾„å’Œå¤§å°
        if not os.path.exists(multimodal_processed_path):
            raise Exception(f"é¢„å¤„ç†åçš„å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {multimodal_processed_path}")
        
        file_size = os.path.getsize(multimodal_processed_path)
        logger.info(f"ğŸ“Š å›¾ç‰‡æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
        
        if file_size == 0:
            raise Exception("å›¾ç‰‡æ–‡ä»¶ä¸ºç©º")
                
        # ğŸš€ è°ƒç”¨Qwen VL LoRAç›´æ¥æœåŠ¡è¿›è¡Œåˆ†æ
        logger.info(f"ğŸš€ å¼€å§‹è°ƒç”¨Qwen VL LoRAç›´æ¥æœåŠ¡ï¼Œæ–‡ä»¶è·¯å¾„: {multimodal_processed_path}")
        ai_result = analyze_homework_with_direct_service(multimodal_processed_path)
        
        if not ai_result.get("success"):
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ— å†…å®¹è¯†åˆ«çš„æƒ…å†µ
            if ai_result.get("error_type") == "no_content":
                logger.warning("âš ï¸ å›¾ç‰‡å†…å®¹æ— æ³•è¯†åˆ«")
                return jsonify({
                    'error': ai_result.get("error", "å›¾ç‰‡ä¸­æ— æ³•è¯†åˆ«åˆ°æœ‰æ•ˆçš„é¢˜ç›®å†…å®¹"),
                    'error_type': 'no_content',
                    'processing_method': 'qwen_vl_lora_no_content',
                    'questions': [],
                    'grading_result': [],
                    'summary': ai_result.get("summary", {}),
                    'task_id': task_id,
                    'timestamp': timestamp
                }), 400
            else:
                raise Exception(f"Qwen VL LoRAåˆ†æå¤±è´¥: {ai_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        logger.info(f"âœ… Qwen VL LoRAç›´æ¥è°ƒç”¨æˆåŠŸï¼Œç”¨æ—¶: {ai_result.get('processing_time', 0):.2f}ç§’")
        
        # ğŸ¯ å¤„ç†æˆåŠŸçš„ç»“æœ
        questions = ai_result.get("questions", [])
        grading_result_data = ai_result.get("grading_result", [])
        summary_data = ai_result.get("summary", {})
        knowledge_analysis_data = ai_result.get("knowledge_analysis", {})
        
        # æ„å»ºå®Œæ•´çš„æ‰¹æ”¹ç»“æœ
        grading_result = _enhance_grading_payload({
            "grading_result": grading_result_data,
            "questions": questions,
            "summary": summary_data,
            "knowledge_analysis": knowledge_analysis_data,
            "method": "qwen_vl_lora_direct"
        })
        
        # ä¸ºæ¯ä¸ªé¢˜ç›®æ·»åŠ å”¯ä¸€æ ‡è¯†
        for i, q in enumerate(questions):
            if not q.get('question_id'):
                q['question_id'] = f"{task_id}_q_{i}"
                q['timestamp'] = timestamp
        
        # ğŸ¯ è®¾ç½®å¤šæ¨¡æ€åˆ†æä¿¡æ¯
        multimodal_analysis = {
            "method": "qwen_vl_lora_direct",
            "analysis": "ä½¿ç”¨Qwen2.5-VL-32B-Instruct-LoRA-Trainedç›´æ¥åˆ†æå›¾ç‰‡å®Œæˆè¯†åˆ«å’Œæ‰¹æ”¹",
            "accuracy": "é«˜ç²¾åº¦å¤šæ¨¡æ€å›¾ç‰‡ç†è§£",
            "model": "Qwen2.5-VL-32B-Instruct-LoRA-Trained",
            "analysis_type": "lora_multimodal"
        }
        
        # è·å–æœ€ç»ˆç»“æœ
        final_grading_result = grading_result["grading_result"]
        knowledge_analysis = grading_result["knowledge_analysis"]
        practice_questions = []  # å¤šæ¨¡æ€AIæš‚ä¸ç”Ÿæˆç»ƒä¹ é¢˜
        
        # ä»å¤šä¸ªæ¥æºè·å–é”™é¢˜çŸ¥è¯†ç‚¹
        wrong_knowledges = knowledge_analysis.get('wrong_knowledge_points', [])
        
        # ğŸš€ æ„å»ºæœ€ç»ˆå“åº”æ•°æ®
        response_data = {
            'task_id': task_id,
            'timestamp': timestamp,
            'cache_bust_id': cache_bust_id,
            'questions': questions,
            'grading_result': final_grading_result,
            'summary': grading_result.get("summary", {}),
            'wrong_knowledges': wrong_knowledges,
            'processing_method': 'qwen_vl_lora_direct',
            'ai_enabled': True,
            'knowledge_analysis': knowledge_analysis,
            'practice_questions': practice_questions,
            'study_suggestions': knowledge_analysis.get('study_recommendations', [])
        }
        
        # ğŸš€ æ·»åŠ ç›´æ¥è°ƒç”¨æœåŠ¡çš„æ–°å­—æ®µ
        response_data.update({
            'learning_suggestions': ai_result.get('learning_suggestions', []),
            'similar_questions': ai_result.get('similar_questions', [])
        })
            
        logger.info(f"âœ… æ·»åŠ æ–°å­—æ®µ - learning_suggestions: {len(ai_result.get('learning_suggestions', []))}æ¡")
        logger.info(f"âœ… æ·»åŠ æ–°å­—æ®µ - similar_questions: {len(ai_result.get('similar_questions', []))}é“")
        
        # æ·»åŠ å¤šæ¨¡æ€åˆ†æä¿¡æ¯
        response_data['multimodal_analysis'] = multimodal_analysis
        
        logger.info(f"âœ… ä½œä¸šæ‰¹æ”¹å®Œæˆï¼é¢˜ç›®æ•°é‡: {len(questions)}, å¤„ç†æ–¹æ³•: qwen_vl_lora_direct")
        
        # è®¾ç½®å“åº”å¤´é˜²æ­¢ç¼“å­˜
        response = jsonify(response_data)
        response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
        response.headers['Pragma'] = 'no-cache'
        response.headers['Expires'] = '0'
        response.headers['X-Request-ID'] = cache_bust_id
        
        return response
        
    except Exception as e:
        logger.error(f"å¤„ç†ä½œä¸šæ—¶å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
        return jsonify({'error': f'å¤„ç†ä½œä¸šæ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}'}), 500

@upload_bp.route('/ai_status', methods=['GET'])
def get_ai_status():
    """è·å–AIæœåŠ¡çŠ¶æ€"""
    try:
        # å»¶è¿Ÿå¯¼å…¥é¿å…å¯åŠ¨æ—¶çš„æ—¥å¿—å™ªéŸ³
        try:
            from app.services.grading_qwen import get_ai_service_status
            status = get_ai_service_status()
        except ImportError:
            # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œè¿”å›åŸºæœ¬çŠ¶æ€
            status = {
                "qwen_service": "available",
                "multimodal_service": "available", 
                "processing_method": "qwen_vl_lora_direct"
            }
        
        return jsonify({
            'status': 'success',
            'ai_service': status
        })
    except Exception as e:
        logger.error(f"è·å–AIæœåŠ¡çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            'status': 'error',
            'message': str(e)
        }), 500

@upload_bp.route('/health/multimodal', methods=['GET'])
def check_multimodal_health():
    """æ£€æŸ¥Qwen2.5-VLå¤šæ¨¡æ€æœåŠ¡å¥åº·çŠ¶æ€"""
    try:
        from app.services.multimodal_client import get_qwen_vl_client
        
        qwen_vl_client = get_qwen_vl_client()
        health_result = qwen_vl_client.health_check()
        
        return jsonify({
            "local_status": "healthy" if health_result.get("status") == "healthy" else "unhealthy",
            "remote_service": health_result,
            "model_type": "Qwen2.5-VL-32B-Instruct-LoRA-Trained",
            "connection": "connected" if health_result.get("status") == "healthy" else "disconnected",
            "api_url": Config.QWEN_VL_API_URL,
            "config": {
                "llm_provider": Config.LLM_PROVIDER,
                "multimodal_enabled": Config.MULTIMODAL_ENABLED,
                "use_qwen_grading": Config.USE_QWEN_GRADING,
                "ocr_fallback_enabled": getattr(Config, 'OCR_FALLBACK_ENABLED', False),
                "max_tokens": Config.MAX_TOKENS,
                "timeout_seconds": Config.TIMEOUT_SECONDS
            }
        })
    except Exception as e:
        logger.error(f"å¤šæ¨¡æ€å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return jsonify({
            "local_status": "error",
            "error": str(e),
            "model_type": "Qwen2.5-VL-32B-Instruct-LoRA-Trained",
            "connection": "failed"
        }), 500

@upload_bp.route('/generate_practice', methods=['POST'])
def generate_practice_questions():
    """ç”Ÿæˆç»ƒä¹ é¢˜"""
    try:
        data = request.get_json()
        knowledge_points = data.get('knowledge_points', [])
        count = data.get('count', 3)
        
        logger.info(f"ç”Ÿæˆç»ƒä¹ é¢˜è¯·æ±‚: çŸ¥è¯†ç‚¹={knowledge_points}, æ•°é‡={count}")
        
        # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
        try:
            from app.services.grading_qwen import get_ai_service_status
            get_ai_service_status()
        except ImportError as e:
            logger.warning(f"æ— æ³•å¯¼å…¥AIæœåŠ¡çŠ¶æ€æ£€æŸ¥: {e}")
        
        # ç®€å•çš„ç»ƒä¹ é¢˜ç”Ÿæˆé€»è¾‘
        practice_questions = []
        for i, kp in enumerate(knowledge_points[:count]):
            practice_questions.append({
                'id': f'practice_{i+1}',
                'question': f'å…³äº"{kp}"çš„ç»ƒä¹ é¢˜ {i+1}',
                'knowledge_point': kp,
                'difficulty': 'medium',
                'type': 'è®¡ç®—é¢˜'
            })
        
        return jsonify({
            'status': 'success',
            'practice_questions': practice_questions,
            'knowledge_points': knowledge_points,
            'generated_count': len(practice_questions)
        })
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆç»ƒä¹ é¢˜å¤±è´¥: {e}")
        return jsonify({
            'status': 'error',
            'message': f'ç”Ÿæˆç»ƒä¹ é¢˜å¤±è´¥: {str(e)}'
        }), 500

def allowed_file(filename):
    """æ£€æŸ¥æ–‡ä»¶æ‰©å±•åæ˜¯å¦å…è®¸"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in current_app.config['ALLOWED_EXTENSIONS']