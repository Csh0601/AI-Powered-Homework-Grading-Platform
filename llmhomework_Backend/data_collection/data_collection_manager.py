#!/usr/bin/env python3
"""
ç»Ÿä¸€æ•°æ®æ”¶é›†ç®¡ç†å™¨
æ•´åˆæ‰€æœ‰æ•°æ®æ”¶é›†ã€å¤„ç†ã€å¢å¼ºã€å¯¼å…¥åŠŸèƒ½
æä¾›ä¸€ç«™å¼æ•°æ®ç®¡ç†è§£å†³æ–¹æ¡ˆ
"""

import os
import sys
import json
import logging
import pandas as pd
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# è®¾ç½®è·¯å¾„
sys.path.append(os.path.dirname(__file__))
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataCollectionManager:
    """æ•°æ®æ”¶é›†ç®¡ç†å¹³å°"""

    def __init__(self):
        self.base_dir = os.path.dirname(__file__)
        self.collectors_dir = os.path.join(self.base_dir, 'collectors')
        self.scripts_dir = os.path.join(self.base_dir, 'scripts')
        self.processed_dir = os.path.join(self.base_dir, 'processed')
        os.makedirs(self.processed_dir, exist_ok=True)

        self.collection_stats = {
            'timestamp': datetime.now().isoformat(),
            'total_knowledge_points': 0,
            'total_questions': 0,
            'total_subjects': 0,
            'data_quality_score': 0.0,
            'processing_steps': []
        }

        # å¯ç”¨æ”¶é›†å™¨åˆ—è¡¨
        self.available_collectors = {
            'smart_generator': {
                'name': 'æ™ºèƒ½æ•°æ®ç”Ÿæˆå™¨',
                'description': 'åŸºäºAIç”Ÿæˆé«˜è´¨é‡çš„æ•™è‚²æ•°æ®',
                'module': 'collectors.smart_data_generator',
                'function': 'generate_full_dataset'
            },
            'data_enhancer': {
                'name': 'æ•°æ®å¢å¼ºå™¨',
                'description': 'æå‡ç°æœ‰æ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§',
                'module': 'collectors.data_enhancer',
                'function': 'run_full_enhancement'
            },
            'education_crawler': {
                'name': 'æ•™è‚²ç½‘ç«™çˆ¬è™«',
                'description': 'ä»åˆæ³•æ•™è‚²ç½‘ç«™æ”¶é›†æ•°æ®',
                'module': 'collectors.legal_education_crawler',
                'function': 'run_full_crawl'
            }
        }

        # æ•°æ®å¤„ç†æµç¨‹
        self.processing_pipeline = [
            {'name': 'æ•°æ®æ”¶é›†', 'function': self.run_data_collection, 'required': True},
            {'name': 'æ•°æ®ç»Ÿä¸€', 'function': self.run_data_unification, 'required': True},
            {'name': 'æ•°æ®å¢å¼º', 'function': self.run_data_enhancement, 'required': True},
            {'name': 'æ•°æ®éªŒè¯', 'function': self.run_data_validation, 'required': True},
            {'name': 'è´¨é‡æŠ¥å‘Š', 'function': self.generate_quality_report, 'required': False}
        ]

    def run_data_collection(self) -> bool:
        """è¿è¡Œæ•°æ®æ”¶é›†"""
        logger.info("ğŸ“Š å¼€å§‹æ•°æ®æ”¶é›†...")

        try:
            # 1. è¿è¡Œæ™ºèƒ½æ•°æ®ç”Ÿæˆå™¨
            logger.info("ğŸ¤– è¿è¡Œæ™ºèƒ½æ•°æ®ç”Ÿæˆå™¨...")
            from collectors.smart_data_generator import SmartDataGenerator

            generator = SmartDataGenerator()
            generator.generate_full_dataset(
                subjects=['math', 'chinese', 'english', 'physics', 'chemistry', 'biology'],
                kp_per_subject=50,
                q_per_subject=80
            )

            step_result = {
                'step': 'æ•°æ®æ”¶é›†',
                'status': 'success',
                'timestamp': datetime.now().isoformat(),
                'details': 'æ™ºèƒ½æ•°æ®ç”Ÿæˆå®Œæˆ'
            }

            self.collection_stats['processing_steps'].append(step_result)
            return True

        except Exception as e:
            logger.error(f"âŒ æ•°æ®æ”¶é›†å¤±è´¥: {e}")
            step_result = {
                'step': 'æ•°æ®æ”¶é›†',
                'status': 'failed',
                'timestamp': datetime.now().isoformat(),
                'error': str(e)
            }
            self.collection_stats['processing_steps'].append(step_result)
            return False

    def run_data_unification(self) -> bool:
        """è¿è¡Œæ•°æ®ç»Ÿä¸€å¤„ç†"""
        logger.info("ğŸ”„ å¼€å§‹æ•°æ®ç»Ÿä¸€å¤„ç†...")

        try:
            # è°ƒç”¨ç»Ÿä¸€å¤„ç†è„šæœ¬
            from scripts.unify_data import unify_all_data
            result = unify_all_data()

            step_result = {
                'step': 'æ•°æ®ç»Ÿä¸€',
                'status': 'success' if result else 'failed',
                'timestamp': datetime.now().isoformat(),
                'details': 'æ•°æ®ç»Ÿä¸€å¤„ç†å®Œæˆ' if result else 'æ•°æ®ç»Ÿä¸€å¤„ç†å¤±è´¥'
            }

            self.collection_stats['processing_steps'].append(step_result)
            return result

        except Exception as e:
            logger.error(f"âŒ æ•°æ®ç»Ÿä¸€å¤„ç†å¤±è´¥: {e}")
            step_result = {
                'step': 'æ•°æ®ç»Ÿä¸€',
                'status': 'failed',
                'timestamp': datetime.now().isoformat(),
                'error': str(e)
            }
            self.collection_stats['processing_steps'].append(step_result)
            return False

    def run_data_enhancement(self) -> bool:
        """è¿è¡Œæ•°æ®å¢å¼º"""
        logger.info("ğŸš€ å¼€å§‹æ•°æ®å¢å¼º...")

        try:
            # è°ƒç”¨æ•°æ®å¢å¼ºå™¨
            from collectors.data_enhancer import DataEnhancer

            enhancer = DataEnhancer()
            kp_file, q_file = enhancer.run_full_enhancement()

            step_result = {
                'step': 'æ•°æ®å¢å¼º',
                'status': 'success' if kp_file and q_file else 'failed',
                'timestamp': datetime.now().isoformat(),
                'details': f'çŸ¥è¯†ç‚¹å¢å¼º: {kp_file}, é¢˜ç›®å¢å¼º: {q_file}' if kp_file and q_file else 'æ•°æ®å¢å¼ºå¤±è´¥'
            }

            self.collection_stats['processing_steps'].append(step_result)
            return kp_file and q_file

        except Exception as e:
            logger.error(f"âŒ æ•°æ®å¢å¼ºå¤±è´¥: {e}")
            step_result = {
                'step': 'æ•°æ®å¢å¼º',
                'status': 'failed',
                'timestamp': datetime.now().isoformat(),
                'error': str(e)
            }
            self.collection_stats['processing_steps'].append(step_result)
            return False

    def run_data_validation(self) -> bool:
        """è¿è¡Œæ•°æ®éªŒè¯"""
        logger.info("ğŸ” å¼€å§‹æ•°æ®éªŒè¯...")

        try:
            # è°ƒç”¨æ•°æ®éªŒè¯è„šæœ¬
            from scripts.validate_data import validate_data_quality
            result = validate_data_quality()

            step_result = {
                'step': 'æ•°æ®éªŒè¯',
                'status': 'success' if result else 'failed',
                'timestamp': datetime.now().isoformat(),
                'details': 'æ•°æ®éªŒè¯å®Œæˆ' if result else 'æ•°æ®éªŒè¯å¤±è´¥'
            }

            self.collection_stats['processing_steps'].append(step_result)
            return result

        except Exception as e:
            logger.error(f"âŒ æ•°æ®éªŒè¯å¤±è´¥: {e}")
            step_result = {
                'step': 'æ•°æ®éªŒè¯',
                'status': 'failed',
                'timestamp': datetime.now().isoformat(),
                'error': str(e)
            }
            self.collection_stats['processing_steps'].append(step_result)
            return False

    def generate_quality_report(self) -> bool:
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        logger.info("ğŸ“Š ç”Ÿæˆè´¨é‡æŠ¥å‘Š...")

        try:
            # ç»Ÿè®¡æ•°æ®
            self._collect_statistics()

            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            report = self._generate_comprehensive_report()

            # ä¿å­˜æŠ¥å‘Š
            report_file = os.path.join(
                self.processed_dir,
                f'data_collection_comprehensive_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
            )

            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)

            logger.info(f"âœ… è´¨é‡æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

            step_result = {
                'step': 'è´¨é‡æŠ¥å‘Š',
                'status': 'success',
                'timestamp': datetime.now().isoformat(),
                'details': f'æŠ¥å‘Šå·²ä¿å­˜: {report_file}'
            }

            self.collection_stats['processing_steps'].append(step_result)
            return True

        except Exception as e:
            logger.error(f"âŒ è´¨é‡æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            step_result = {
                'step': 'è´¨é‡æŠ¥å‘Š',
                'status': 'failed',
                'timestamp': datetime.now().isoformat(),
                'error': str(e)
            }
            self.collection_stats['processing_steps'].append(step_result)
            return False

    def _collect_statistics(self):
        """æ”¶é›†æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # ç»Ÿè®¡çŸ¥è¯†ç‚¹æ•°é‡
            kp_files = list(Path(self.processed_dir).glob('knowledge_points_*.csv'))
            total_kp = 0

            for file_path in kp_files:
                try:
                    df = pd.read_csv(file_path)
                    total_kp += len(df)
                except:
                    continue

            # ç»Ÿè®¡é¢˜ç›®æ•°é‡
            q_files = list(Path(self.processed_dir).glob('questions_*.csv'))
            total_q = 0

            for file_path in q_files:
                try:
                    df = pd.read_csv(file_path)
                    total_q += len(df)
                except:
                    continue

            # ç»Ÿè®¡å­¦ç§‘æ•°é‡
            subjects = set()
            for file_path in kp_files + q_files:
                try:
                    df = pd.read_csv(file_path)
                    if 'subject' in df.columns:
                        subjects.update(df['subject'].unique())
                except:
                    continue

            # è®¡ç®—è´¨é‡åˆ†æ•°
            quality_score = self._calculate_overall_quality()

            self.collection_stats.update({
                'total_knowledge_points': total_kp,
                'total_questions': total_q,
                'total_subjects': len(subjects),
                'data_quality_score': quality_score
            })

        except Exception as e:
            logger.error(f"âŒ ç»Ÿè®¡æ”¶é›†å¤±è´¥: {e}")

    def _calculate_overall_quality(self) -> float:
        """è®¡ç®—æ•´ä½“æ•°æ®è´¨é‡åˆ†æ•°"""
        try:
            # åŸºäºå„ç§æŒ‡æ ‡è®¡ç®—è´¨é‡åˆ†æ•°
            quality_score = 0.0
            score_factors = 0

            # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            required_files = [
                'knowledge_points_unified_*.csv',
                'questions_unified_*.csv',
                'validation_report.json'
            ]

            for pattern in required_files:
                files = list(Path(self.processed_dir).glob(pattern))
                if files:
                    quality_score += 20.0  # æ¯ä¸ªå¿…éœ€æ–‡ä»¶20åˆ†
                    score_factors += 1

            # æ£€æŸ¥æ•°æ®é‡
            if self.collection_stats.get('total_knowledge_points', 0) > 200:
                quality_score += 15.0
            elif self.collection_stats.get('total_knowledge_points', 0) > 100:
                quality_score += 10.0

            if self.collection_stats.get('total_questions', 0) > 500:
                quality_score += 15.0
            elif self.collection_stats.get('total_questions', 0) > 200:
                quality_score += 10.0

            # æ£€æŸ¥å­¦ç§‘è¦†ç›–
            if self.collection_stats.get('total_subjects', 0) >= 6:
                quality_score += 15.0
            elif self.collection_stats.get('total_subjects', 0) >= 3:
                quality_score += 10.0

            # éªŒè¯æŠ¥å‘Šè´¨é‡
            validation_report = os.path.join(self.processed_dir, 'validation_report.json')
            if os.path.exists(validation_report):
                try:
                    with open(validation_report, 'r', encoding='utf-8') as f:
                        report = json.load(f)
                        overall_quality = report.get('validation_summary', {}).get('overall_quality', 'unknown')
                        if overall_quality == 'excellent':
                            quality_score += 15.0
                        elif overall_quality == 'good':
                            quality_score += 10.0
                        elif overall_quality == 'fair':
                            quality_score += 5.0
                except:
                    pass

            # æ ‡å‡†åŒ–ä¸º0-100åˆ†
            max_score = 100.0
            return min(max_score, quality_score)

        except Exception as e:
            logger.error(f"âŒ è´¨é‡è®¡ç®—å¤±è´¥: {e}")
            return 0.0

    def _generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        return {
            'report_type': 'comprehensive_data_collection_report',
            'generation_time': datetime.now().isoformat(),
            'summary': {
                'total_knowledge_points': self.collection_stats['total_knowledge_points'],
                'total_questions': self.collection_stats['total_questions'],
                'total_subjects': self.collection_stats['total_subjects'],
                'data_quality_score': self.collection_stats['data_quality_score'],
                'overall_grade': self._get_quality_grade(self.collection_stats['data_quality_score'])
            },
            'detailed_statistics': self.collection_stats,
            'processing_history': self.collection_stats['processing_steps'],
            'file_inventory': self._generate_file_inventory(),
            'recommendations': self._generate_recommendations(),
            'next_steps': [
                'æ£€æŸ¥æ•°æ®è´¨é‡æŠ¥å‘Š',
                'æ ¹æ®å»ºè®®ä¼˜åŒ–æ•°æ®æ”¶é›†ç­–ç•¥',
                'å‡†å¤‡æ•°æ®å¯¼å…¥æ•°æ®åº“',
                'è¿›è¡ŒAIæ¨¡å‹è®­ç»ƒæµ‹è¯•',
                'ç›‘æ§æ•°æ®ä½¿ç”¨æ•ˆæœ'
            ]
        }

    def _get_quality_grade(self, score: float) -> str:
        """æ ¹æ®åˆ†æ•°è·å–è´¨é‡ç­‰çº§"""
        if score >= 90:
            return 'ä¼˜ç§€ (Açº§)'
        elif score >= 80:
            return 'è‰¯å¥½ (Bçº§)'
        elif score >= 70:
            return 'ä¸€èˆ¬ (Cçº§)'
        elif score >= 60:
            return 'éœ€æ”¹è¿› (Dçº§)'
        else:
            return 'ä¸åˆæ ¼ (Fçº§)'

    def _generate_file_inventory(self) -> Dict[str, List[str]]:
        """ç”Ÿæˆæ–‡ä»¶æ¸…å•"""
        inventory = {}

        # çŸ¥è¯†ç‚¹æ–‡ä»¶
        kp_files = list(Path(self.processed_dir).glob('knowledge_points_*.csv'))
        inventory['knowledge_points'] = [str(f.name) for f in sorted(kp_files)]

        # é¢˜ç›®æ–‡ä»¶
        q_files = list(Path(self.processed_dir).glob('questions_*.csv'))
        inventory['questions'] = [str(f.name) for f in sorted(q_files)]

        # æŠ¥å‘Šæ–‡ä»¶
        report_files = list(Path(self.processed_dir).glob('*report*.json'))
        inventory['reports'] = [str(f.name) for f in sorted(report_files)]

        return inventory

    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        quality_score = self.collection_stats['data_quality_score']

        if quality_score < 60:
            recommendations.append('æ•°æ®è´¨é‡ä¸åˆæ ¼ï¼Œå»ºè®®é‡æ–°æ”¶é›†æ•°æ®')
        elif quality_score < 80:
            recommendations.append('æ•°æ®è´¨é‡ä¸€èˆ¬ï¼Œå»ºè®®å¢åŠ æ›´å¤šé«˜è´¨é‡æ•°æ®æº')

        if self.collection_stats['total_knowledge_points'] < 300:
            recommendations.append('çŸ¥è¯†ç‚¹æ•°é‡ä¸è¶³ï¼Œå»ºè®®å¢åŠ çŸ¥è¯†ç‚¹æ”¶é›†é‡')

        if self.collection_stats['total_questions'] < 800:
            recommendations.append('é¢˜ç›®æ•°é‡ä¸è¶³ï¼Œå»ºè®®å¢åŠ é¢˜ç›®æ”¶é›†é‡')

        if self.collection_stats['total_subjects'] < 6:
            recommendations.append('å­¦ç§‘è¦†ç›–ä¸å¤Ÿå…¨é¢ï¼Œå»ºè®®å¢åŠ æ›´å¤šå­¦ç§‘æ•°æ®')

        # æ£€æŸ¥å¤„ç†æ­¥éª¤æ˜¯å¦éƒ½æˆåŠŸ
        failed_steps = [step for step in self.collection_stats['processing_steps'] if step['status'] == 'failed']
        if failed_steps:
            recommendations.append(f'å‘ç° {len(failed_steps)} ä¸ªå¤„ç†æ­¥éª¤å¤±è´¥ï¼Œéœ€è¦é‡æ–°æ‰§è¡Œ')

        if not recommendations:
            recommendations.append('æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥è¿›è¡Œä¸‹ä¸€æ­¥å¤„ç†')

        return recommendations

    def import_data_to_database(self) -> bool:
        """å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“"""
        logger.info("ğŸ’¾ å¼€å§‹æ•°æ®å¯¼å…¥...")
        
        try:
            from scripts.import_to_db import import_to_database
            result = import_to_database()
            
            step_result = {
                'step': 'æ•°æ®å¯¼å…¥',
                'status': 'success' if result else 'failed',
                'timestamp': datetime.now().isoformat(),
                'details': 'æ•°æ®å¯¼å…¥å®Œæˆ' if result else 'æ•°æ®å¯¼å…¥å¤±è´¥'
            }
            
            self.collection_stats['processing_steps'].append(step_result)
            return result
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®å¯¼å…¥å¤±è´¥: {e}")
            step_result = {
                'step': 'æ•°æ®å¯¼å…¥',
                'status': 'failed',
                'timestamp': datetime.now().isoformat(),
                'error': str(e)
            }
            self.collection_stats['processing_steps'].append(step_result)
            return False

    def collect_data_by_method(self, method="all") -> bool:
        """æŒ‰æŒ‡å®šæ–¹æ³•æ”¶é›†æ•°æ®"""
        logger.info(f"ğŸš€ å¼€å§‹æ•°æ®æ”¶é›†: {method}")
        
        try:
            if method in ["all", "ai"]:
                logger.info("ğŸ¤– è¿è¡ŒAIæ•°æ®ç”Ÿæˆå™¨...")
                from collectors.smart_data_generator import SmartDataGenerator
                generator = SmartDataGenerator()
                generator.generate_full_dataset(
                    subjects=['math', 'chinese', 'english', 'physics', 'chemistry', 'biology'],
                    kp_per_subject=50,
                    q_per_subject=80
                )
            
            if method in ["all", "crawl"]:
                logger.info("ğŸŒ è¿è¡Œç½‘ç«™çˆ¬è™«...")
                from collectors.legal_education_crawler import main as crawler_main
                crawler_main()
            
            if method in ["all", "pdf"]:
                logger.info("ğŸ“„ è¿è¡ŒPDFå¤„ç†å™¨...")
                from collectors.pdf_document_processor import main as pdf_main
                pdf_main()
            
            logger.info("âœ… æ•°æ®æ”¶é›†å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®æ”¶é›†å¤±è´¥: {e}")
            return False

    def run_full_collection_pipeline(self, include_import: bool = True) -> bool:
        """è¿è¡Œå®Œæ•´çš„æ•°æ®æ”¶é›†æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´æ•°æ®æ”¶é›†æµç¨‹...")

        # æ·»åŠ æ•°æ®å¯¼å…¥æ­¥éª¤
        pipeline = self.processing_pipeline.copy()
        if include_import:
            pipeline.append({'name': 'æ•°æ®å¯¼å…¥', 'function': self.import_data_to_database, 'required': False})

        success_count = 0
        total_steps = len(pipeline)

        for step in pipeline:
            logger.info(f"\n{'='*50}")
            logger.info(f"ğŸ“‹ æ‰§è¡Œæ­¥éª¤: {step['name']}")
            logger.info(f"{'='*50}")

            if step['function']():
                success_count += 1
                logger.info(f"âœ… {step['name']} å®Œæˆ")
            else:
                if step['required']:
                    logger.error(f"âŒ å¿…éœ€æ­¥éª¤ {step['name']} å¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
                    break
                else:
                    logger.warning(f"âš ï¸ å¯é€‰æ­¥éª¤ {step['name']} å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ")

        success_rate = success_count / total_steps * 100
        logger.info(f"\n{'='*50}")
        logger.info("ğŸ‰ æ•°æ®æ”¶é›†æµç¨‹å®Œæˆï¼")
        logger.info(f"ğŸ“Š æˆåŠŸç‡: {success_count}/{total_steps} ({success_rate:.1f}%)")
        logger.info(f"{'='*50}")

        return success_rate >= 80.0  # 80%æˆåŠŸç‡è§†ä¸ºæˆåŠŸ

    def get_collection_status(self) -> Dict[str, Any]:
        """è·å–æ”¶é›†çŠ¶æ€"""
        return {
            'status': 'running' if any(s['status'] == 'success' for s in self.collection_stats['processing_steps']) else 'not_started',
            'progress': len([s for s in self.collection_stats['processing_steps'] if s['status'] == 'success']) / len(self.processing_pipeline) * 100,
            'current_step': self.collection_stats['processing_steps'][-1]['step'] if self.collection_stats['processing_steps'] else 'æœªå¼€å§‹',
            'statistics': self.collection_stats
        }

    def get_data_status(self) -> Dict[str, Any]:
        """è·å–æ•°æ®çŠ¶æ€"""
        status = {
            "timestamp": datetime.now().isoformat(),
            "raw_data": {},
            "processed_data": {},
            "database_status": "unknown"
        }
        
        # æ£€æŸ¥åŸå§‹æ•°æ®
        raw_dir = os.path.join(self.base_dir, "..", "raw", "subjects")
        if os.path.exists(raw_dir):
            for subject in os.listdir(raw_dir):
                subject_dir = os.path.join(raw_dir, subject)
                if os.path.isdir(subject_dir):
                    file_count = sum([len(files) for r, d, files in os.walk(subject_dir)])
                    status["raw_data"][subject] = file_count
        
        # æ£€æŸ¥å¤„ç†åæ•°æ®
        kp_file = os.path.join(self.processed_dir, "knowledge_points_unified.csv")
        q_file = os.path.join(self.processed_dir, "questions_unified.csv")
        
        status["processed_data"]["knowledge_points_exists"] = os.path.exists(kp_file)
        status["processed_data"]["questions_exists"] = os.path.exists(q_file)
        
        if status["processed_data"]["knowledge_points_exists"]:
            df = pd.read_csv(kp_file)
            status["processed_data"]["knowledge_points_count"] = len(df)
        
        if status["processed_data"]["questions_exists"]:
            df = pd.read_csv(q_file)
            status["processed_data"]["questions_count"] = len(df)
        
        return status

    def show_interactive_menu(self):
        """æ˜¾ç¤ºäº¤äº’å¼èœå•"""
        print("\n" + "="*60)
        print("ğŸ“š AIä½œä¸šæ‰¹æ”¹ç³»ç»Ÿ - ç»Ÿä¸€æ•°æ®ç®¡ç†å·¥å…·")
        print("="*60)
        print("1. ğŸ¤– æ”¶é›†æ•°æ® (AIç”Ÿæˆ)")
        print("2. ğŸŒ æ”¶é›†æ•°æ® (ç½‘ç«™çˆ¬è™«)")
        print("3. ğŸ“„ æ”¶é›†æ•°æ® (PDFå¤„ç†)")
        print("4. ğŸ”„ æ”¶é›†æ•°æ® (å…¨éƒ¨æ–¹å¼)")
        print("5. âš™ï¸  å¤„ç†æ•°æ®")
        print("6. ğŸ” éªŒè¯æ•°æ®")
        print("7. ğŸš€ å¢å¼ºæ•°æ®")
        print("8. ğŸ’¾ å¯¼å…¥æ•°æ®åº“")
        print("9. ğŸ¯ è¿è¡Œå®Œæ•´ç®¡é“")
        print("10. ğŸ“Š æŸ¥çœ‹æ•°æ®çŠ¶æ€")
        print("11. ğŸ“‹ æŸ¥çœ‹æ”¶é›†çŠ¶æ€")
        print("12. âŒ é€€å‡º")
        print("="*60)

    def run_interactive_mode(self):
        """è¿è¡Œäº¤äº’æ¨¡å¼"""
        while True:
            self.show_interactive_menu()
            choice = input("è¯·é€‰æ‹©æ“ä½œ (1-12): ").strip()
            
            if choice == "1":
                self.collect_data_by_method("ai")
            elif choice == "2":
                self.collect_data_by_method("crawl")
            elif choice == "3":
                self.collect_data_by_method("pdf")
            elif choice == "4":
                self.collect_data_by_method("all")
            elif choice == "5":
                self.run_data_unification()
            elif choice == "6":
                self.run_data_validation()
            elif choice == "7":
                self.run_data_enhancement()
            elif choice == "8":
                self.import_data_to_database()
            elif choice == "9":
                collect_method = input("é€‰æ‹©æ”¶é›†æ–¹å¼ (ai/crawl/pdf/all) [all]: ").strip() or "all"
                self.collect_data_by_method(collect_method)
                self.run_full_collection_pipeline(include_import=True)
            elif choice == "10":
                status = self.get_data_status()
                print("\nğŸ“Š æ•°æ®çŠ¶æ€:")
                print(f"ğŸ“… æ£€æŸ¥æ—¶é—´: {status['timestamp']}")
                print(f"ğŸ“ åŸå§‹æ•°æ®: {status['raw_data']}")
                print(f"âš™ï¸  å¤„ç†åæ•°æ®: {status['processed_data']}")
            elif choice == "11":
                status = self.get_collection_status()
                print("\nğŸ“‹ æ”¶é›†çŠ¶æ€:")
                print(json.dumps(status, ensure_ascii=False, indent=2))
            elif choice == "12":
                print("ğŸ‘‹ é€€å‡ºæ•°æ®ç®¡ç†å·¥å…·")
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
            
            input("\næŒ‰å›è½¦é”®ç»§ç»­...")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ§  å¯åŠ¨æ•°æ®æ”¶é›†ç®¡ç†å¹³å°...")

    try:
        manager = DataCollectionManager()

        # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
        if len(sys.argv) > 1:
            command = sys.argv[1]

            if command == 'status':
                status = manager.get_collection_status()
                logger.info("ğŸ“Š å½“å‰çŠ¶æ€:")
                logger.info(json.dumps(status, ensure_ascii=False, indent=2))

            elif command == 'data-status':
                status = manager.get_data_status()
                logger.info("ğŸ“Š æ•°æ®çŠ¶æ€:")
                logger.info(json.dumps(status, ensure_ascii=False, indent=2))

            elif command == 'interactive' or command == 'menu':
                manager.run_interactive_mode()

            elif command == 'full':
                success = manager.run_full_collection_pipeline()
                if success:
                    logger.info("ğŸ‰ å®Œæ•´æ•°æ®æ”¶é›†æµç¨‹æˆåŠŸå®Œæˆï¼")
                else:
                    logger.error("âŒ å®Œæ•´æ•°æ®æ”¶é›†æµç¨‹å¤±è´¥")

            elif command == 'collect':
                method = sys.argv[2] if len(sys.argv) > 2 else "all"
                success = manager.collect_data_by_method(method)
                if success:
                    logger.info(f"âœ… æ•°æ®æ”¶é›† ({method}) å®Œæˆ")
                else:
                    logger.error(f"âŒ æ•°æ®æ”¶é›† ({method}) å¤±è´¥")

            elif command == 'step':
                if len(sys.argv) > 2:
                    step_name = sys.argv[2]
                    step_functions = {
                        'collection': manager.run_data_collection,
                        'unification': manager.run_data_unification,
                        'enhancement': manager.run_data_enhancement,
                        'validation': manager.run_data_validation,
                        'report': manager.generate_quality_report,
                        'import': manager.import_data_to_database
                    }

                    if step_name in step_functions:
                        success = step_functions[step_name]()
                        if success:
                            logger.info(f"âœ… {step_name} æ­¥éª¤å®Œæˆ")
                        else:
                            logger.error(f"âŒ {step_name} æ­¥éª¤å¤±è´¥")
                    else:
                        logger.error(f"âŒ æœªçŸ¥æ­¥éª¤: {step_name}")
                        logger.info("å¯ç”¨æ­¥éª¤: collection, unification, enhancement, validation, report, import")
                else:
                    logger.error("âŒ è¯·æŒ‡å®šæ­¥éª¤åç§°: collection, unification, enhancement, validation, report, import")

            elif command == 'help':
                print("\nğŸ“– å¯ç”¨å‘½ä»¤:")
                print("  status          - æŸ¥çœ‹æ”¶é›†çŠ¶æ€")
                print("  data-status     - æŸ¥çœ‹æ•°æ®çŠ¶æ€")
                print("  interactive     - è¿›å…¥äº¤äº’æ¨¡å¼")
                print("  full            - è¿è¡Œå®Œæ•´æµç¨‹")
                print("  collect [method] - æ”¶é›†æ•°æ® (ai/crawl/pdf/all)")
                print("  step [name]     - è¿è¡Œå•ä¸ªæ­¥éª¤")
                print("  help            - æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")

            else:
                logger.error(f"âŒ æœªçŸ¥å‘½ä»¤: {command}")
                logger.info("ä½¿ç”¨ 'help' æŸ¥çœ‹å¯ç”¨å‘½ä»¤")

        else:
            # é»˜è®¤è¿›å…¥äº¤äº’æ¨¡å¼
            manager.run_interactive_mode()

    except Exception as e:
        logger.error(f"âŒ è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

