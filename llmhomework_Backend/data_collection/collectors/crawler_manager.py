#!/usr/bin/env python3
"""
çˆ¬è™«ç®¡ç†å™¨
ç»Ÿä¸€ç®¡ç†æ‰€æœ‰åˆæ³•çš„æ•°æ®è·å–å·¥å…·
"""

import os
import json
import time
import logging
from datetime import datetime
from typing import Dict, List, Any

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CrawlerManager:
    """çˆ¬è™«ç®¡ç†å™¨"""
    
    def __init__(self):
        self.base_dir = os.path.dirname(__file__)
        self.output_dir = os.path.join(self.base_dir, 'all_crawled_data')
        os.makedirs(self.output_dir, exist_ok=True)
        
        # çˆ¬è™«é…ç½®
        self.crawlers = {
            'smart_generator': {
                'name': 'æ™ºèƒ½æ•°æ®ç”Ÿæˆå™¨',
                'module': 'smart_data_generator',
                'description': 'åŸºäºè¯¾ç¨‹æ ‡å‡†ç”Ÿæˆé«˜è´¨é‡æ•°æ®',
                'enabled': True,
                'priority': 1
            },
            'legal_crawler': {
                'name': 'åˆæ³•æ•™è‚²çˆ¬è™«',
                'module': 'legal_education_crawler',
                'description': 'çˆ¬å–å®˜æ–¹æ•™è‚²ç½‘ç«™',
                'enabled': True,
                'priority': 2
            },
            'pdf_processor': {
                'name': 'PDFæ–‡æ¡£å¤„ç†å™¨',
                'module': 'pdf_document_processor',
                'description': 'å¤„ç†å®˜æ–¹æ•™è‚²æ–‡æ¡£',
                'enabled': True,
                'priority': 3
            }
        }
        
        self.results = {
            'start_time': datetime.now().isoformat(),
            'crawlers_run': [],
            'total_knowledge_points': 0,
            'total_questions': 0,
            'errors': [],
            'files_generated': []
        }
    
    def run_crawler(self, crawler_key: str) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªçˆ¬è™«"""
        crawler_config = self.crawlers.get(crawler_key)
        if not crawler_config or not crawler_config['enabled']:
            logger.warning(f"âš ï¸ çˆ¬è™« {crawler_key} æœªå¯ç”¨æˆ–ä¸å­˜åœ¨")
            return {'success': False, 'error': 'çˆ¬è™«æœªå¯ç”¨'}
        
        logger.info(f"ğŸš€ å¯åŠ¨ {crawler_config['name']}...")
        
        try:
            start_time = time.time()
            
            if crawler_key == 'smart_generator':
                result = self._run_smart_generator()
            elif crawler_key == 'legal_crawler':
                result = self._run_legal_crawler()
            elif crawler_key == 'pdf_processor':
                result = self._run_pdf_processor()
            else:
                return {'success': False, 'error': f'æœªçŸ¥çˆ¬è™«: {crawler_key}'}
            
            end_time = time.time()
            duration = end_time - start_time
            
            result.update({
                'crawler': crawler_key,
                'duration': duration,
                'success': True
            })
            
            logger.info(f"âœ… {crawler_config['name']} å®Œæˆï¼Œè€—æ—¶ {duration:.2f}ç§’")
            return result
            
        except Exception as e:
            error_msg = f"{crawler_config['name']} è¿è¡Œå¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            self.results['errors'].append(error_msg)
            
            return {
                'success': False,
                'error': error_msg,
                'crawler': crawler_key
            }
    
    def _run_smart_generator(self) -> Dict[str, Any]:
        """è¿è¡Œæ™ºèƒ½æ•°æ®ç”Ÿæˆå™¨"""
        try:
            from smart_data_generator import SmartDataGenerator
            
            generator = SmartDataGenerator()
            generator.generate_full_dataset(
                subjects=['math', 'chinese', 'english'],
                kp_per_subject=40,
                q_per_subject=60
            )
            
            return {
                'knowledge_points': 120,  # 3ç§‘ * 40ä¸ª
                'questions': 180,         # 3ç§‘ * 60é“
                'source': 'generated'
            }
            
        except ImportError:
            return {'knowledge_points': 0, 'questions': 0, 'error': 'ç”Ÿæˆå™¨æ¨¡å—æœªæ‰¾åˆ°'}
    
    def _run_legal_crawler(self) -> Dict[str, Any]:
        """è¿è¡Œåˆæ³•æ•™è‚²çˆ¬è™«"""
        try:
            from legal_education_crawler import LegalEducationCrawler
            
            crawler = LegalEducationCrawler()
            kp_count, q_count = crawler.run_full_crawl()
            
            return {
                'knowledge_points': kp_count,
                'questions': q_count,
                'source': 'crawled'
            }
            
        except ImportError:
            return {'knowledge_points': 0, 'questions': 0, 'error': 'çˆ¬è™«æ¨¡å—æœªæ‰¾åˆ°'}
    
    def _run_pdf_processor(self) -> Dict[str, Any]:
        """è¿è¡ŒPDFæ–‡æ¡£å¤„ç†å™¨"""
        try:
            from pdf_document_processor import PDFDocumentProcessor
            
            processor = PDFDocumentProcessor()
            kp_count = processor.run_processing()
            
            return {
                'knowledge_points': kp_count,
                'questions': 0,  # PDFå¤„ç†å™¨ä¸»è¦æå–çŸ¥è¯†ç‚¹
                'source': 'pdf_extracted'
            }
            
        except ImportError:
            return {'knowledge_points': 0, 'questions': 0, 'error': 'PDFå¤„ç†å™¨æ¨¡å—æœªæ‰¾åˆ°'}
    
    def run_all_crawlers(self):
        """è¿è¡Œæ‰€æœ‰å¯ç”¨çš„çˆ¬è™«"""
        logger.info("ğŸŒ å¼€å§‹è¿è¡Œæ‰€æœ‰åˆæ³•çˆ¬è™«...")
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_crawlers = sorted(
            self.crawlers.items(),
            key=lambda x: x[1]['priority']
        )
        
        for crawler_key, config in sorted_crawlers:
            if not config['enabled']:
                continue
            
            logger.info(f"ğŸ“‹ {config['name']}: {config['description']}")
            
            result = self.run_crawler(crawler_key)
            self.results['crawlers_run'].append(result)
            
            if result['success']:
                self.results['total_knowledge_points'] += result.get('knowledge_points', 0)
                self.results['total_questions'] += result.get('questions', 0)
            
            # çˆ¬è™«é—´ä¼‘æ¯ï¼Œé¿å…è¿‡è½½
            time.sleep(2)
        
        self.results['end_time'] = datetime.now().isoformat()
        
        # ç»Ÿä¸€å¤„ç†æ•°æ®
        self._unify_all_data()
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_final_report()
    
    def _unify_all_data(self):
        """ç»Ÿä¸€å¤„ç†æ‰€æœ‰çˆ¬å–çš„æ•°æ®"""
        logger.info("ğŸ”„ å¼€å§‹ç»Ÿä¸€å¤„ç†æ•°æ®...")
        
        try:
            from unify_generated_data import unify_data
            stats = unify_data()
            
            self.results['unified_stats'] = stats
            logger.info("âœ… æ•°æ®ç»Ÿä¸€å¤„ç†å®Œæˆ")
            
        except Exception as e:
            error_msg = f"æ•°æ®ç»Ÿä¸€å¤„ç†å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            self.results['errors'].append(error_msg)
    
    def _generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        report = {
            'crawler_session': {
                'session_id': f"crawl_session_{timestamp}",
                'start_time': self.results['start_time'],
                'end_time': self.results.get('end_time'),
                'total_duration': self._calculate_total_duration()
            },
            'crawler_results': self.results['crawlers_run'],
            'summary': {
                'total_knowledge_points': self.results['total_knowledge_points'],
                'total_questions': self.results['total_questions'],
                'successful_crawlers': len([r for r in self.results['crawlers_run'] if r['success']]),
                'failed_crawlers': len([r for r in self.results['crawlers_run'] if not r['success']]),
                'errors_count': len(self.results['errors'])
            },
            'errors': self.results['errors'],
            'unified_stats': self.results.get('unified_stats', {}),
            'next_steps': [
                'æ£€æŸ¥unified_data.csvæ–‡ä»¶è´¨é‡',
                'è¿è¡Œæ•°æ®éªŒè¯è„šæœ¬',
                'å¯¼å…¥æ•°æ®åº“',
                'æµ‹è¯•AIæ‰¹æ”¹ç³»ç»Ÿ'
            ]
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(self.output_dir, f'crawler_session_report_{timestamp}.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # æ§åˆ¶å°è¾“å‡ºæ‘˜è¦
        self._print_summary(report)
        
        return report
    
    def _calculate_total_duration(self) -> float:
        """è®¡ç®—æ€»è¿è¡Œæ—¶é—´"""
        if 'end_time' in self.results:
            start = datetime.fromisoformat(self.results['start_time'])
            end = datetime.fromisoformat(self.results['end_time'])
            return (end - start).total_seconds()
        return 0
    
    def _print_summary(self, report: Dict):
        """æ‰“å°æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ‰ åˆæ³•çˆ¬è™«è¿è¡Œå®Œæˆ!")
        print("="*60)
        
        summary = report['summary']
        print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"  - çŸ¥è¯†ç‚¹æ€»æ•°: {summary['total_knowledge_points']}")
        print(f"  - é¢˜ç›®æ€»æ•°: {summary['total_questions']}")
        print(f"  - æˆåŠŸçˆ¬è™«: {summary['successful_crawlers']}")
        print(f"  - å¤±è´¥çˆ¬è™«: {summary['failed_crawlers']}")
        
        if summary['errors_count'] > 0:
            print(f"âš ï¸  é”™è¯¯æ•°é‡: {summary['errors_count']}")
        
        print(f"\nğŸ“ æ•°æ®ä½ç½®:")
        print(f"  - ç»Ÿä¸€æ•°æ®: processed/knowledge_points_unified.csv")
        print(f"  - ç»Ÿä¸€é¢˜ç›®: processed/questions_unified.csv")
        print(f"  - è¯¦ç»†æŠ¥å‘Š: {self.output_dir}/")
        
        print(f"\nğŸ”„ ä¸‹ä¸€æ­¥æ“ä½œ:")
        for step in report['next_steps']:
            print(f"  - {step}")
    
    def run_quick_test(self):
        """å¿«é€Ÿæµ‹è¯•æ‰€æœ‰çˆ¬è™«"""
        logger.info("âš¡ è¿è¡Œå¿«é€Ÿæµ‹è¯•...")
        
        # åªè¿è¡Œæ™ºèƒ½ç”Ÿæˆå™¨ï¼ˆæœ€å¿«ï¼‰
        result = self.run_crawler('smart_generator')
        
        if result['success']:
            print("âœ… å¿«é€Ÿæµ‹è¯•æˆåŠŸ!")
            print(f"ğŸ“Š ç”Ÿæˆæ•°æ®: {result.get('knowledge_points', 0)}ä¸ªçŸ¥è¯†ç‚¹, {result.get('questions', 0)}é“é¢˜ç›®")
        else:
            print("âŒ å¿«é€Ÿæµ‹è¯•å¤±è´¥!")
            print(f"é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ›ï¸ çˆ¬è™«ç®¡ç†å™¨")
    print("==============")
    print("1. è¿è¡Œæ‰€æœ‰çˆ¬è™«")
    print("2. å¿«é€Ÿæµ‹è¯•")
    print("3. è¿è¡Œå•ä¸ªçˆ¬è™«")
    
    choice = input("\nè¯·é€‰æ‹©æ“ä½œ (1-3): ").strip()
    
    manager = CrawlerManager()
    
    if choice == '1':
        manager.run_all_crawlers()
    elif choice == '2':
        manager.run_quick_test()
    elif choice == '3':
        print("\nå¯ç”¨çˆ¬è™«:")
        for key, config in manager.crawlers.items():
            status = "âœ…" if config['enabled'] else "âŒ"
            print(f"  {key}: {config['name']} {status}")
        
        crawler_key = input("\nè¯·è¾“å…¥çˆ¬è™«key: ").strip()
        result = manager.run_crawler(crawler_key)
        
        if result['success']:
            print("âœ… çˆ¬è™«è¿è¡ŒæˆåŠŸ!")
        else:
            print(f"âŒ çˆ¬è™«è¿è¡Œå¤±è´¥: {result.get('error')}")
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")

if __name__ == "__main__":
    main()
