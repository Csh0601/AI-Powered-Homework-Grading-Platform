#!/usr/bin/env python3
"""
PDFæ–‡æ¡£å¤„ç†å™¨
ä¸“é—¨å¤„ç†å®˜æ–¹å‘å¸ƒçš„æ•™è‚²PDFæ–‡æ¡£
æå–çŸ¥è¯†ç‚¹å’Œå­¦ä¹ ç›®æ ‡
"""

import os
import json
import pandas as pd
import logging
from datetime import datetime
from typing import List, Dict, Any
import re

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PDFDocumentProcessor:
    """PDFæ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self):
        self.output_dir = os.path.join(os.path.dirname(__file__), 'pdf_extracted_data')
        os.makedirs(self.output_dir, exist_ok=True)
        
        # çŸ¥è¯†ç‚¹æå–æ¨¡å¼
        self.knowledge_patterns = [
            r'(\d+\..*?ç†è§£.*?)',
            r'(\d+\..*?æŒæ¡.*?)',
            r'(\d+\..*?èƒ½å¤Ÿ.*?)',
            r'(\d+\..*?ä¼š.*?)',
            r'(.*?çš„æ¦‚å¿µ)',
            r'(.*?çš„æ€§è´¨)',
            r'(.*?çš„æ–¹æ³•)',
            r'(.*?çš„åº”ç”¨)',
            r'å­¦ç”Ÿåº”å½“(.*?)(?=ã€‚|ï¼Œ|ï¼›)',
            r'è¦æ±‚å­¦ç”Ÿ(.*?)(?=ã€‚|ï¼Œ|ï¼›)',
            r'æ ¸å¿ƒç´ å…».*?(.*?)(?=ã€‚|ï¼Œ|ï¼›)'
        ]
        
        # å­¦ç§‘å…³é”®è¯æ˜ å°„
        self.subject_keywords = {
            'æ•°å­¦': ['æ•°å­¦', 'ä»£æ•°', 'å‡ ä½•', 'å‡½æ•°', 'æ–¹ç¨‹', 'ç»Ÿè®¡', 'æ¦‚ç‡', 'è¿ç®—'],
            'è¯­æ–‡': ['è¯­æ–‡', 'é˜…è¯»', 'å†™ä½œ', 'æ–‡å­¦', 'è¯­è¨€', 'æ–‡å­—', 'å¤æ–‡', 'è¯—æ­Œ'],
            'è‹±è¯­': ['è‹±è¯­', 'English', 'è¯­æ³•', 'è¯æ±‡', 'å¬åŠ›', 'å£è¯­', 'é˜…è¯»', 'å†™ä½œ'],
            'ç‰©ç†': ['ç‰©ç†', 'åŠ›å­¦', 'ç”µå­¦', 'å…‰å­¦', 'çƒ­å­¦', 'å£°å­¦'],
            'åŒ–å­¦': ['åŒ–å­¦', 'åŒ–å­¦ååº”', 'å…ƒç´ ', 'åŒ–åˆç‰©', 'å®éªŒ'],
            'ç”Ÿç‰©': ['ç”Ÿç‰©', 'ç»†èƒ', 'é—ä¼ ', 'ç”Ÿæ€', 'è¿›åŒ–'],
            'å†å²': ['å†å²', 'æœä»£', 'äº‹ä»¶', 'äººç‰©', 'åˆ¶åº¦'],
            'åœ°ç†': ['åœ°ç†', 'åœ°å½¢', 'æ°”å€™', 'äººå£', 'èµ„æº'],
            'æ”¿æ²»': ['æ”¿æ²»', 'æ³•å¾‹', 'é“å¾·', 'ç¤¾ä¼š', 'å…¬æ°‘']
        }
        
        self.extracted_data = {
            'knowledge_points': [],
            'learning_objectives': [],
            'metadata': {
                'process_time': datetime.now().isoformat(),
                'files_processed': [],
                'total_extractions': 0
            }
        }
    
    def extract_from_text(self, text: str, filename: str) -> Dict[str, List[Dict]]:
        """ä»æ–‡æœ¬ä¸­æå–çŸ¥è¯†ç‚¹"""
        knowledge_points = []
        learning_objectives = []
        
        # ç¡®å®šå­¦ç§‘
        subject = self._detect_subject(text, filename)
        grade = self._detect_grade(text, filename)
        
        # æå–çŸ¥è¯†ç‚¹
        for pattern in self.knowledge_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0] if match[0] else match[1] if len(match) > 1 else ''
                
                if len(match) > 5 and len(match) < 200:
                    # æ¸…ç†æ–‡æœ¬
                    clean_match = self._clean_text(match)
                    
                    if clean_match and not self._is_duplicate(clean_match, knowledge_points):
                        kp = {
                            'name': clean_match,
                            'subject': subject,
                            'grade': grade,
                            'chapter': self._extract_chapter(text, clean_match),
                            'description': f'æ ¹æ®å®˜æ–¹æ–‡æ¡£æå–ï¼š{clean_match}',
                            'difficulty_level': self._estimate_difficulty(clean_match),
                            'importance_level': self._estimate_importance(clean_match),
                            'keywords': self._generate_keywords(clean_match, subject),
                            'source': filename,
                            'extraction_time': datetime.now().isoformat()
                        }
                        knowledge_points.append(kp)
        
        # æå–å­¦ä¹ ç›®æ ‡
        objective_patterns = [
            r'å­¦ä¹ ç›®æ ‡[ï¼š:](.*?)(?=\n|ã€‚)',
            r'æ•™å­¦ç›®æ ‡[ï¼š:](.*?)(?=\n|ã€‚)',
            r'èƒ½åŠ›è¦æ±‚[ï¼š:](.*?)(?=\n|ã€‚)',
            r'æ ¸å¿ƒç´ å…»[ï¼š:](.*?)(?=\n|ã€‚)'
        ]
        
        for pattern in objective_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            for match in matches:
                clean_objective = self._clean_text(match)
                if clean_objective:
                    learning_objectives.append({
                        'objective': clean_objective,
                        'subject': subject,
                        'grade': grade,
                        'source': filename
                    })
        
        return {
            'knowledge_points': knowledge_points,
            'learning_objectives': learning_objectives
        }
    
    def process_sample_curriculum_standards(self):
        """å¤„ç†ç¤ºä¾‹è¯¾ç¨‹æ ‡å‡†æ–‡æ¡£ï¼ˆæ¨¡æ‹ŸPDFå†…å®¹ï¼‰"""
        logger.info("ğŸ“– å¤„ç†ç¤ºä¾‹è¯¾ç¨‹æ ‡å‡†æ–‡æ¡£...")
        
        # æ¨¡æ‹Ÿä¸åŒå­¦ç§‘çš„è¯¾ç¨‹æ ‡å‡†å†…å®¹
        sample_documents = {
            'æ•°å­¦è¯¾ç¨‹æ ‡å‡†.pdf': """
            ä¹‰åŠ¡æ•™è‚²æ•°å­¦è¯¾ç¨‹æ ‡å‡†
            
            æ ¸å¿ƒç´ å…»åŒ…æ‹¬ï¼šæ•°å­¦æŠ½è±¡ã€é€»è¾‘æ¨ç†ã€æ•°å­¦å»ºæ¨¡ã€æ•°å­¦è¿ç®—ã€ç›´è§‚æƒ³è±¡ã€æ•°æ®åˆ†æã€‚
            
            ä¸ƒå¹´çº§æ•°å­¦å­¦ä¹ è¦æ±‚ï¼š
            1. ç†è§£æœ‰ç†æ•°çš„æ¦‚å¿µï¼ŒæŒæ¡æœ‰ç†æ•°çš„å››åˆ™è¿ç®—
            2. èƒ½å¤Ÿåœ¨æ•°è½´ä¸Šè¡¨ç¤ºæœ‰ç†æ•°ï¼Œç†è§£ç›¸åæ•°å’Œç»å¯¹å€¼
            3. æŒæ¡æ•´å¼çš„æ¦‚å¿µï¼Œä¼šè¿›è¡Œæ•´å¼çš„åŠ å‡è¿ç®—
            4. ç†è§£ä¸€å…ƒä¸€æ¬¡æ–¹ç¨‹çš„æ¦‚å¿µï¼Œä¼šè§£ç®€å•çš„ä¸€å…ƒä¸€æ¬¡æ–¹ç¨‹
            5. è®¤è¯†åŸºæœ¬çš„å‡ ä½•å›¾å½¢ï¼Œç†è§£ç‚¹ã€çº¿ã€é¢çš„å…³ç³»
            
            å…«å¹´çº§æ•°å­¦å­¦ä¹ è¦æ±‚ï¼š
            1. ç†è§£å®æ•°çš„æ¦‚å¿µï¼Œä¼šè¿›è¡ŒäºŒæ¬¡æ ¹å¼çš„åŒ–ç®€
            2. æŒæ¡ä¸€æ¬¡å‡½æ•°çš„æ¦‚å¿µå’Œæ€§è´¨ï¼Œä¼šç”»ä¸€æ¬¡å‡½æ•°å›¾åƒ
            3. ç†è§£å…¨ç­‰ä¸‰è§’å½¢çš„æ¦‚å¿µï¼ŒæŒæ¡å…¨ç­‰ä¸‰è§’å½¢çš„åˆ¤å®š
            4. èƒ½å¤Ÿè¿›è¡Œç®€å•çš„æ•°æ®æ”¶é›†å’Œæ•´ç†
            
            ä¹å¹´çº§æ•°å­¦å­¦ä¹ è¦æ±‚ï¼š
            1. ç†è§£äºŒæ¬¡å‡½æ•°çš„æ¦‚å¿µï¼ŒæŒæ¡äºŒæ¬¡å‡½æ•°çš„æ€§è´¨
            2. æŒæ¡åœ†çš„åŸºæœ¬æ€§è´¨ï¼Œä¼šè®¡ç®—åœ†çš„å‘¨é•¿å’Œé¢ç§¯
            3. ç†è§£æ¦‚ç‡çš„æ¦‚å¿µï¼Œä¼šè®¡ç®—ç®€å•äº‹ä»¶çš„æ¦‚ç‡
            """,
            
            'è¯­æ–‡è¯¾ç¨‹æ ‡å‡†.pdf': """
            ä¹‰åŠ¡æ•™è‚²è¯­æ–‡è¯¾ç¨‹æ ‡å‡†
            
            æ ¸å¿ƒç´ å…»åŒ…æ‹¬ï¼šè¯­è¨€å»ºæ„ä¸è¿ç”¨ã€æ€ç»´å‘å±•ä¸æå‡ã€å®¡ç¾é‰´èµä¸åˆ›é€ ã€æ–‡åŒ–ä¼ æ‰¿ä¸ç†è§£ã€‚
            
            ä¸ƒå¹´çº§è¯­æ–‡å­¦ä¹ è¦æ±‚ï¼š
            1. èƒ½å¤Ÿæµåˆ©åœ°æœ—è¯»è¯¾æ–‡ï¼Œç†è§£æ–‡ç« çš„åŸºæœ¬å†…å®¹
            2. æŒæ¡å¸¸ç”¨æ±‰å­—çš„ä¹¦å†™ï¼Œä¼šæŸ¥å­—å…¸
            3. ç†è§£è®°å™æ–‡çš„åŸºæœ¬ç‰¹ç‚¹ï¼Œä¼šåˆ†æäººç‰©å½¢è±¡
            4. èƒ½å¤Ÿå†™å‡ºæ¡ç†æ¸…æ¥šçš„è®°å™æ–‡
            5. äº†è§£åŸºæœ¬çš„æ–‡å­¦å¸¸è¯†
            
            å…«å¹´çº§è¯­æ–‡å­¦ä¹ è¦æ±‚ï¼š
            1. èƒ½å¤Ÿé˜…è¯»æµ…æ˜“çš„æ–‡è¨€æ–‡ï¼Œç†è§£åŸºæœ¬å†…å®¹
            2. æŒæ¡è¯´æ˜æ–‡çš„åŸºæœ¬ç‰¹ç‚¹ï¼Œä¼šåˆ†æè¯´æ˜æ–¹æ³•
            3. èƒ½å¤Ÿå†™å‡ºç»“æ„å®Œæ•´çš„è¯´æ˜æ–‡
            4. ç†è§£è¯—æ­Œçš„åŸºæœ¬ç‰¹ç‚¹ï¼Œä¼šèµæç®€å•çš„è¯—æ­Œ
            
            ä¹å¹´çº§è¯­æ–‡å­¦ä¹ è¦æ±‚ï¼š
            1. èƒ½å¤Ÿé˜…è¯»ä¸€èˆ¬çš„æ–‡è¨€æ–‡ï¼Œç†è§£æ–‡è¨€å®è¯å’Œè™šè¯
            2. ç†è§£è®®è®ºæ–‡çš„åŸºæœ¬ç‰¹ç‚¹ï¼Œä¼šåˆ†æè®ºè¯æ–¹æ³•
            3. èƒ½å¤Ÿå†™å‡ºè§‚ç‚¹æ˜ç¡®çš„è®®è®ºæ–‡
            4. æŒæ¡ç»¼åˆæ€§å­¦ä¹ çš„åŸºæœ¬æ–¹æ³•
            """,
            
            'è‹±è¯­è¯¾ç¨‹æ ‡å‡†.pdf': """
            ä¹‰åŠ¡æ•™è‚²è‹±è¯­è¯¾ç¨‹æ ‡å‡†
            
            æ ¸å¿ƒç´ å…»åŒ…æ‹¬ï¼šè¯­è¨€èƒ½åŠ›ã€æ–‡åŒ–æ„è¯†ã€æ€ç»´å“è´¨ã€å­¦ä¹ èƒ½åŠ›ã€‚
            
            ä¸ƒå¹´çº§è‹±è¯­å­¦ä¹ è¦æ±‚ï¼š
            1. æŒæ¡26ä¸ªå­—æ¯çš„è¯»éŸ³å’Œä¹¦å†™
            2. ç†è§£ä¸€èˆ¬ç°åœ¨æ—¶çš„æ¦‚å¿µï¼Œä¼šä½¿ç”¨beåŠ¨è¯
            3. æŒæ¡åŸºæœ¬çš„é—®å€™ç”¨è¯­å’Œæ—¥å¸¸äº¤é™…ç”¨è¯­
            4. èƒ½å¤Ÿè¿›è¡Œç®€å•çš„è‡ªæˆ‘ä»‹ç»
            5. æŒæ¡åŸºæœ¬çš„åè¯å•å¤æ•°å˜åŒ–è§„åˆ™
            
            å…«å¹´çº§è‹±è¯­å­¦ä¹ è¦æ±‚ï¼š
            1. ç†è§£ä¸€èˆ¬è¿‡å»æ—¶çš„æ¦‚å¿µï¼Œä¼šä½¿ç”¨è¿‡å»å¼
            2. æŒæ¡ç°åœ¨è¿›è¡Œæ—¶çš„ç”¨æ³•
            3. èƒ½å¤Ÿæè¿°è¿‡å»å‘ç”Ÿçš„äº‹æƒ…
            4. ç†è§£æ¯”è¾ƒçº§å’Œæœ€é«˜çº§çš„ç”¨æ³•
            
            ä¹å¹´çº§è‹±è¯­å­¦ä¹ è¦æ±‚ï¼š
            1. ç†è§£ç°åœ¨å®Œæˆæ—¶çš„æ¦‚å¿µå’Œç”¨æ³•
            2. æŒæ¡è¢«åŠ¨è¯­æ€çš„åŸºæœ¬å½¢å¼
            3. èƒ½å¤Ÿè¿›è¡Œè¾ƒå¤æ‚çš„è¯­è¨€è¡¨è¾¾
            4. ç†è§£å®šè¯­ä»å¥çš„åŸºæœ¬ç”¨æ³•
            """
        }
        
        for filename, content in sample_documents.items():
            logger.info(f"ğŸ“„ å¤„ç†æ–‡æ¡£: {filename}")
            
            extracted = self.extract_from_text(content, filename)
            
            self.extracted_data['knowledge_points'].extend(extracted['knowledge_points'])
            self.extracted_data['learning_objectives'].extend(extracted['learning_objectives'])
            self.extracted_data['metadata']['files_processed'].append(filename)
            
            logger.info(f"âœ… ä»{filename}æå–: {len(extracted['knowledge_points'])}ä¸ªçŸ¥è¯†ç‚¹")
    
    def _detect_subject(self, text: str, filename: str) -> str:
        """æ£€æµ‹å­¦ç§‘"""
        for subject, keywords in self.subject_keywords.items():
            if any(keyword in text or keyword in filename for keyword in keywords):
                return subject
        return 'é€šç”¨'
    
    def _detect_grade(self, text: str, filename: str) -> str:
        """æ£€æµ‹å¹´çº§"""
        grade_patterns = {
            'Grade 7': ['ä¸ƒå¹´çº§', 'åˆä¸€', '7å¹´çº§'],
            'Grade 8': ['å…«å¹´çº§', 'åˆäºŒ', '8å¹´çº§'],
            'Grade 9': ['ä¹å¹´çº§', 'åˆä¸‰', '9å¹´çº§']
        }
        
        for grade, patterns in grade_patterns.items():
            if any(pattern in text or pattern in filename for pattern in patterns):
                return grade
        return 'Grade 7'  # é»˜è®¤
    
    def _extract_chapter(self, text: str, knowledge_point: str) -> str:
        """æå–ç« èŠ‚ä¿¡æ¯"""
        # åœ¨çŸ¥è¯†ç‚¹é™„è¿‘æŸ¥æ‰¾ç« èŠ‚ä¿¡æ¯
        lines = text.split('\n')
        for i, line in enumerate(lines):
            if knowledge_point in line:
                # å‘ä¸ŠæŸ¥æ‰¾ç« èŠ‚æ ‡é¢˜
                for j in range(max(0, i-5), i):
                    if re.search(r'ç¬¬.*?ç« |ç¬¬.*?å•å…ƒ|Chapter|Unit', lines[j]):
                        return lines[j].strip()
        return 'åŸºç¡€ç« èŠ‚'
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œç¬¦å·
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s()ï¼ˆï¼‰]', '', text)
        return text.strip()
    
    def _is_duplicate(self, text: str, existing_list: List[Dict]) -> bool:
        """æ£€æŸ¥æ˜¯å¦é‡å¤"""
        return any(item['name'] == text for item in existing_list)
    
    def _estimate_difficulty(self, text: str) -> int:
        """ä¼°ç®—éš¾åº¦ç­‰çº§"""
        difficulty_keywords = {
            1: ['äº†è§£', 'è®¤è¯†', 'çŸ¥é“'],
            2: ['ç†è§£', 'æ˜ç¡®', 'æ¸…æ¥š'],
            3: ['æŒæ¡', 'ç†Ÿç»ƒ', 'åº”ç”¨'],
            4: ['åˆ†æ', 'ç»¼åˆ', 'åˆ¤æ–­'],
            5: ['åˆ›æ–°', 'è®¾è®¡', 'è¯„ä»·']
        }
        
        for level, keywords in difficulty_keywords.items():
            if any(keyword in text for keyword in keywords):
                return level
        return 2  # é»˜è®¤éš¾åº¦
    
    def _estimate_importance(self, text: str) -> int:
        """ä¼°ç®—é‡è¦ç¨‹åº¦"""
        importance_keywords = {
            5: ['æ ¸å¿ƒ', 'é‡è¦', 'å…³é”®', 'å¿…é¡»'],
            4: ['æŒæ¡', 'ç†Ÿç»ƒ', 'ä¸»è¦'],
            3: ['ç†è§£', 'æ˜ç¡®', 'åŸºæœ¬'],
            2: ['äº†è§£', 'è®¤è¯†', 'ç®€å•'],
            1: ['çŸ¥é“', 'åˆæ­¥', 'ä¸€èˆ¬']
        }
        
        for level, keywords in importance_keywords.items():
            if any(keyword in text for keyword in keywords):
                return level
        return 3  # é»˜è®¤é‡è¦ç¨‹åº¦
    
    def _generate_keywords(self, text: str, subject: str) -> str:
        """ç”Ÿæˆå…³é”®è¯"""
        keywords = [subject]
        
        # ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯
        important_words = re.findall(r'[\u4e00-\u9fa5]{2,4}', text)
        keywords.extend(important_words[:3])  # å–å‰3ä¸ªä¸­æ–‡è¯
        
        return '|'.join(keywords)
    
    def save_extracted_data(self):
        """ä¿å­˜æå–çš„æ•°æ®"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æ›´æ–°å…ƒæ•°æ®
        self.extracted_data['metadata']['total_extractions'] = len(self.extracted_data['knowledge_points'])
        
        # ä¿å­˜çŸ¥è¯†ç‚¹
        if self.extracted_data['knowledge_points']:
            kp_df = pd.DataFrame(self.extracted_data['knowledge_points'])
            kp_file = os.path.join(self.output_dir, f'knowledge_points_extracted_{timestamp}.csv')
            kp_df.to_csv(kp_file, index=False, encoding='utf-8')
            logger.info(f"ğŸ’¾ çŸ¥è¯†ç‚¹æ•°æ®å·²ä¿å­˜: {kp_file}")
        
        # ä¿å­˜å­¦ä¹ ç›®æ ‡
        if self.extracted_data['learning_objectives']:
            obj_df = pd.DataFrame(self.extracted_data['learning_objectives'])
            obj_file = os.path.join(self.output_dir, f'learning_objectives_{timestamp}.csv')
            obj_df.to_csv(obj_file, index=False, encoding='utf-8')
            logger.info(f"ğŸ’¾ å­¦ä¹ ç›®æ ‡å·²ä¿å­˜: {obj_file}")
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_file = os.path.join(self.output_dir, f'extraction_metadata_{timestamp}.json')
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.extracted_data['metadata'], f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š æå–å…ƒæ•°æ®å·²ä¿å­˜: {metadata_file}")
        
        return len(self.extracted_data['knowledge_points'])
    
    def run_processing(self):
        """è¿è¡Œå®Œæ•´çš„å¤„ç†æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹PDFæ–‡æ¡£å¤„ç†...")
        
        try:
            # å¤„ç†ç¤ºä¾‹æ–‡æ¡£
            self.process_sample_curriculum_standards()
            
            # ä¿å­˜æ•°æ®
            kp_count = self.save_extracted_data()
            
            logger.info("âœ… PDFæ–‡æ¡£å¤„ç†å®Œæˆ!")
            logger.info(f"ğŸ“Š æå–ç»“æœ: {kp_count}ä¸ªçŸ¥è¯†ç‚¹")
            logger.info(f"ğŸ“ æ•°æ®ä¿å­˜åœ¨: {self.output_dir}")
            
            return kp_count
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ“„ å¯åŠ¨PDFæ–‡æ¡£å¤„ç†å™¨...")
    print("ğŸ“š ä¸“é—¨å¤„ç†å®˜æ–¹æ•™è‚²æ–‡æ¡£")
    
    try:
        processor = PDFDocumentProcessor()
        kp_count = processor.run_processing()
        
        print(f"\nğŸ‰ å¤„ç†å®Œæˆ!")
        print(f"ğŸ“ˆ ç»Ÿè®¡:")
        print(f"  - çŸ¥è¯†ç‚¹: {kp_count} ä¸ª")
        print(f"ğŸ“ æ–‡ä»¶ä½ç½®: {processor.output_dir}")
        
        print(f"\nğŸ”„ ä¸‹ä¸€æ­¥:")
        print(f"1. æ£€æŸ¥æå–çš„æ•°æ®è´¨é‡")
        print(f"2. åˆå¹¶åˆ°ä¸»æ•°æ®: python unify_generated_data.py")
        print(f"3. éªŒè¯æ•°æ®: python ../scripts/validate_collected_data.py")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main()
