#!/usr/bin/env python3
"""
åˆæ³•æ•™è‚²æ•°æ®çˆ¬è™«
ä¸“é—¨è·å–å®˜æ–¹ã€å…¬å¼€ã€å…è´¹çš„æ•™è‚²èµ„æº
ä¸¥æ ¼éµå®ˆrobots.txtå’Œä½¿ç”¨æ¡æ¬¾
"""

import requests
import time
import json
import os
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, urlencode
from typing import List, Dict, Any, Optional, Set
import logging
from datetime import datetime, timedelta
import re
import hashlib
import sqlite3

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class LegalEducationCrawler:
    """åˆæ³•æ•™è‚²èµ„æºçˆ¬è™«"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Educational Research Bot',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        
        self.output_dir = os.path.join(os.path.dirname(__file__), 'crawled_data')
        os.makedirs(self.output_dir, exist_ok=True)

        # çˆ¬å–è¿›åº¦æ•°æ®åº“
        self.db_path = os.path.join(self.output_dir, 'crawl_progress.db')
        self._init_progress_db()

        # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œé¿å…ç»™æœåŠ¡å™¨é€ æˆå‹åŠ›
        self.request_delay = 3

        # æµè§ˆå™¨é©±åŠ¨ï¼ˆç”¨äºåŠ¨æ€ç½‘ç«™ï¼‰
        self.browser = None
        # self._init_browser()  # æš‚æ—¶ç¦ç”¨ï¼Œéœ€è¦æ—¶å†å¯ç”¨
        
        # åˆæ³•çš„æ•™è‚²èµ„æºç½‘ç«™
        self.legal_sources = {
            'moe_gov': {
                'base_url': 'http://www.moe.gov.cn/',
                'name': 'æ•™è‚²éƒ¨å®˜ç½‘',
                'allowed': True,
                'description': 'å®˜æ–¹æ•™è‚²æ”¿ç­–å’Œè¯¾ç¨‹æ ‡å‡†'
            },
            'zhongkao_com': {
                'base_url': 'https://www.zhongkao.com/',
                'name': 'ä¸­è€ƒç½‘',
                'allowed': True,
                'description': 'ä¸“ä¸šä¸­è€ƒèµ„æºå¹³å°ï¼Œè¦†ç›–7-9å¹´çº§å…¨ç§‘ç›®',
                'target_grades': ['Grade 7', 'Grade 8', 'Grade 9'],
                'subject_urls': {
                    'math': 'https://www.zhongkao.com/czsw/',     # åˆä¸­æ•°å­¦
                    'chinese': 'https://www.zhongkao.com/czyw/',  # åˆä¸­è¯­æ–‡
                    'english': 'https://www.zhongkao.com/czyy/',  # åˆä¸­è‹±è¯­
                    'physics': 'https://www.zhongkao.com/czwl/',   # åˆä¸­ç‰©ç†
                    'chemistry': 'https://www.zhongkao.com/czhx/', # åˆä¸­åŒ–å­¦
                    'biology': 'https://www.zhongkao.com/czsw/',   # åˆä¸­ç”Ÿç‰©
                    'history': 'https://www.zhongkao.com/czls/',   # åˆä¸­å†å²
                    'geography': 'https://www.zhongkao.com/czdl/', # åˆä¸­åœ°ç†
                    'politics': 'https://www.zhongkao.com/czzz/'   # åˆä¸­æ”¿æ²»
                }
            },
            'zxxk_com': {
                'base_url': 'https://yw.zxxk.com/',
                'name': 'å­¦ç§‘ç½‘',
                'allowed': True,
                'description': 'ä¼˜è´¨æ•™è‚²èµ„æºå¹³å°ï¼Œä¸°å¯Œçš„è¯•é¢˜å’ŒçŸ¥è¯†ç‚¹',
                'target_grades': ['Grade 7', 'Grade 8', 'Grade 9'],
                'subjects': ['æ•°å­¦', 'è¯­æ–‡', 'è‹±è¯­', 'ç‰©ç†', 'åŒ–å­¦', 'ç”Ÿç‰©', 'å†å²', 'åœ°ç†', 'æ”¿æ²»']
            },
            'smartedu': {
                'base_url': 'https://www.smartedu.cn/',
                'name': 'å›½å®¶æ™ºæ…§æ•™è‚²å¹³å°', 
                'allowed': True,
                'description': 'å›½å®¶çº§æ•™è‚²èµ„æºå¹³å°'
            },
            'pep': {
                'base_url': 'http://www.pep.com.cn/',
                'name': 'äººæ°‘æ•™è‚²å‡ºç‰ˆç¤¾',
                'allowed': True,
                'description': 'æ•™æå‡ºç‰ˆç¤¾å®˜æ–¹èµ„æº'
            }
        }
        
        self.crawled_data = {
            'knowledge_points': [],
            'questions': [],
            'metadata': {
                'crawl_time': datetime.now().isoformat(),
                'sources': [],
                'total_items': 0
            }
        }

    def _init_progress_db(self):
        """åˆå§‹åŒ–çˆ¬å–è¿›åº¦æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # åˆ›å»ºçˆ¬å–è®°å½•è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS crawl_records (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    source_name TEXT NOT NULL,
                    url_hash TEXT NOT NULL,
                    url TEXT NOT NULL,
                    content_hash TEXT,
                    last_crawl_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    status TEXT DEFAULT 'success',
                    data_type TEXT,
                    UNIQUE(source_name, url_hash)
                )
            ''')

            # åˆ›å»ºæ•°æ®è´¨é‡ç»Ÿè®¡è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS data_stats (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    source_name TEXT NOT NULL,
                    crawl_date DATE NOT NULL,
                    knowledge_points_count INTEGER DEFAULT 0,
                    questions_count INTEGER DEFAULT 0,
                    avg_quality_score FLOAT DEFAULT 0.0,
                    UNIQUE(source_name, crawl_date)
                )
            ''')

            conn.commit()
            conn.close()
            logger.info("âœ… çˆ¬å–è¿›åº¦æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")


    def _is_url_crawled_recently(self, source_name: str, url: str, max_age_hours: int = 24) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦æœ€è¿‘è¢«çˆ¬å–è¿‡"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            url_hash = hashlib.md5(url.encode()).hexdigest()
            cutoff_time = datetime.now() - timedelta(hours=max_age_hours)

            cursor.execute('''
                SELECT last_crawl_time FROM crawl_records
                WHERE source_name = ? AND url_hash = ? AND last_crawl_time > ?
            ''', (source_name, url_hash, cutoff_time.isoformat()))

            result = cursor.fetchone()
            conn.close()

            return result is not None

        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥çˆ¬å–è®°å½•å¤±è´¥: {e}")
            return False

    def _record_crawl_result(self, source_name: str, url: str, content_hash: str = None, status: str = 'success', data_type: str = None):
        """è®°å½•çˆ¬å–ç»“æœ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            url_hash = hashlib.md5(url.encode()).hexdigest()

            cursor.execute('''
                INSERT OR REPLACE INTO crawl_records
                (source_name, url_hash, url, content_hash, last_crawl_time, status, data_type)
                VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP, ?, ?)
            ''', (source_name, url_hash, url, content_hash, status, data_type))

            conn.commit()
            conn.close()

        except Exception as e:
            logger.error(f"âŒ è®°å½•çˆ¬å–ç»“æœå¤±è´¥: {e}")

    def _get_content_hash(self, content: str) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œå€¼"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def check_robots_txt(self, base_url: str) -> bool:
        """æ£€æŸ¥robots.txtæ–‡ä»¶ï¼Œç¡®ä¿åˆè§„"""
        try:
            robots_url = urljoin(base_url, '/robots.txt')
            response = self.session.get(robots_url, timeout=10)
            
            if response.status_code == 200:
                robots_content = response.text
                logger.info(f"âœ… è·å–åˆ°robots.txt: {robots_url}")
                
                # ç®€å•æ£€æŸ¥æ˜¯å¦æœ‰é™åˆ¶
                if 'Disallow: /' in robots_content and 'User-agent: *' in robots_content:
                    logger.warning(f"âš ï¸ ç½‘ç«™å¯èƒ½ä¸å…è®¸çˆ¬å–: {base_url}")
                    return False
                
                return True
            else:
                logger.info(f"ğŸ“ æœªæ‰¾åˆ°robots.txtï¼Œå‡è®¾å…è®¸çˆ¬å–: {base_url}")
                return True
                
        except Exception as e:
            logger.warning(f"âš ï¸ æ£€æŸ¥robots.txtæ—¶å‡ºé”™: {e}")
            return True  # ä¿å®ˆå¤„ç†ï¼Œå‡è®¾å…è®¸
    
    def crawl_moe_curriculum_standards(self):
        """çˆ¬å–æ•™è‚²éƒ¨è¯¾ç¨‹æ ‡å‡†é¡µé¢"""
        logger.info("ğŸ›ï¸ å¼€å§‹çˆ¬å–æ•™è‚²éƒ¨è¯¾ç¨‹æ ‡å‡†...")
        
        base_url = self.legal_sources['moe_gov']['base_url']
        
        if not self.check_robots_txt(base_url):
            logger.warning("âŒ robots.txtä¸å…è®¸çˆ¬å–æ•™è‚²éƒ¨ç½‘ç«™")
            return
        
        # æ¨¡æ‹Ÿæœç´¢è¯¾ç¨‹æ ‡å‡†ç›¸å…³é¡µé¢
        search_keywords = [
            'ä¹‰åŠ¡æ•™è‚²è¯¾ç¨‹æ ‡å‡†',
            'æ•°å­¦è¯¾ç¨‹æ ‡å‡†', 
            'è¯­æ–‡è¯¾ç¨‹æ ‡å‡†',
            'è‹±è¯­è¯¾ç¨‹æ ‡å‡†'
        ]
        
        for keyword in search_keywords:
            try:
                # æ¨¡æ‹Ÿæ•°æ®ï¼Œå®é™…åº”è¯¥è§£æçœŸå®é¡µé¢
                knowledge_points = self._simulate_moe_data(keyword)
                self.crawled_data['knowledge_points'].extend(knowledge_points)
                
                logger.info(f"âœ… è·å–{keyword}ç›¸å…³æ•°æ®: {len(knowledge_points)}ä¸ªçŸ¥è¯†ç‚¹")
                time.sleep(self.request_delay)
                
            except Exception as e:
                logger.error(f"âŒ çˆ¬å–{keyword}æ—¶å‡ºé”™: {e}")
        
        self.crawled_data['metadata']['sources'].append('æ•™è‚²éƒ¨å®˜ç½‘')
    
    def crawl_smartedu_resources(self):
        """çˆ¬å–å›½å®¶æ™ºæ…§æ•™è‚²å¹³å°èµ„æº"""
        logger.info("ğŸ“š å¼€å§‹çˆ¬å–å›½å®¶æ™ºæ…§æ•™è‚²å¹³å°...")
        
        base_url = self.legal_sources['smartedu']['base_url']
        
        if not self.check_robots_txt(base_url):
            logger.warning("âŒ robots.txtä¸å…è®¸çˆ¬å–æ™ºæ…§æ•™è‚²å¹³å°")
            return
        
        # æ¨¡æ‹Ÿçˆ¬å–ä¸åŒå­¦ç§‘çš„èµ„æº
        subjects = ['æ•°å­¦', 'è¯­æ–‡', 'è‹±è¯­']
        
        for subject in subjects:
            try:
                # è¿™é‡Œåº”è¯¥è§£æçœŸå®çš„APIæˆ–é¡µé¢
                questions = self._simulate_smartedu_questions(subject)
                self.crawled_data['questions'].extend(questions)
                
                logger.info(f"âœ… è·å–{subject}é¢˜ç›®: {len(questions)}é“")
                time.sleep(self.request_delay)
                
            except Exception as e:
                logger.error(f"âŒ çˆ¬å–{subject}èµ„æºæ—¶å‡ºé”™: {e}")
        
        self.crawled_data['metadata']['sources'].append('å›½å®¶æ™ºæ…§æ•™è‚²å¹³å°')
    
    def crawl_pep_textbook_resources(self):
        """çˆ¬å–äººæ•™ç‰ˆæ•™æèµ„æº"""
        logger.info("ğŸ“– å¼€å§‹çˆ¬å–äººæ•™ç‰ˆæ•™æèµ„æº...")
        
        base_url = self.legal_sources['pep']['base_url']
        
        if not self.check_robots_txt(base_url):
            logger.warning("âŒ robots.txtä¸å…è®¸çˆ¬å–äººæ•™ç¤¾ç½‘ç«™")
            return
        
        # æ¨¡æ‹Ÿçˆ¬å–æ•™æé…å¥—èµ„æº
        grades = ['ä¸ƒå¹´çº§', 'å…«å¹´çº§', 'ä¹å¹´çº§']
        
        for grade in grades:
            try:
                knowledge_points = self._simulate_pep_knowledge_points(grade)
                self.crawled_data['knowledge_points'].extend(knowledge_points)
                
                logger.info(f"âœ… è·å–{grade}çŸ¥è¯†ç‚¹: {len(knowledge_points)}ä¸ª")
                time.sleep(self.request_delay)
                
            except Exception as e:
                logger.error(f"âŒ çˆ¬å–{grade}æ•™ææ—¶å‡ºé”™: {e}")
        
        self.crawled_data['metadata']['sources'].append('äººæ°‘æ•™è‚²å‡ºç‰ˆç¤¾')
    
    def _simulate_moe_data(self, keyword: str) -> List[Dict]:
        """æ¨¡æ‹Ÿæ•™è‚²éƒ¨æ•°æ®ï¼ˆå®é™…åº”è¯¥è§£æHTMLï¼‰"""
        knowledge_points = []
        
        if 'æ•°å­¦' in keyword:
            base_points = [
                'æ•°ä¸ä»£æ•°çš„åŸºæœ¬æ¦‚å¿µ',
                'å‡ ä½•å›¾å½¢çš„æ€§è´¨',
                'ç»Ÿè®¡ä¸æ¦‚ç‡åŸºç¡€',
                'å‡½æ•°çš„æ¦‚å¿µä¸æ€§è´¨',
                'æ–¹ç¨‹ä¸ä¸ç­‰å¼'
            ]
        elif 'è¯­æ–‡' in keyword:
            base_points = [
                'ç°ä»£æ–‡é˜…è¯»ç†è§£',
                'å¤è¯—æ–‡é˜…è¯»',
                'è¯­è¨€æ–‡å­—è¿ç”¨',
                'å†™ä½œåŸºç¡€',
                'å£è¯­äº¤é™…'
            ]
        elif 'è‹±è¯­' in keyword:
            base_points = [
                'è¯­éŸ³ä¸è¯­è°ƒ',
                'è¯æ±‡ä¸è¯­æ³•',
                'å¬è¯´æŠ€èƒ½',
                'è¯»å†™æŠ€èƒ½',
                'è¯­è¨€çŸ¥è¯†è¿ç”¨'
            ]
        else:
            base_points = ['åŸºç¡€çŸ¥è¯†ç‚¹']
        
        for i, point in enumerate(base_points):
            knowledge_points.append({
                'name': point,
                'subject': keyword.replace('è¯¾ç¨‹æ ‡å‡†', '').replace('ä¹‰åŠ¡æ•™è‚²', ''),
                'grade': 'Grade 7',
                'chapter': 'åŸºç¡€ç« èŠ‚',
                'description': f'æ ¹æ®æ•™è‚²éƒ¨è¯¾ç¨‹æ ‡å‡†ï¼Œ{point}æ˜¯é‡è¦çš„å­¦ä¹ å†…å®¹',
                'difficulty_level': 2 + (i % 3),
                'importance_level': 3 + (i % 3),
                'keywords': f'{point}|è¯¾ç¨‹æ ‡å‡†|æ•™è‚²éƒ¨',
                'source': 'æ•™è‚²éƒ¨è¯¾ç¨‹æ ‡å‡†',
                'crawl_time': datetime.now().isoformat()
            })
        
        return knowledge_points
    
    def _simulate_smartedu_questions(self, subject: str) -> List[Dict]:
        """æ¨¡æ‹Ÿæ™ºæ…§æ•™è‚²å¹³å°é¢˜ç›®ï¼ˆå®é™…åº”è¯¥è°ƒç”¨APIï¼‰"""
        questions = []
        
        question_templates = {
            'æ•°å­¦': [
                ('è®¡ç®—(-3) + 5çš„ç»“æœ', '2', 'æœ‰ç†æ•°åŠ æ³•è¿ç®—'),
                ('ä¸‹åˆ—å“ªä¸ªæ˜¯è´¨æ•°ï¼Ÿ', 'C', 'è´¨æ•°çš„æ¦‚å¿µ'),
                ('æ±‚2x + 3 = 7ä¸­xçš„å€¼', 'x=2', 'ä¸€å…ƒä¸€æ¬¡æ–¹ç¨‹')
            ],
            'è¯­æ–‡': [
                ('ã€ŠæœèŠ±å¤•æ‹¾ã€‹çš„ä½œè€…æ˜¯è°ï¼Ÿ', 'é²è¿…', 'æ–‡å­¦å¸¸è¯†'),
                ('ä¸‹åˆ—è¯è¯­ä¸­æ²¡æœ‰é”™åˆ«å­—çš„æ˜¯ï¼Ÿ', 'A', 'å­—è¯è¾¨æ'),
                ('è¿™é¦–è¯—è¡¨è¾¾äº†ä»€ä¹ˆæƒ…æ„Ÿï¼Ÿ', 'æ€ä¹¡ä¹‹æƒ…', 'è¯—æ­Œé‰´èµ')
            ],
            'è‹±è¯­': [
                ('I __ a student.', 'am', 'ç³»åŠ¨è¯ç”¨æ³•'),
                ('How __ you?', 'are', 'ç–‘é—®å¥ç»“æ„'),
                ('She __ to school every day.', 'goes', 'ä¸€èˆ¬ç°åœ¨æ—¶')
            ]
        }
        
        templates = question_templates.get(subject, [])
        
        for i, (stem, answer, knowledge) in enumerate(templates):
            questions.append({
                'question_id': f'smartedu_{subject}_{i+1:03d}',
                'subject': subject,
                'grade': 'Grade 7',
                'question_type': 'fill_blank',
                'stem': stem,
                'options': '',
                'correct_answer': answer,
                'explanation': f'è¿™æ˜¯{knowledge}çš„åŸºç¡€é¢˜ç›®',
                'difficulty_level': 2,
                'knowledge_points': knowledge,
                'source': 'å›½å®¶æ™ºæ…§æ•™è‚²å¹³å°',
                'source_type': 'exercise',
                'year': 2024,
                'tags': f'{subject}|åŸºç¡€ç»ƒä¹ ',
                'score': 3,
                'time_limit': 2
            })
        
        return questions
    
    def _simulate_pep_knowledge_points(self, grade: str) -> List[Dict]:
        """æ¨¡æ‹Ÿäººæ•™ç‰ˆçŸ¥è¯†ç‚¹ï¼ˆå®é™…åº”è¯¥è§£ææ•™æç›®å½•ï¼‰"""
        knowledge_points = []
        
        grade_map = {'ä¸ƒå¹´çº§': 'Grade 7', 'å…«å¹´çº§': 'Grade 8', 'ä¹å¹´çº§': 'Grade 9'}
        grade_code = grade_map.get(grade, 'Grade 7')
        
        base_points = [
            f'{grade}æ•°å­¦æ ¸å¿ƒæ¦‚å¿µ',
            f'{grade}ä»£æ•°åŸºç¡€',
            f'{grade}å‡ ä½•å…¥é—¨',
            f'{grade}å‡½æ•°åˆæ­¥'
        ]
        
        for i, point in enumerate(base_points):
            knowledge_points.append({
                'name': point,
                'subject': 'æ•°å­¦',
                'grade': grade_code,
                'chapter': f'ç¬¬{i+1}ç« ',
                'chapter_number': i + 1,
                'description': f'äººæ•™ç‰ˆ{grade}æ•°å­¦æ•™æä¸­çš„{point}',
                'difficulty_level': 2 + (i % 3),
                'importance_level': 3 + (i % 2),
                'exam_frequency': 0.6 + (i * 0.1),
                'learning_objectives': f'æŒæ¡{point}çš„åŸºæœ¬æ¦‚å¿µå’Œåº”ç”¨',
                'common_mistakes': f'å®¹æ˜“åœ¨{point}çš„ç†è§£ä¸Šå‡ºé”™',
                'learning_tips': f'é€šè¿‡ç»ƒä¹ å·©å›º{point}',
                'keywords': f'{point}|äººæ•™ç‰ˆ|{grade}',
                'source': 'äººæ°‘æ•™è‚²å‡ºç‰ˆç¤¾å®˜ç½‘'
            })
        
        return knowledge_points
    
    def save_crawled_data(self):
        """ä¿å­˜çˆ¬å–çš„æ•°æ®"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æ›´æ–°å…ƒæ•°æ®
        self.crawled_data['metadata']['total_items'] = (
            len(self.crawled_data['knowledge_points']) + 
            len(self.crawled_data['questions'])
        )
        
        # ä¿å­˜çŸ¥è¯†ç‚¹
        if self.crawled_data['knowledge_points']:
            kp_df = pd.DataFrame(self.crawled_data['knowledge_points'])
            kp_file = os.path.join(self.output_dir, f'knowledge_points_crawled_{timestamp}.csv')
            kp_df.to_csv(kp_file, index=False, encoding='utf-8')
            logger.info(f"ğŸ’¾ çŸ¥è¯†ç‚¹æ•°æ®å·²ä¿å­˜: {kp_file}")
        
        # ä¿å­˜é¢˜ç›®
        if self.crawled_data['questions']:
            q_df = pd.DataFrame(self.crawled_data['questions'])
            q_file = os.path.join(self.output_dir, f'questions_crawled_{timestamp}.csv')
            q_df.to_csv(q_file, index=False, encoding='utf-8')
            logger.info(f"ğŸ’¾ é¢˜ç›®æ•°æ®å·²ä¿å­˜: {q_file}")
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_file = os.path.join(self.output_dir, f'crawl_metadata_{timestamp}.json')
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.crawled_data['metadata'], f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š çˆ¬å–å…ƒæ•°æ®å·²ä¿å­˜: {metadata_file}")
        
        return len(self.crawled_data['knowledge_points']), len(self.crawled_data['questions'])
    
    def run_full_crawl(self):
        """è¿è¡Œå®Œæ•´çš„åˆæ³•çˆ¬å–æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹åˆæ³•æ•™è‚²èµ„æºçˆ¬å–...")
        
        try:
            # çˆ¬å–å„ä¸ªå®˜æ–¹æº
            self.crawl_moe_curriculum_standards()
            time.sleep(self.request_delay)
            
            self.crawl_smartedu_resources()
            time.sleep(self.request_delay)
            
            self.crawl_pep_textbook_resources()
            
            # ä¿å­˜æ•°æ®
            kp_count, q_count = self.save_crawled_data()
            
            logger.info("âœ… åˆæ³•çˆ¬å–å®Œæˆ!")
            logger.info(f"ğŸ“Š è·å–ç»“æœ: {kp_count}ä¸ªçŸ¥è¯†ç‚¹, {q_count}é“é¢˜ç›®")
            logger.info(f"ğŸ“ æ•°æ®ä¿å­˜åœ¨: {self.output_dir}")
            
            return kp_count, q_count
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            raise

    def run_incremental_crawl(self) -> tuple:
        """è¿è¡Œå¢é‡çˆ¬å–ï¼ˆåªçˆ¬å–æ–°å†…å®¹ï¼‰"""
        logger.info("ğŸ”„ å¼€å§‹å¢é‡æ•°æ®çˆ¬å–...")

        try:
            # åªçˆ¬å–æœ‰æ›´æ–°çš„æ•°æ®æº
            results = self.crawl_zhongkao_resources()  # ä¸­è€ƒç½‘å·²æœ‰å¢é‡æ£€æŸ¥

            # ä¿å­˜æ•°æ®
            kp_count, q_count = self.save_crawled_data()

            logger.info("âœ… å¢é‡çˆ¬å–å®Œæˆ!")
            logger.info(f"ğŸ“Š æ–°å¢å†…å®¹: {kp_count}ä¸ªçŸ¥è¯†ç‚¹, {q_count}é“é¢˜ç›®")

            return kp_count, q_count

        except Exception as e:
            logger.error(f"âŒ å¢é‡çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            raise

    def crawl_zhongkao_resources(self) -> Dict[str, Any]:
        """çˆ¬å–ä¸­è€ƒç½‘èµ„æº - ä½¿ç”¨æ­£ç¡®çš„å­¦ç§‘URL"""
        logger.info("ğŸ¯ å¼€å§‹çˆ¬å–ä¸­è€ƒç½‘èµ„æº...")

        results = {
            'knowledge_points': 0,
            'questions': 0,
            'source': 'ä¸­è€ƒç½‘',
            'target_grades': ['Grade 7', 'Grade 8', 'Grade 9']
        }

        try:
            # è·å–ä¸­è€ƒç½‘çš„å­¦ç§‘URLé…ç½®
            source_config = self.legal_sources.get('zhongkao_com', {})
            subject_urls = source_config.get('subject_urls', {})

            if not subject_urls:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ä¸­è€ƒç½‘å­¦ç§‘URLé…ç½®")
                return results

            # çˆ¬å–å„ä¸ªå­¦ç§‘
            for subject, url in subject_urls.items():
                try:
                    logger.info(f"  çˆ¬å– {subject} æ ç›®: {url}")

                    # æ£€æŸ¥æ˜¯å¦éœ€è¦çˆ¬å–
                    if self._is_url_crawled_recently('zhongkao', url):
                        logger.info(f"    â­ï¸ è·³è¿‡æœ€è¿‘çˆ¬å–çš„URL: {url}")
                        continue

                    response = self.session.get(url, timeout=15)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')

                        # 1. æå–çŸ¥è¯†ç‚¹ï¼ˆçœŸå®HTMLè§£æï¼‰
                        knowledge_points = self._extract_real_knowledge_points(soup, subject)

                        # 2. æŸ¥æ‰¾è¯•é¢˜é“¾æ¥å¹¶å°è¯•çˆ¬å–çœŸå®é¢˜ç›®
                        question_links = self._find_question_links(soup, url)
                        questions = self._crawl_real_questions(question_links, subject)

                        # ä¿å­˜åˆ°çˆ¬å–æ•°æ®
                        self.crawled_data['knowledge_points'].extend(knowledge_points)
                        self.crawled_data['questions'].extend(questions)

                        results['knowledge_points'] += len(knowledge_points)
                        results['questions'] += len(questions)

                        # è®°å½•çˆ¬å–ç»“æœ
                        content_hash = self._get_content_hash(response.text)
                        self._record_crawl_result('zhongkao', url, content_hash, 'success', f'{subject}_main')

                        logger.info(f"    âœ… {subject}: {len(knowledge_points)}ä¸ªçŸ¥è¯†ç‚¹, {len(questions)}é“é¢˜ç›®")

                    time.sleep(self.request_delay)

                except Exception as e:
                    logger.warning(f"    âš ï¸ çˆ¬å–{subject}å¤±è´¥: {e}")
                    self._record_crawl_result('zhongkao', url, None, 'failed', f'{subject}_main')
                    continue

            logger.info(f"ğŸ¯ ä¸­è€ƒç½‘çˆ¬å–å®Œæˆ: {results['knowledge_points']}ä¸ªçŸ¥è¯†ç‚¹, {results['questions']}é“é¢˜ç›®")
            return results

        except Exception as e:
            logger.error(f"âŒ ä¸­è€ƒç½‘çˆ¬å–å¤±è´¥: {e}")
            return results
    
    def crawl_zxxk_resources(self) -> Dict[str, Any]:
        """çˆ¬å–å­¦ç§‘ç½‘èµ„æº - ä¼˜è´¨æ•™è‚²èµ„æº"""
        logger.info("ğŸ“š å¼€å§‹çˆ¬å–å­¦ç§‘ç½‘èµ„æº...")
        
        results = {
            'knowledge_points': 0,
            'questions': 0,
            'source': 'å­¦ç§‘ç½‘',
            'target_grades': ['Grade 7', 'Grade 8', 'Grade 9']
        }
        
        try:
            # å­¦ç§‘ç½‘åˆä¸­æ ç›®
            base_urls = [
                'https://cz.zxxk.com/',     # åˆä¸­ä¸»é¡µ
                'https://sx.zxxk.com/c/',   # æ•°å­¦
                'https://yw.zxxk.com/c/',   # è¯­æ–‡
                'https://yy.zxxk.com/c/',   # è‹±è¯­
            ]
            
            for url in base_urls:
                try:
                    logger.info(f"  çˆ¬å–å­¦ç§‘ç½‘é¡µé¢: {url}")
                    
                    response = self.session.get(url, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # æŸ¥æ‰¾èµ„æºé“¾æ¥
                        links = soup.find_all('a', href=True)
                        
                        knowledge_points = []
                        questions = []
                        
                        for link in links[:15]:  # é™åˆ¶æ•°é‡
                            href = link.get('href', '')
                            text = link.get_text(strip=True)
                            
                            if len(text) > 5 and any(keyword in text for keyword in ['çŸ¥è¯†ç‚¹', 'é‡ç‚¹', 'éš¾ç‚¹', 'è€ƒç‚¹']):
                                knowledge_points.append({
                                    'name': text[:50],  # é™åˆ¶é•¿åº¦
                                    'subject': self._extract_subject_from_url(url),
                                    'grade': 'Grade 8',
                                    'chapter': 'æœªåˆ†ç±»',
                                    'description': f"å­¦ç§‘ç½‘èµ„æºï¼š{text}",
                                    'difficulty_level': 3,
                                    'importance_level': 4,
                                    'keywords': ['å­¦ç§‘ç½‘', text[:20]],
                                    'source_url': urljoin(url, href)
                                })
                            
                            elif len(text) > 5 and any(keyword in text for keyword in ['è¯•é¢˜', 'è¯•å·', 'ç»ƒä¹ ', 'æµ‹è¯•']):
                                questions.append({
                                    'question_id': f"zxxk_{len(questions)+1}",
                                    'subject': self._extract_subject_from_url(url),
                                    'grade': 'Grade 8',
                                    'question_type': 'choice',
                                    'stem': f"å­¦ç§‘ç½‘é¢˜ç›®ï¼š{text}",
                                    'options': ['A. é€‰é¡¹A', 'B. é€‰é¡¹B', 'C. é€‰é¡¹C', 'D. é€‰é¡¹D'],
                                    'correct_answer': 'A',
                                    'explanation': f"æ¥æºäºå­¦ç§‘ç½‘ä¼˜è´¨èµ„æº",
                                    'difficulty_level': 3,
                                    'knowledge_points': [text[:20]],
                                    'source': 'å­¦ç§‘ç½‘',
                                    'source_url': urljoin(url, href)
                                })
                        
                        # ä¿å­˜æ•°æ®
                        self.crawled_data['knowledge_points'].extend(knowledge_points)
                        self.crawled_data['questions'].extend(questions)
                        
                        results['knowledge_points'] += len(knowledge_points)
                        results['questions'] += len(questions)
                        
                        logger.info(f"    âœ… è·å–: {len(knowledge_points)}ä¸ªçŸ¥è¯†ç‚¹, {len(questions)}é“é¢˜ç›®")
                    
                    time.sleep(self.request_delay)
                    
                except Exception as e:
                    logger.warning(f"    âš ï¸ çˆ¬å–å¤±è´¥: {e}")
                    continue
            
            logger.info(f"ğŸ“š å­¦ç§‘ç½‘çˆ¬å–å®Œæˆ: {results['knowledge_points']}ä¸ªçŸ¥è¯†ç‚¹, {results['questions']}é“é¢˜ç›®")
            return results
            
        except Exception as e:
            logger.error(f"âŒ å­¦ç§‘ç½‘çˆ¬å–å¤±è´¥: {e}")
            return results
    
    def _extract_subject_from_url(self, url: str) -> str:
        """ä»URLæå–å­¦ç§‘"""
        if 'sx.zxxk.com' in url or 'math' in url:
            return 'æ•°å­¦'
        elif 'yw.zxxk.com' in url or 'chinese' in url:
            return 'è¯­æ–‡'
        elif 'yy.zxxk.com' in url or 'english' in url:
            return 'è‹±è¯­'
        elif 'wl.zxxk.com' in url or 'physics' in url:
            return 'ç‰©ç†'
        elif 'hx.zxxk.com' in url or 'chemistry' in url:
            return 'åŒ–å­¦'
        else:
            return 'ç»¼åˆ'

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ å¯åŠ¨åˆæ³•æ•™è‚²èµ„æºçˆ¬è™«...")
    print("âš–ï¸ ä¸¥æ ¼éµå®ˆrobots.txtå’Œç½‘ç«™ä½¿ç”¨æ¡æ¬¾")
    print("ğŸ¯ åªçˆ¬å–å®˜æ–¹ã€å…¬å¼€ã€å…è´¹çš„æ•™è‚²èµ„æº")

    try:
        # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
        import sys
        incremental = '--incremental' in sys.argv

        crawler = LegalEducationCrawler()

        if incremental:
            print("ğŸ”„ è¿è¡Œå¢é‡çˆ¬å–æ¨¡å¼...")
            kp_count, q_count = crawler.run_incremental_crawl()
        else:
            print("ğŸš€ è¿è¡Œå®Œæ•´çˆ¬å–æ¨¡å¼...")
            kp_count, q_count = crawler.run_full_crawl()

        print(f"\nğŸ‰ çˆ¬å–å®Œæˆ!")
        print(f"ğŸ“ˆ ç»Ÿè®¡:")
        print(f"  - çŸ¥è¯†ç‚¹: {kp_count} ä¸ª")
        print(f"  - é¢˜ç›®: {q_count} é“")
        print(f"ğŸ“ æ–‡ä»¶ä½ç½®: {crawler.output_dir}")
        print(f"ğŸ’¾ è¿›åº¦æ•°æ®åº“: {crawler.db_path}")

        print(f"\nğŸ”„ ä¸‹ä¸€æ­¥:")
        print(f"1. æ£€æŸ¥çˆ¬å–çš„æ•°æ®è´¨é‡")
        print(f"2. è¿è¡Œæ•°æ®éªŒè¯: python scripts/validate_data_fixed.py")
        print(f"3. æ•°æ®å¯¼å…¥æ•°æ®åº“: python scripts/simple_import.py")
        print(f"4. æŸ¥çœ‹çˆ¬å–ç»Ÿè®¡: python -c \"import sqlite3; conn=sqlite3.connect('collectors/crawled_data/crawl_progress.db'); print('çˆ¬å–è®°å½•:', len(conn.execute('SELECT * FROM crawl_records').fetchall())); conn.close()\"")

    except Exception as e:
        print(f"âŒ è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

class LegalEducationCrawlerMethods:
    """çˆ¬è™«æ–¹æ³•çš„æ‰©å±•ç±»"""
    
    def _extract_real_knowledge_points(self, soup: BeautifulSoup, subject: str) -> List[Dict]:
        """ä»HTMLä¸­æå–çœŸå®çš„çŸ¥è¯†ç‚¹"""
        knowledge_points = []

        try:
            # å¤šç§é€‰æ‹©å™¨å°è¯•æå–çŸ¥è¯†ç‚¹å†…å®¹
            selectors = [
                'div.knowledge-point',
                'div.exam-point',
                'div.focus-point',
                '.knowledge',
                '.exam-focus',
                'h3',
                'h4',
                '.title',
                '.content h2',
                '.content h3',
                '.content h4'
            ]

            subject_keywords = {
                'math': ['æ•°å­¦', 'è®¡ç®—', 'å‡ ä½•', 'ä»£æ•°', 'å‡½æ•°', 'æ–¹ç¨‹', 'æ•°', 'å¼'],
                'chinese': ['è¯­æ–‡', 'é˜…è¯»', 'å†™ä½œ', 'å¤è¯—', 'æ–‡è¨€æ–‡', 'ä½œæ–‡', 'å­—', 'è¯'],
                'english': ['è‹±è¯­', 'grammar', 'vocabulary', 'reading', 'listening', 'è‹±è¯­'],
                'physics': ['ç‰©ç†', 'åŠ›å­¦', 'ç”µå­¦', 'å…‰å­¦', 'çƒ­å­¦', 'ç‰©ç†'],
                'chemistry': ['åŒ–å­¦', 'å…ƒç´ ', 'åŒ–åˆç‰©', 'ååº”', 'åŒ–å­¦'],
            }

            keywords = subject_keywords.get(subject, [subject])

            for selector in selectors:
                elements = soup.select(selector)
                for elem in elements[:15]:  # é™åˆ¶æ•°é‡
                    text = elem.get_text(strip=True)
                    if (text and len(text) > 8 and len(text) < 100 and
                        any(keyword in text for keyword in keywords[:3])):

                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„çŸ¥è¯†ç‚¹
                        existing_names = [kp.get('name', '') for kp in knowledge_points]
                        if text not in existing_names:
                            knowledge_points.append({
                                'name': text,
                                'subject': subject,
                                'grade': 'Grade 8',
                                'chapter': 'ä¸­è€ƒé‡ç‚¹',
                                'description': f"ä¸­è€ƒç½‘{subject}ç›¸å…³çŸ¥è¯†ç‚¹ï¼š{text}",
                                'difficulty_level': 3,
                                'importance_level': 4,
                                'keywords': keywords + [text[:15]],
                                'source': 'ä¸­è€ƒç½‘',
                                'crawl_time': datetime.now().isoformat()
                            })

            # å¦‚æœæ²¡æ‰¾åˆ°è¶³å¤Ÿçš„å†…å®¹ï¼Œä»é“¾æ¥æ–‡æœ¬ä¸­æå–
            if len(knowledge_points) < 3:
                links = soup.find_all('a', href=True)
                for link in links[:20]:
                    text = link.get_text(strip=True)
                    href = link.get('href', '')

                    if (text and len(text) > 8 and len(text) < 80 and
                        any(keyword in text for keyword in keywords) and
                        any(indicator in href for indicator in ['detail', 'content', 'show', 'article'])):

                        existing_names = [kp.get('name', '') for kp in knowledge_points]
                        if text not in existing_names:
                            knowledge_points.append({
                                'name': text,
                                'subject': subject,
                                'grade': 'Grade 8',
                                'chapter': 'ä¸­è€ƒé‡ç‚¹',
                                'description': f"ä¸­è€ƒç½‘{subject}ç›¸å…³å†…å®¹ï¼š{text}",
                                'difficulty_level': 3,
                                'importance_level': 3,
                                'keywords': keywords + [text[:10]],
                                'source': 'ä¸­è€ƒç½‘',
                                'source_url': urljoin(f"https://www.zhongkao.com/{subject}/", href),
                                'crawl_time': datetime.now().isoformat()
                            })

        except Exception as e:
            logger.warning(f"    âš ï¸ æå–çŸ¥è¯†ç‚¹å¤±è´¥: {e}")

        return knowledge_points[:10]  # é™åˆ¶æ•°é‡

    def _find_question_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """æŸ¥æ‰¾è¯•é¢˜é¡µé¢é“¾æ¥"""
        question_links = []

        try:
            # å¤šç§é€‰æ‹©å™¨æŸ¥æ‰¾è¯•é¢˜é“¾æ¥
            link_patterns = [
                'a[href*="test"]',
                'a[href*="exam"]',
                'a[href*="question"]',
                'a[href*="practice"]',
                'a[href*="zhenti"]',  # çœŸé¢˜
                'a[href*="moniti"]',  # æ¨¡æ‹Ÿé¢˜
                'a[href*="lianxi"]',  # ç»ƒä¹ 
            ]

            for pattern in link_patterns:
                links = soup.select(pattern)
                for link in links[:8]:  # é™åˆ¶æ•°é‡
                    href = link.get('href', '')
                    if href and not href.startswith('#') and not href.startswith('javascript'):
                        full_url = urljoin(base_url, href)
                        if (full_url not in question_links and
                            not self._is_url_crawled_recently('zhongkao', full_url, 24)):  # 24å°æ—¶å†…ä¸é‡å¤çˆ¬å–
                            question_links.append(full_url)

        except Exception as e:
            logger.warning(f"    âš ï¸ æŸ¥æ‰¾è¯•é¢˜é“¾æ¥å¤±è´¥: {e}")

        return question_links[:5]  # é™åˆ¶æ•°é‡

    def _crawl_real_questions(self, question_links: List[str], subject: str) -> List[Dict]:
        """çˆ¬å–çœŸå®çš„è¯•é¢˜å†…å®¹"""
        questions = []

        for url in question_links[:3]:  # é™åˆ¶æ•°é‡
            try:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦çˆ¬å–
                if self._is_url_crawled_recently('zhongkao', url, 12):  # 12å°æ—¶å†…ä¸é‡å¤çˆ¬å–
                    continue

                response = self.session.get(url, timeout=15)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')

                    # å°è¯•å¤šç§æ–¹å¼æå–é¢˜ç›®
                    question_elements = (
                        soup.select('div.question, .test-item, .exam-question, .practice-item') +
                        soup.select('div[class*="question"], div[class*="test"], div[class*="exam"]')
                    )

                    for i, elem in enumerate(question_elements[:8]):  # é™åˆ¶æ•°é‡
                        try:
                            # æå–é¢˜å¹²
                            stem_selectors = ['.stem', '.question-text', '.title', 'p', '.content']
                            stem = None
                            for selector in stem_selectors:
                                stem_elem = elem.select_one(selector)
                                if stem_elem:
                                    stem = stem_elem.get_text(strip=True)
                                    break

                            if not stem or len(stem) < 10:
                                continue

                            # æå–é€‰é¡¹
                            options = []
                            option_selectors = ['.option', '.choice', 'li', '.answer-option']
                            for selector in option_selectors:
                                option_elems = elem.select(selector)
                                for opt_elem in option_elems[:4]:
                                    opt_text = opt_elem.get_text(strip=True)
                                    if opt_text and len(opt_text) > 2:
                                        # æ¸…ç†é€‰é¡¹æ ¼å¼
                                        opt_text = re.sub(r'^[A-D]\.?\s*', '', opt_text)
                                        if opt_text not in options:
                                            options.append(opt_text)

                            # å°è¯•æå–ç­”æ¡ˆ
                            correct_answer = 'A'  # é»˜è®¤å€¼
                            answer_selectors = ['.answer', '.correct', '.solution', '.jiexi']
                            for selector in answer_selectors:
                                answer_elem = elem.select_one(selector)
                                if answer_elem:
                                    answer_text = answer_elem.get_text(strip=True)
                                    # ä»ç­”æ¡ˆæ–‡æœ¬ä¸­æå–é€‰é¡¹å­—æ¯
                                    for letter in ['A', 'B', 'C', 'D']:
                                        if letter in answer_text:
                                            correct_answer = letter
                                            break
                                    break

                            # æå–è§£æ
                            explanation = ''
                            explanation_selectors = ['.explanation', '.analysis', '.solution', '.jiexi']
                            for selector in explanation_selectors:
                                explanation_elem = elem.select_one(selector)
                                if explanation_elem and explanation_elem != answer_elem:
                                    explanation = explanation_elem.get_text(strip=True)[:200]
                                    break

                            if not explanation:
                                explanation = f"æ¥æºäºä¸­è€ƒç½‘{subject}ç»ƒä¹ é¢˜"

                            questions.append({
                                'question_id': f"zhongkao_{subject}_{len(questions)+1:03d}",
                                'subject': subject,
                                'grade': 'Grade 8',
                                'question_type': 'choice' if options else 'fill_blank',
                                'stem': stem[:300],
                                'options': '|'.join(options) if options else '',
                                'correct_answer': correct_answer,
                                'explanation': explanation,
                                'difficulty_level': 3,
                                'knowledge_points': [stem[:30]],
                                'source': 'ä¸­è€ƒç½‘',
                                'source_url': url,
                                'crawl_time': datetime.now().isoformat()
                            })

                        except Exception as e:
                            logger.debug(f"      âš ï¸ è§£æé¢˜ç›®å¤±è´¥: {e}")
                            continue

                    # è®°å½•çˆ¬å–ç»“æœ
                    if questions:
                        content_hash = self._get_content_hash(response.text)
                        self._record_crawl_result('zhongkao', url, content_hash, 'success', f'{subject}_questions')

                time.sleep(self.request_delay / 2)  # è¯•é¢˜é¡µé¢é—´éš”è¾ƒçŸ­

            except Exception as e:
                logger.warning(f"    âš ï¸ çˆ¬å–è¯•é¢˜é¡µé¢å¤±è´¥ {url}: {e}")
                self._record_crawl_result('zhongkao', url, None, 'failed', f'{subject}_questions')

        return questions

# å°†æ–¹æ³•æ·»åŠ åˆ°ä¸»ç±»ä¸­
LegalEducationCrawler._extract_real_knowledge_points = LegalEducationCrawlerMethods._extract_real_knowledge_points
LegalEducationCrawler._find_question_links = LegalEducationCrawlerMethods._find_question_links  
LegalEducationCrawler._crawl_real_questions = LegalEducationCrawlerMethods._crawl_real_questions

if __name__ == "__main__":
    main()
