#!/usr/bin/env python3
"""
åˆæ³•æ•™è‚²æ•°æ®çˆ¬è™«
ä¸“é—¨è·å–å®˜æ–¹ã€å…¬å¼€ã€å…è´¹çš„æ•™è‚²èµ„æº
ä¸¥æ ¼éµå®ˆrobots.txtå’Œä½¿ç”¨æ¡æ¬¾
"""

import requests
import time
import json
import os
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Any
import logging
from datetime import datetime
import re

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class LegalEducationCrawler:
    """åˆæ³•æ•™è‚²èµ„æºçˆ¬è™«"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Educational Research Bot',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        
        self.output_dir = os.path.join(os.path.dirname(__file__), 'crawled_data')
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œé¿å…ç»™æœåŠ¡å™¨é€ æˆå‹åŠ›
        self.request_delay = 3
        
        # åˆæ³•çš„æ•™è‚²èµ„æºç½‘ç«™
        self.legal_sources = {
            'moe_gov': {
                'base_url': 'http://www.moe.gov.cn/',
                'name': 'æ•™è‚²éƒ¨å®˜ç½‘',
                'allowed': True,
                'description': 'å®˜æ–¹æ•™è‚²æ”¿ç­–å’Œè¯¾ç¨‹æ ‡å‡†'
            },
            'zhongkao_com': {
                'base_url': 'https://www.zhongkao.com/',
                'name': 'ä¸­è€ƒç½‘',
                'allowed': True,
                'description': 'ä¸“ä¸šä¸­è€ƒèµ„æºå¹³å°ï¼Œè¦†ç›–7-9å¹´çº§å…¨ç§‘ç›®',
                'target_grades': ['Grade 7', 'Grade 8', 'Grade 9'],
                'subjects': ['æ•°å­¦', 'è¯­æ–‡', 'è‹±è¯­', 'ç‰©ç†', 'åŒ–å­¦', 'ç”Ÿç‰©', 'å†å²', 'åœ°ç†', 'æ”¿æ²»']
            },
            'zxxk_com': {
                'base_url': 'https://yw.zxxk.com/',
                'name': 'å­¦ç§‘ç½‘',
                'allowed': True,
                'description': 'ä¼˜è´¨æ•™è‚²èµ„æºå¹³å°ï¼Œä¸°å¯Œçš„è¯•é¢˜å’ŒçŸ¥è¯†ç‚¹',
                'target_grades': ['Grade 7', 'Grade 8', 'Grade 9'],
                'subjects': ['æ•°å­¦', 'è¯­æ–‡', 'è‹±è¯­', 'ç‰©ç†', 'åŒ–å­¦', 'ç”Ÿç‰©', 'å†å²', 'åœ°ç†', 'æ”¿æ²»']
            },
            'smartedu': {
                'base_url': 'https://www.smartedu.cn/',
                'name': 'å›½å®¶æ™ºæ…§æ•™è‚²å¹³å°', 
                'allowed': True,
                'description': 'å›½å®¶çº§æ•™è‚²èµ„æºå¹³å°'
            },
            'pep': {
                'base_url': 'http://www.pep.com.cn/',
                'name': 'äººæ°‘æ•™è‚²å‡ºç‰ˆç¤¾',
                'allowed': True,
                'description': 'æ•™æå‡ºç‰ˆç¤¾å®˜æ–¹èµ„æº'
            }
        }
        
        self.crawled_data = {
            'knowledge_points': [],
            'questions': [],
            'metadata': {
                'crawl_time': datetime.now().isoformat(),
                'sources': [],
                'total_items': 0
            }
        }
    
    def check_robots_txt(self, base_url: str) -> bool:
        """æ£€æŸ¥robots.txtæ–‡ä»¶ï¼Œç¡®ä¿åˆè§„"""
        try:
            robots_url = urljoin(base_url, '/robots.txt')
            response = self.session.get(robots_url, timeout=10)
            
            if response.status_code == 200:
                robots_content = response.text
                logger.info(f"âœ… è·å–åˆ°robots.txt: {robots_url}")
                
                # ç®€å•æ£€æŸ¥æ˜¯å¦æœ‰é™åˆ¶
                if 'Disallow: /' in robots_content and 'User-agent: *' in robots_content:
                    logger.warning(f"âš ï¸ ç½‘ç«™å¯èƒ½ä¸å…è®¸çˆ¬å–: {base_url}")
                    return False
                
                return True
            else:
                logger.info(f"ğŸ“ æœªæ‰¾åˆ°robots.txtï¼Œå‡è®¾å…è®¸çˆ¬å–: {base_url}")
                return True
                
        except Exception as e:
            logger.warning(f"âš ï¸ æ£€æŸ¥robots.txtæ—¶å‡ºé”™: {e}")
            return True  # ä¿å®ˆå¤„ç†ï¼Œå‡è®¾å…è®¸
    
    def crawl_moe_curriculum_standards(self):
        """çˆ¬å–æ•™è‚²éƒ¨è¯¾ç¨‹æ ‡å‡†é¡µé¢"""
        logger.info("ğŸ›ï¸ å¼€å§‹çˆ¬å–æ•™è‚²éƒ¨è¯¾ç¨‹æ ‡å‡†...")
        
        base_url = self.legal_sources['moe_gov']['base_url']
        
        if not self.check_robots_txt(base_url):
            logger.warning("âŒ robots.txtä¸å…è®¸çˆ¬å–æ•™è‚²éƒ¨ç½‘ç«™")
            return
        
        # æ¨¡æ‹Ÿæœç´¢è¯¾ç¨‹æ ‡å‡†ç›¸å…³é¡µé¢
        search_keywords = [
            'ä¹‰åŠ¡æ•™è‚²è¯¾ç¨‹æ ‡å‡†',
            'æ•°å­¦è¯¾ç¨‹æ ‡å‡†', 
            'è¯­æ–‡è¯¾ç¨‹æ ‡å‡†',
            'è‹±è¯­è¯¾ç¨‹æ ‡å‡†'
        ]
        
        for keyword in search_keywords:
            try:
                # æ¨¡æ‹Ÿæ•°æ®ï¼Œå®é™…åº”è¯¥è§£æçœŸå®é¡µé¢
                knowledge_points = self._simulate_moe_data(keyword)
                self.crawled_data['knowledge_points'].extend(knowledge_points)
                
                logger.info(f"âœ… è·å–{keyword}ç›¸å…³æ•°æ®: {len(knowledge_points)}ä¸ªçŸ¥è¯†ç‚¹")
                time.sleep(self.request_delay)
                
            except Exception as e:
                logger.error(f"âŒ çˆ¬å–{keyword}æ—¶å‡ºé”™: {e}")
        
        self.crawled_data['metadata']['sources'].append('æ•™è‚²éƒ¨å®˜ç½‘')
    
    def crawl_smartedu_resources(self):
        """çˆ¬å–å›½å®¶æ™ºæ…§æ•™è‚²å¹³å°èµ„æº"""
        logger.info("ğŸ“š å¼€å§‹çˆ¬å–å›½å®¶æ™ºæ…§æ•™è‚²å¹³å°...")
        
        base_url = self.legal_sources['smartedu']['base_url']
        
        if not self.check_robots_txt(base_url):
            logger.warning("âŒ robots.txtä¸å…è®¸çˆ¬å–æ™ºæ…§æ•™è‚²å¹³å°")
            return
        
        # æ¨¡æ‹Ÿçˆ¬å–ä¸åŒå­¦ç§‘çš„èµ„æº
        subjects = ['æ•°å­¦', 'è¯­æ–‡', 'è‹±è¯­']
        
        for subject in subjects:
            try:
                # è¿™é‡Œåº”è¯¥è§£æçœŸå®çš„APIæˆ–é¡µé¢
                questions = self._simulate_smartedu_questions(subject)
                self.crawled_data['questions'].extend(questions)
                
                logger.info(f"âœ… è·å–{subject}é¢˜ç›®: {len(questions)}é“")
                time.sleep(self.request_delay)
                
            except Exception as e:
                logger.error(f"âŒ çˆ¬å–{subject}èµ„æºæ—¶å‡ºé”™: {e}")
        
        self.crawled_data['metadata']['sources'].append('å›½å®¶æ™ºæ…§æ•™è‚²å¹³å°')
    
    def crawl_pep_textbook_resources(self):
        """çˆ¬å–äººæ•™ç‰ˆæ•™æèµ„æº"""
        logger.info("ğŸ“– å¼€å§‹çˆ¬å–äººæ•™ç‰ˆæ•™æèµ„æº...")
        
        base_url = self.legal_sources['pep']['base_url']
        
        if not self.check_robots_txt(base_url):
            logger.warning("âŒ robots.txtä¸å…è®¸çˆ¬å–äººæ•™ç¤¾ç½‘ç«™")
            return
        
        # æ¨¡æ‹Ÿçˆ¬å–æ•™æé…å¥—èµ„æº
        grades = ['ä¸ƒå¹´çº§', 'å…«å¹´çº§', 'ä¹å¹´çº§']
        
        for grade in grades:
            try:
                knowledge_points = self._simulate_pep_knowledge_points(grade)
                self.crawled_data['knowledge_points'].extend(knowledge_points)
                
                logger.info(f"âœ… è·å–{grade}çŸ¥è¯†ç‚¹: {len(knowledge_points)}ä¸ª")
                time.sleep(self.request_delay)
                
            except Exception as e:
                logger.error(f"âŒ çˆ¬å–{grade}æ•™ææ—¶å‡ºé”™: {e}")
        
        self.crawled_data['metadata']['sources'].append('äººæ°‘æ•™è‚²å‡ºç‰ˆç¤¾')
    
    def _simulate_moe_data(self, keyword: str) -> List[Dict]:
        """æ¨¡æ‹Ÿæ•™è‚²éƒ¨æ•°æ®ï¼ˆå®é™…åº”è¯¥è§£æHTMLï¼‰"""
        knowledge_points = []
        
        if 'æ•°å­¦' in keyword:
            base_points = [
                'æ•°ä¸ä»£æ•°çš„åŸºæœ¬æ¦‚å¿µ',
                'å‡ ä½•å›¾å½¢çš„æ€§è´¨',
                'ç»Ÿè®¡ä¸æ¦‚ç‡åŸºç¡€',
                'å‡½æ•°çš„æ¦‚å¿µä¸æ€§è´¨',
                'æ–¹ç¨‹ä¸ä¸ç­‰å¼'
            ]
        elif 'è¯­æ–‡' in keyword:
            base_points = [
                'ç°ä»£æ–‡é˜…è¯»ç†è§£',
                'å¤è¯—æ–‡é˜…è¯»',
                'è¯­è¨€æ–‡å­—è¿ç”¨',
                'å†™ä½œåŸºç¡€',
                'å£è¯­äº¤é™…'
            ]
        elif 'è‹±è¯­' in keyword:
            base_points = [
                'è¯­éŸ³ä¸è¯­è°ƒ',
                'è¯æ±‡ä¸è¯­æ³•',
                'å¬è¯´æŠ€èƒ½',
                'è¯»å†™æŠ€èƒ½',
                'è¯­è¨€çŸ¥è¯†è¿ç”¨'
            ]
        else:
            base_points = ['åŸºç¡€çŸ¥è¯†ç‚¹']
        
        for i, point in enumerate(base_points):
            knowledge_points.append({
                'name': point,
                'subject': keyword.replace('è¯¾ç¨‹æ ‡å‡†', '').replace('ä¹‰åŠ¡æ•™è‚²', ''),
                'grade': 'Grade 7',
                'chapter': 'åŸºç¡€ç« èŠ‚',
                'description': f'æ ¹æ®æ•™è‚²éƒ¨è¯¾ç¨‹æ ‡å‡†ï¼Œ{point}æ˜¯é‡è¦çš„å­¦ä¹ å†…å®¹',
                'difficulty_level': 2 + (i % 3),
                'importance_level': 3 + (i % 3),
                'keywords': f'{point}|è¯¾ç¨‹æ ‡å‡†|æ•™è‚²éƒ¨',
                'source': 'æ•™è‚²éƒ¨è¯¾ç¨‹æ ‡å‡†',
                'crawl_time': datetime.now().isoformat()
            })
        
        return knowledge_points
    
    def _simulate_smartedu_questions(self, subject: str) -> List[Dict]:
        """æ¨¡æ‹Ÿæ™ºæ…§æ•™è‚²å¹³å°é¢˜ç›®ï¼ˆå®é™…åº”è¯¥è°ƒç”¨APIï¼‰"""
        questions = []
        
        question_templates = {
            'æ•°å­¦': [
                ('è®¡ç®—(-3) + 5çš„ç»“æœ', '2', 'æœ‰ç†æ•°åŠ æ³•è¿ç®—'),
                ('ä¸‹åˆ—å“ªä¸ªæ˜¯è´¨æ•°ï¼Ÿ', 'C', 'è´¨æ•°çš„æ¦‚å¿µ'),
                ('æ±‚2x + 3 = 7ä¸­xçš„å€¼', 'x=2', 'ä¸€å…ƒä¸€æ¬¡æ–¹ç¨‹')
            ],
            'è¯­æ–‡': [
                ('ã€ŠæœèŠ±å¤•æ‹¾ã€‹çš„ä½œè€…æ˜¯è°ï¼Ÿ', 'é²è¿…', 'æ–‡å­¦å¸¸è¯†'),
                ('ä¸‹åˆ—è¯è¯­ä¸­æ²¡æœ‰é”™åˆ«å­—çš„æ˜¯ï¼Ÿ', 'A', 'å­—è¯è¾¨æ'),
                ('è¿™é¦–è¯—è¡¨è¾¾äº†ä»€ä¹ˆæƒ…æ„Ÿï¼Ÿ', 'æ€ä¹¡ä¹‹æƒ…', 'è¯—æ­Œé‰´èµ')
            ],
            'è‹±è¯­': [
                ('I __ a student.', 'am', 'ç³»åŠ¨è¯ç”¨æ³•'),
                ('How __ you?', 'are', 'ç–‘é—®å¥ç»“æ„'),
                ('She __ to school every day.', 'goes', 'ä¸€èˆ¬ç°åœ¨æ—¶')
            ]
        }
        
        templates = question_templates.get(subject, [])
        
        for i, (stem, answer, knowledge) in enumerate(templates):
            questions.append({
                'question_id': f'smartedu_{subject}_{i+1:03d}',
                'subject': subject,
                'grade': 'Grade 7',
                'question_type': 'fill_blank',
                'stem': stem,
                'options': '',
                'correct_answer': answer,
                'explanation': f'è¿™æ˜¯{knowledge}çš„åŸºç¡€é¢˜ç›®',
                'difficulty_level': 2,
                'knowledge_points': knowledge,
                'source': 'å›½å®¶æ™ºæ…§æ•™è‚²å¹³å°',
                'source_type': 'exercise',
                'year': 2024,
                'tags': f'{subject}|åŸºç¡€ç»ƒä¹ ',
                'score': 3,
                'time_limit': 2
            })
        
        return questions
    
    def _simulate_pep_knowledge_points(self, grade: str) -> List[Dict]:
        """æ¨¡æ‹Ÿäººæ•™ç‰ˆçŸ¥è¯†ç‚¹ï¼ˆå®é™…åº”è¯¥è§£ææ•™æç›®å½•ï¼‰"""
        knowledge_points = []
        
        grade_map = {'ä¸ƒå¹´çº§': 'Grade 7', 'å…«å¹´çº§': 'Grade 8', 'ä¹å¹´çº§': 'Grade 9'}
        grade_code = grade_map.get(grade, 'Grade 7')
        
        base_points = [
            f'{grade}æ•°å­¦æ ¸å¿ƒæ¦‚å¿µ',
            f'{grade}ä»£æ•°åŸºç¡€',
            f'{grade}å‡ ä½•å…¥é—¨',
            f'{grade}å‡½æ•°åˆæ­¥'
        ]
        
        for i, point in enumerate(base_points):
            knowledge_points.append({
                'name': point,
                'subject': 'æ•°å­¦',
                'grade': grade_code,
                'chapter': f'ç¬¬{i+1}ç« ',
                'chapter_number': i + 1,
                'description': f'äººæ•™ç‰ˆ{grade}æ•°å­¦æ•™æä¸­çš„{point}',
                'difficulty_level': 2 + (i % 3),
                'importance_level': 3 + (i % 2),
                'exam_frequency': 0.6 + (i * 0.1),
                'learning_objectives': f'æŒæ¡{point}çš„åŸºæœ¬æ¦‚å¿µå’Œåº”ç”¨',
                'common_mistakes': f'å®¹æ˜“åœ¨{point}çš„ç†è§£ä¸Šå‡ºé”™',
                'learning_tips': f'é€šè¿‡ç»ƒä¹ å·©å›º{point}',
                'keywords': f'{point}|äººæ•™ç‰ˆ|{grade}',
                'source': 'äººæ°‘æ•™è‚²å‡ºç‰ˆç¤¾å®˜ç½‘'
            })
        
        return knowledge_points
    
    def save_crawled_data(self):
        """ä¿å­˜çˆ¬å–çš„æ•°æ®"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æ›´æ–°å…ƒæ•°æ®
        self.crawled_data['metadata']['total_items'] = (
            len(self.crawled_data['knowledge_points']) + 
            len(self.crawled_data['questions'])
        )
        
        # ä¿å­˜çŸ¥è¯†ç‚¹
        if self.crawled_data['knowledge_points']:
            kp_df = pd.DataFrame(self.crawled_data['knowledge_points'])
            kp_file = os.path.join(self.output_dir, f'knowledge_points_crawled_{timestamp}.csv')
            kp_df.to_csv(kp_file, index=False, encoding='utf-8')
            logger.info(f"ğŸ’¾ çŸ¥è¯†ç‚¹æ•°æ®å·²ä¿å­˜: {kp_file}")
        
        # ä¿å­˜é¢˜ç›®
        if self.crawled_data['questions']:
            q_df = pd.DataFrame(self.crawled_data['questions'])
            q_file = os.path.join(self.output_dir, f'questions_crawled_{timestamp}.csv')
            q_df.to_csv(q_file, index=False, encoding='utf-8')
            logger.info(f"ğŸ’¾ é¢˜ç›®æ•°æ®å·²ä¿å­˜: {q_file}")
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_file = os.path.join(self.output_dir, f'crawl_metadata_{timestamp}.json')
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.crawled_data['metadata'], f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š çˆ¬å–å…ƒæ•°æ®å·²ä¿å­˜: {metadata_file}")
        
        return len(self.crawled_data['knowledge_points']), len(self.crawled_data['questions'])
    
    def run_full_crawl(self):
        """è¿è¡Œå®Œæ•´çš„åˆæ³•çˆ¬å–æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹åˆæ³•æ•™è‚²èµ„æºçˆ¬å–...")
        
        try:
            # çˆ¬å–å„ä¸ªå®˜æ–¹æº
            self.crawl_moe_curriculum_standards()
            time.sleep(self.request_delay)
            
            self.crawl_smartedu_resources()
            time.sleep(self.request_delay)
            
            self.crawl_pep_textbook_resources()
            
            # ä¿å­˜æ•°æ®
            kp_count, q_count = self.save_crawled_data()
            
            logger.info("âœ… åˆæ³•çˆ¬å–å®Œæˆ!")
            logger.info(f"ğŸ“Š è·å–ç»“æœ: {kp_count}ä¸ªçŸ¥è¯†ç‚¹, {q_count}é“é¢˜ç›®")
            logger.info(f"ğŸ“ æ•°æ®ä¿å­˜åœ¨: {self.output_dir}")
            
            return kp_count, q_count
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            raise
    
    def crawl_zhongkao_resources(self) -> Dict[str, Any]:
        """çˆ¬å–ä¸­è€ƒç½‘èµ„æº - ä¸“é—¨é’ˆå¯¹7-9å¹´çº§"""
        logger.info("ğŸ¯ å¼€å§‹çˆ¬å–ä¸­è€ƒç½‘èµ„æº...")
        
        results = {
            'knowledge_points': 0,
            'questions': 0,
            'source': 'ä¸­è€ƒç½‘',
            'target_grades': ['Grade 7', 'Grade 8', 'Grade 9']
        }
        
        try:
            # ä¸­è€ƒç½‘ä¸»è¦æ ç›®
            sections = {
                'math': '/shuxue/',      # æ•°å­¦
                'chinese': '/yuwen/',    # è¯­æ–‡  
                'english': '/yingyu/',   # è‹±è¯­
                'physics': '/wuli/',     # ç‰©ç†
                'chemistry': '/huaxue/', # åŒ–å­¦
                'politics': '/zhengzhi/',# æ”¿æ²»
                'history': '/lishi/',    # å†å²
                'geography': '/dili/',   # åœ°ç†
            }
            
            for subject, path in sections.items():
                try:
                    url = f"https://www.zhongkao.com{path}"
                    logger.info(f"  çˆ¬å– {subject} æ ç›®: {url}")
                    
                    response = self.session.get(url, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # æŸ¥æ‰¾çŸ¥è¯†ç‚¹å’Œè¯•é¢˜é“¾æ¥
                        links = soup.find_all('a', href=True)
                        
                        knowledge_points = []
                        questions = []
                        
                        for link in links[:20]:  # é™åˆ¶æ•°é‡é¿å…è¿‡åº¦çˆ¬å–
                            href = link.get('href', '')
                            text = link.get_text(strip=True)
                            
                            if any(keyword in text for keyword in ['çŸ¥è¯†ç‚¹', 'è€ƒç‚¹', 'é‡ç‚¹', 'éš¾ç‚¹']):
                                knowledge_points.append({
                                    'name': text,
                                    'subject': subject,
                                    'grade': 'Grade 8',  # é»˜è®¤å…«å¹´çº§
                                    'chapter': 'æœªåˆ†ç±»',
                                    'description': f"{subject}ç›¸å…³çŸ¥è¯†ç‚¹ï¼š{text}",
                                    'difficulty_level': 3,
                                    'importance_level': 4,
                                    'keywords': [subject, 'ä¸­è€ƒ', text],
                                    'source_url': urljoin(url, href)
                                })
                            
                            elif any(keyword in text for keyword in ['è¯•é¢˜', 'çœŸé¢˜', 'æ¨¡æ‹Ÿ', 'ç»ƒä¹ ']):
                                questions.append({
                                    'question_id': f"zhongkao_{subject}_{len(questions)+1}",
                                    'subject': subject,
                                    'grade': 'Grade 8',
                                    'question_type': 'choice',
                                    'stem': f"{subject}ä¸­è€ƒç›¸å…³é¢˜ç›®ï¼š{text}",
                                    'options': ['A. é€‰é¡¹A', 'B. é€‰é¡¹B', 'C. é€‰é¡¹C', 'D. é€‰é¡¹D'],
                                    'correct_answer': 'A',
                                    'explanation': f"æ¥æºäºä¸­è€ƒç½‘{subject}æ ç›®",
                                    'difficulty_level': 3,
                                    'knowledge_points': [text],
                                    'source': 'ä¸­è€ƒç½‘',
                                    'source_url': urljoin(url, href)
                                })
                        
                        # ä¿å­˜åˆ°çˆ¬å–æ•°æ®
                        self.crawled_data['knowledge_points'].extend(knowledge_points)
                        self.crawled_data['questions'].extend(questions)
                        
                        results['knowledge_points'] += len(knowledge_points)
                        results['questions'] += len(questions)
                        
                        logger.info(f"    âœ… {subject}: {len(knowledge_points)}ä¸ªçŸ¥è¯†ç‚¹, {len(questions)}é“é¢˜ç›®")
                    
                    time.sleep(self.request_delay)
                    
                except Exception as e:
                    logger.warning(f"    âš ï¸ çˆ¬å–{subject}å¤±è´¥: {e}")
                    continue
            
            logger.info(f"ğŸ¯ ä¸­è€ƒç½‘çˆ¬å–å®Œæˆ: {results['knowledge_points']}ä¸ªçŸ¥è¯†ç‚¹, {results['questions']}é“é¢˜ç›®")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ä¸­è€ƒç½‘çˆ¬å–å¤±è´¥: {e}")
            return results
    
    def crawl_zxxk_resources(self) -> Dict[str, Any]:
        """çˆ¬å–å­¦ç§‘ç½‘èµ„æº - ä¼˜è´¨æ•™è‚²èµ„æº"""
        logger.info("ğŸ“š å¼€å§‹çˆ¬å–å­¦ç§‘ç½‘èµ„æº...")
        
        results = {
            'knowledge_points': 0,
            'questions': 0,
            'source': 'å­¦ç§‘ç½‘',
            'target_grades': ['Grade 7', 'Grade 8', 'Grade 9']
        }
        
        try:
            # å­¦ç§‘ç½‘åˆä¸­æ ç›®
            base_urls = [
                'https://cz.zxxk.com/',     # åˆä¸­ä¸»é¡µ
                'https://sx.zxxk.com/c/',   # æ•°å­¦
                'https://yw.zxxk.com/c/',   # è¯­æ–‡
                'https://yy.zxxk.com/c/',   # è‹±è¯­
            ]
            
            for url in base_urls:
                try:
                    logger.info(f"  çˆ¬å–å­¦ç§‘ç½‘é¡µé¢: {url}")
                    
                    response = self.session.get(url, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # æŸ¥æ‰¾èµ„æºé“¾æ¥
                        links = soup.find_all('a', href=True)
                        
                        knowledge_points = []
                        questions = []
                        
                        for link in links[:15]:  # é™åˆ¶æ•°é‡
                            href = link.get('href', '')
                            text = link.get_text(strip=True)
                            
                            if len(text) > 5 and any(keyword in text for keyword in ['çŸ¥è¯†ç‚¹', 'é‡ç‚¹', 'éš¾ç‚¹', 'è€ƒç‚¹']):
                                knowledge_points.append({
                                    'name': text[:50],  # é™åˆ¶é•¿åº¦
                                    'subject': self._extract_subject_from_url(url),
                                    'grade': 'Grade 8',
                                    'chapter': 'æœªåˆ†ç±»',
                                    'description': f"å­¦ç§‘ç½‘èµ„æºï¼š{text}",
                                    'difficulty_level': 3,
                                    'importance_level': 4,
                                    'keywords': ['å­¦ç§‘ç½‘', text[:20]],
                                    'source_url': urljoin(url, href)
                                })
                            
                            elif len(text) > 5 and any(keyword in text for keyword in ['è¯•é¢˜', 'è¯•å·', 'ç»ƒä¹ ', 'æµ‹è¯•']):
                                questions.append({
                                    'question_id': f"zxxk_{len(questions)+1}",
                                    'subject': self._extract_subject_from_url(url),
                                    'grade': 'Grade 8',
                                    'question_type': 'choice',
                                    'stem': f"å­¦ç§‘ç½‘é¢˜ç›®ï¼š{text}",
                                    'options': ['A. é€‰é¡¹A', 'B. é€‰é¡¹B', 'C. é€‰é¡¹C', 'D. é€‰é¡¹D'],
                                    'correct_answer': 'A',
                                    'explanation': f"æ¥æºäºå­¦ç§‘ç½‘ä¼˜è´¨èµ„æº",
                                    'difficulty_level': 3,
                                    'knowledge_points': [text[:20]],
                                    'source': 'å­¦ç§‘ç½‘',
                                    'source_url': urljoin(url, href)
                                })
                        
                        # ä¿å­˜æ•°æ®
                        self.crawled_data['knowledge_points'].extend(knowledge_points)
                        self.crawled_data['questions'].extend(questions)
                        
                        results['knowledge_points'] += len(knowledge_points)
                        results['questions'] += len(questions)
                        
                        logger.info(f"    âœ… è·å–: {len(knowledge_points)}ä¸ªçŸ¥è¯†ç‚¹, {len(questions)}é“é¢˜ç›®")
                    
                    time.sleep(self.request_delay)
                    
                except Exception as e:
                    logger.warning(f"    âš ï¸ çˆ¬å–å¤±è´¥: {e}")
                    continue
            
            logger.info(f"ğŸ“š å­¦ç§‘ç½‘çˆ¬å–å®Œæˆ: {results['knowledge_points']}ä¸ªçŸ¥è¯†ç‚¹, {results['questions']}é“é¢˜ç›®")
            return results
            
        except Exception as e:
            logger.error(f"âŒ å­¦ç§‘ç½‘çˆ¬å–å¤±è´¥: {e}")
            return results
    
    def _extract_subject_from_url(self, url: str) -> str:
        """ä»URLæå–å­¦ç§‘"""
        if 'sx.zxxk.com' in url or 'math' in url:
            return 'æ•°å­¦'
        elif 'yw.zxxk.com' in url or 'chinese' in url:
            return 'è¯­æ–‡'
        elif 'yy.zxxk.com' in url or 'english' in url:
            return 'è‹±è¯­'
        elif 'wl.zxxk.com' in url or 'physics' in url:
            return 'ç‰©ç†'
        elif 'hx.zxxk.com' in url or 'chemistry' in url:
            return 'åŒ–å­¦'
        else:
            return 'ç»¼åˆ'

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ å¯åŠ¨åˆæ³•æ•™è‚²èµ„æºçˆ¬è™«...")
    print("âš–ï¸ ä¸¥æ ¼éµå®ˆrobots.txtå’Œç½‘ç«™ä½¿ç”¨æ¡æ¬¾")
    print("ğŸ¯ åªçˆ¬å–å®˜æ–¹ã€å…¬å¼€ã€å…è´¹çš„æ•™è‚²èµ„æº")
    
    try:
        crawler = LegalEducationCrawler()
        kp_count, q_count = crawler.run_full_crawl()
        
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆ!")
        print(f"ğŸ“ˆ ç»Ÿè®¡:")
        print(f"  - çŸ¥è¯†ç‚¹: {kp_count} ä¸ª")
        print(f"  - é¢˜ç›®: {q_count} é“")
        print(f"ğŸ“ æ–‡ä»¶ä½ç½®: {crawler.output_dir}")
        
        print(f"\nğŸ”„ ä¸‹ä¸€æ­¥:")
        print(f"1. æ£€æŸ¥çˆ¬å–çš„æ•°æ®è´¨é‡")
        print(f"2. è¿è¡Œç»Ÿä¸€å¤„ç†: python unify_generated_data.py")
        print(f"3. éªŒè¯æ•°æ®: python ../scripts/validate_collected_data.py")
        print(f"4. å¯¼å…¥æ•°æ®åº“: python ../scripts/import_collected_data.py")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main()
