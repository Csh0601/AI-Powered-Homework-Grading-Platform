#!/usr/bin/env python3
"""
ç®€åŒ–çš„æ•°æ®ç»Ÿä¸€å¤„ç†è„šæœ¬
å°†ä¸åŒæ¥æºçš„åŸå§‹æ•°æ®ç»Ÿä¸€ä¸ºæ ‡å‡†æ ¼å¼
"""

import os
import sys
import pandas as pd
import logging
from datetime import datetime
from pathlib import Path

# è®¾ç½®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def unify_all_data():
    """ç»Ÿä¸€å¤„ç†æ‰€æœ‰æ•°æ®"""
    logger.info("ğŸ”„ å¼€å§‹ç»Ÿä¸€å¤„ç†æ•°æ®...")
    
    # è®¾ç½®è·¯å¾„
    base_dir = Path(__file__).parent.parent
    raw_dir = base_dir / "raw" / "subjects"
    processed_dir = base_dir / "processed"
    
    # ç¡®ä¿processedç›®å½•å­˜åœ¨
    processed_dir.mkdir(exist_ok=True)
    
    all_knowledge_points = []
    all_questions = []
    
    # éå†æ‰€æœ‰å­¦ç§‘ç›®å½•
    for subject_dir in raw_dir.iterdir():
        if not subject_dir.is_dir():
            continue
            
        subject_name = subject_dir.name
        logger.info(f"ğŸ“š å¤„ç†å­¦ç§‘: {subject_name}")
        
        # å¤„ç†çŸ¥è¯†ç‚¹
        kp_dir = subject_dir / "knowledge_points"
        if kp_dir.exists():
            for kp_file in kp_dir.glob("*.csv"):
                try:
                    df = pd.read_csv(kp_file, encoding='utf-8')
                    df['subject'] = subject_name
                    all_knowledge_points.append(df)
                    logger.info(f"  âœ… åŠ è½½çŸ¥è¯†ç‚¹: {kp_file.name} ({len(df)}æ¡)")
                except Exception as e:
                    logger.error(f"  âŒ åŠ è½½çŸ¥è¯†ç‚¹å¤±è´¥: {kp_file.name} - {e}")
        
        # å¤„ç†é¢˜ç›®
        for question_type in ["exam_questions", "mock_questions"]:
            q_dir = subject_dir / question_type
            if q_dir.exists():
                for q_file in q_dir.glob("*.csv"):
                    try:
                        df = pd.read_csv(q_file, encoding='utf-8')
                        df['subject'] = subject_name
                        df['source_type'] = question_type
                        all_questions.append(df)
                        logger.info(f"  âœ… åŠ è½½é¢˜ç›®: {q_file.name} ({len(df)}æ¡)")
                    except Exception as e:
                        logger.error(f"  âŒ åŠ è½½é¢˜ç›®å¤±è´¥: {q_file.name} - {e}")
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    if all_knowledge_points:
        kp_combined = pd.concat(all_knowledge_points, ignore_index=True)
        kp_file = processed_dir / f"knowledge_points_unified_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        kp_combined.to_csv(kp_file, index=False, encoding='utf-8')
        logger.info(f"âœ… çŸ¥è¯†ç‚¹ç»Ÿä¸€å®Œæˆ: {kp_file} ({len(kp_combined)}æ¡)")
    else:
        kp_file = None
        logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°çŸ¥è¯†ç‚¹æ•°æ®")
    
    if all_questions:
        q_combined = pd.concat(all_questions, ignore_index=True)
        q_file = processed_dir / f"questions_unified_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        q_combined.to_csv(q_file, index=False, encoding='utf-8')
        logger.info(f"âœ… é¢˜ç›®ç»Ÿä¸€å®Œæˆ: {q_file} ({len(q_combined)}æ¡)")
    else:
        q_file = None
        logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°é¢˜ç›®æ•°æ®")
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    stats = {
        "timestamp": datetime.now().isoformat(),
        "knowledge_points_count": len(kp_combined) if all_knowledge_points else 0,
        "questions_count": len(q_combined) if all_questions else 0,
        "subjects_processed": len([d for d in raw_dir.iterdir() if d.is_dir()]),
        "files_processed": len(all_knowledge_points) + len(all_questions)
    }
    
    stats_file = processed_dir / f"unification_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    import json
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“Š ç»Ÿä¸€å¤„ç†ç»Ÿè®¡:")
    logger.info(f"  - çŸ¥è¯†ç‚¹: {stats['knowledge_points_count']}æ¡")
    logger.info(f"  - é¢˜ç›®: {stats['questions_count']}æ¡")
    logger.info(f"  - å¤„ç†å­¦ç§‘: {stats['subjects_processed']}ä¸ª")
    logger.info(f"  - å¤„ç†æ–‡ä»¶: {stats['files_processed']}ä¸ª")
    
    return kp_file, q_file

if __name__ == "__main__":
    unify_all_data()
